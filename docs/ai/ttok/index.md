---
title: ttok
---

# ttok

**`ttok`** √® un altro strumento creato da **Simon Willison**, per contare il numero di token di un testo e/o troncarlo in base a un numero desiderato di questi.

!!! note "Cosa sono i token?"

    I **token** sono le unit√† base in cui gli LLM suddividono il testo per elaborarlo. Un token pu√≤ essere una parola intera, parte di una parola, un carattere di punteggiatura o anche uno spazio. Ad esempio, la frase "Ciao mondo!" potrebbe essere divisa in 3 token: "Ciao", " mondo", "!".

√à fondamentale conoscere il numero di token perch√© i modelli LLM hanno un limite massimo di token che possono elaborare, in input e output. Inoltre il **costo** dell'**utilizzo** di un modello LLM √® spesso calcolato in base al numero di token elaborati.

`ttok` √® scritto in Python e si pu√≤ installare con `pip` o con `uv`:

```bash
pip install ttok
```

```bash
uv tool install ttok
```

Il modello di default utilizzato per il conteggio dei token √® quello di GPT-3.5 e GPT-4.

## Utilizzo

### Base

Il comando base si pu√≤ lanciare cos√¨:

```bash
ttok Ciao mondo
```

In output si ottiene il numero di token:

```bash
3
```

Oppure √® possibile fargli leggere l'output di un comando, come ad esempio `curl` e conteggiare il numero di token della pagina web su Wikipedia dedicata a Aaron Swartz:

```bash
curl -kL 'https://en.wikipedia.org/wiki/Aaron_Swartz' | ttok
```

In questo caso si hanno circa `230.000` token di output. Questo si pu√≤ passare a un `LLM` tramite [`llm` cli](../llm_cli/index.md) per chiedere di riassumere il testo per un messaggio da inviare in chat:

```
curl -kL 'https://en.wikipedia.org/wiki/Aaron_Swartz' | \
llm "mi scrivi un messaggio breve per stimolare le persone a scoprire chi √® Aaron. Lo invier√≤ in chat" | ttok
```

In output si ottiene qualcosa come il testo di sotto, che √® lungo circa `70` token:

```text {. }
Hai mai sentito parlare di Aaron Swartz? ü§î Era un genio, un attivista e un pioniere del web. La sua storia √® incredibile e ti far√† riflettere sul futuro di internet e della conoscenza. Scoprilo anche tu! üòâ"
```

Conoscendo il numero di token di input e di output, in base al modello di LLM scelto, si pu√≤ calcolare il [costo del servizio](#costo-dei-modelli-llm-per-numero-di-token).

### Troncare il testo a un certo numero di token

Se si vuole troncare il testo a un certo numero di token, si pu√≤ usare l'opzione `-t` seguita dal numero di token desiderato. Ad esempio, per troncare il testo a `10` token:

```bash
ttok -t 10 "Ciao mondo, questo √® un testo di esempio che voglio troncare a dieci token."
```

In output si ottiene il testo troncato:

```text {.wordwrap-code }
Ciao mondo, questo √® un testo di
```

### Visualizzare i token

Per visualizzare i token di un testo, si pu√≤ usare l'opzione `--encode`:

```bash
ttok "Ciao mondo, questo √® un testo di esempio" --encode
```

In output si ottengono gli ID dei token (dei 13 token del testo di esempio):

```text
34 23332 70809 11 34560 11676 653 1296 78 1891 1560 3342 822
```

Per convertire questi ID in testo leggibile, si pu√≤ usare l'opzione `--decode`:

```bash
ttok "34 23332 70809 11 34560 11676 653 1296 78 1891 1560 3342 822" --decode
```

## Costo dei modelli LLM per numero di token

Simon Willison - √® sempre bene rinnovare una volta al giorno **grazie Simon** -  ha realizzato un [calcolatore di costi per token e modello](https://www.llm-prices.com/), che permette di calcolare il costo del servizio.

[![](images/token_price.png)](https://www.llm-prices.com)
