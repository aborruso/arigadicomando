{"config":{"lang":["it"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"a riga di comando","text":"<p>La  riga di comando (la <code>CLI</code>) ha reso facile e comode tante delle cose che faccio con il PC. Allora ho voluto dedicargli uno spazio, sia per farne conoscere i pregi a chi ancora li ignora o sfrutta poco, sia per mettere a fattor comune alcuni dei miei appunti.</p> <p>\u00c8 un argomento molto ampio, che qui verr\u00e0 esploso sopratutto su due filoni principali:</p> <ul> <li>i suoi strumenti per leggere, trasformare e analizzare file di testo strutturati;</li> <li>le cose che mi piacciono, che mi sembrano \"geniali\", irrinunciabili, buffe, \"mai pi\u00f9 senza\", ecc..</li> </ul> <p>\u00c8 dedicato sopratutto al suo utilizzo nel mondo  Linux ed \u00e8 quindi per tutti.</p>"},{"location":"ai/","title":"L'Intelligenza Artificiale a riga di comando","text":"<p>\ud83d\udea7 Pagina in costruzione</p> <p>Questa pagina \u00e8 ancora in lavorazione. Il contenuto \u00e8 incompleto e non revisionato.</p>"},{"location":"ai/#lintelligenza-artificiale-a-riga-di-comando","title":"L'Intelligenza Artificiale a riga di comando","text":"<p>Immagina di avere una \"calcolatrice per le parole\": uno strumento che, anzich\u00e9 fare operazioni matematiche, lavora direttamente sul linguaggio. Questo \u00e8 ci\u00f2 che rappresentano i modelli linguistici di grandi dimensioni (LLM) come ChatGPT. Sono sistemi progettati per manipolare, trasformare, riscrivere, sintetizzare e analizzare testi. E cosa c'\u00e8 di pi\u00f9 testuale, diretto e potente della riga di comando?</p> <p>Tip</p> <p>Il riferimento alla \"calcolatrice per le parole\" viene da una delle letture pi\u00f9 stimolanti e didattiche a tema: \"Think of language models like ChatGPT as a \u201ccalculator for words\u201d\" dell'eccezionale Simon Willison.</p> <p>La shell \u00e8 da sempre il regno della manipolazione del testo. Ogni comando, ogni output, ogni pipeline \u00e8 un flusso di stringhe. Usare gli LLM da riga di comando non \u00e8 solo possibile: \u00e8 naturale. Quando interagiamo con un LLM in questo contesto, sfruttiamo al massimo la sua natura testuale e la potenza del terminale.</p> <p>Grazie agli LLM possiamo:</p> <ul> <li>Riformulare stringhe, messaggi, comandi</li> <li>Spiegare script o righe di codice complesso</li> <li>Generare documentazione a partire da contenuti testuali</li> <li>Estrarre informazioni da log, configurazioni, output di comandi</li> <li>Rispondere a domande su contenuti testuali locali (file, man page, ecc.)</li> </ul> <p>Queste operazioni si integrano perfettamente con l'uso di pipe (<code>|</code>), redirezioni (<code>&gt;</code>), comandi come <code>cat</code>, <code>grep</code>, <code>jq</code>, <code>awk</code>, e con strumenti di automazione e scripting.</p> <p>Nota</p> <p>Il termine \"AI\" \u00e8 generico, inflazionato e spesso usato in modo fuorviante. Qui, per la scelta degli strumenti e della modalit\u00e0 d'uso, \u00e8 forse pi\u00f9 appropriato parlare di LLM (Large Language Models), un sottoinsieme specifico dell'intelligenza artificiale focalizzato sulla comprensione e generazione del linguaggio naturale. Questo sar\u00e0 pertanto il termine pi\u00f9 usato in questa sezione.</p>"},{"location":"ai/#cosa-troverai-in-questa-sezione","title":"Cosa troverai in questa sezione","text":"<p>Saranno presentati strumenti con cui \u00e8 comodo utilizzare gli LLM e l'AI a riga di comando. Ne sono disponibili decine e decine, ma qui ne troverai un ristretto numero scelti a cura della redazione. Se vuoi contribuire, o proporre altri apri una issue o una pull request su GitHub.</p>"},{"location":"charts/","title":"Charts","text":""},{"location":"charts/#hardcoded","title":"Hardcoded","text":"<pre><code>```vegalite\n{\n  \"description\": \"A simple bar chart with embedded data.\",\n  \"data\": {\n    \"values\": [\n      {\"a\": \"A\", \"b\": 28}, {\"a\": \"B\", \"b\": 55}, {\"a\": \"C\", \"b\": 43},\n      {\"a\": \"D\", \"b\": 91}, {\"a\": \"E\", \"b\": 81}, {\"a\": \"F\", \"b\": 53},\n      {\"a\": \"G\", \"b\": 19}, {\"a\": \"H\", \"b\": 87}, {\"a\": \"I\", \"b\": 52}\n    ]\n  },\n  \"mark\": {\"type\": \"bar\", \"tooltip\": true},\n  \"encoding\": {\n    \"x\": {\"field\": \"a\", \"type\": \"nominal\", \"axis\": {\"labelAngle\": 0}},\n    \"y\": {\"field\": \"b\", \"type\": \"quantitative\"}\n  }\n}\n```\n</code></pre> <pre><code>{\n  \"description\": \"A simple bar chart with embedded data.\",\n  \"data\": {\n    \"values\": [\n      {\"a\": \"A\", \"b\": 28}, {\"a\": \"B\", \"b\": 55}, {\"a\": \"C\", \"b\": 43},\n      {\"a\": \"D\", \"b\": 91}, {\"a\": \"E\", \"b\": 81}, {\"a\": \"F\", \"b\": 53},\n      {\"a\": \"G\", \"b\": 19}, {\"a\": \"H\", \"b\": 87}, {\"a\": \"I\", \"b\": 52}\n    ]\n  },\n  \"mark\": {\"type\": \"bar\", \"tooltip\": true},\n  \"encoding\": {\n    \"x\": {\"field\": \"a\", \"type\": \"nominal\", \"axis\": {\"labelAngle\": 0}},\n    \"y\": {\"field\": \"b\", \"type\": \"quantitative\"}\n  }\n}\n</code></pre>"},{"location":"charts/#da-json","title":"DA JSON","text":"<pre><code>```vegalite\n{\n  \"description\": \"A simple bar chart with embedded data.\",\n  \"data\": {\"url\" : \"data/basic_bar_chart.json\"},\n  \"mark\": {\"type\": \"bar\", \"tooltip\": true},\n  \"encoding\": {\n    \"x\": {\"field\": \"a\", \"type\": \"nominal\", \"axis\": {\"labelAngle\": 0}},\n    \"y\": {\"field\": \"b\", \"type\": \"quantitative\"}\n  }\n}\n```\n</code></pre> <pre><code>{\n  \"description\": \"A simple bar chart with embedded data.\",\n  \"data\": {\"url\" : \"data/basic_bar_chart.json\"},\n  \"mark\": {\"type\": \"bar\", \"tooltip\": true},\n  \"encoding\": {\n    \"x\": {\"field\": \"a\", \"type\": \"nominal\", \"axis\": {\"labelAngle\": 0}},\n    \"y\": {\"field\": \"b\", \"type\": \"quantitative\"}\n  }\n}\n</code></pre>"},{"location":"charts/#da-csv","title":"DA CSV","text":"<pre><code>```vegalite\n{\n  \"description\": \"A simple bar chart with embedded data.\",\n  \"data\": {\"url\" : \"data/basic_bar_chart.csv\"},\n  \"mark\": {\"type\": \"bar\", \"tooltip\": true},\n  \"encoding\": {\n    \"x\": {\"field\": \"a\", \"type\": \"nominal\", \"axis\": {\"labelAngle\": 0}},\n    \"y\": {\"field\": \"b\", \"type\": \"quantitative\"}\n  }\n}\n```\n</code></pre> <pre><code>{\n  \"description\": \"A simple bar chart with embedded data.\",\n  \"data\": {\"url\" : \"data/basic_bar_chart.csv\"},\n  \"mark\": {\"type\": \"bar\", \"tooltip\": true},\n  \"encoding\": {\n    \"x\": {\"field\": \"a\", \"type\": \"nominal\", \"axis\": {\"labelAngle\": 0}},\n    \"y\": {\"field\": \"b\", \"type\": \"quantitative\"}\n  }\n}\n</code></pre>"},{"location":"cli/","title":"La CLI","text":"<p>La riga di comando - in inglese CLI, la Command Line Intereface - \u00e8 un'interfaccia di dialogo testuale tra utente e computer, che interpreta i comandi inseriti da tastiera e li esegue. \u00c8 utilizzabile anche tramite script.</p> <p></p> <p>Esistono decine di interfacce a riga di comando, per tutti i sistemi operativi. Qui si far\u00e0 riferimento a una generica CLI, in un sistema operativo Linux. \u00c8 possibile attivarla in tutti i sistemi operativi: Linux (ovviamente), Mac OS (che \u00e8 basato su un sistema operativo Unix), Windows (il modo consigliato per Win \u00e8 Windows Subsystem for Linux), ChromeOS, ecc..</p> <p>La CLI (chiamata anche <code>shell</code>) inizialmente pu\u00f2 intimorire un po', ma pu\u00f2 essere un'ancora di salvezza per tantissime operazioni/elaborazioni.</p> <p>Esistono diversi sistemi di shell (distinti, ma ampiamente compatibili tra loro), tra i quali il pi\u00f9 popolare \u00e8 Bash, acronimo di \"Bourne again shell\". Bash \u00e8 sia una raccolta di utility (come <code>grep</code>, un potente strumento per fare ricerche di testo), che un linguaggio di programmazione, con i suoi <code>for loop</code> per ripetere operazioni su pi\u00f9 file.</p> <p>Molte discipline computazionali, come la bioinformatica, fanno affidamento sulla riga di comando. Ma tutte le persone che utilizzano un personal computer possono trarne vantaggio, perch\u00e9 il mouse non \u00e8 scalabile, mentre con la tastiera si pu\u00f2 sollevare il mondo \ud83d\ude43.</p> <p>Alcuni esempi.</p>"},{"location":"cli/#operazioni-su-piu-file-in-blocco","title":"Operazioni su pi\u00f9 file in blocco","text":"<p>Forse la caratteristica pi\u00f9 potente della shell \u00e8 la capacit\u00e0 di ripetere semplici processi su pi\u00f9 file. Una persona potrebbe, ad esempio, rinominare sistematicamente i propri file ed aggiungere al loro nome la data, o convertirli da un formato all'altro.</p> <p>Elvira Almiraghi, per i suoi studi di post-dottorato, aveva la necessit\u00e0 di aprire centinaia di immagini presenti in una cartella, invertirne i colori e modificarne anche luminosit\u00e0, saturazione e tonalit\u00e0. Operazioni di questo tipo, con un mouse, possono durare ore. Elvira, con la riga di comando e sfruttando ImageMagick - una utility dedicata alle immagini - ha semplificato tutto in queste righe:</p> <pre><code>for file in *.png; do\n  convert $file -channel RGB -negate -modulate 100,100,200 out_$file\ndone\n</code></pre>"},{"location":"cli/#gestire-file-di-grandi-dimensioni","title":"Gestire file di grandi dimensioni","text":"<p>Alcuni file di testo strutturato hanno dimensioni che li rendono inutilizzabili con la gran parte del software.</p> <p>Pasubio Giovinezza, un esperto di DNA, si trova giornalmente ad esempio a gestire file con milioni di varianti genetiche, che non si possono aprire con un foglio elettronico, perch\u00e9 in questo tipo di applicazioni le dimensioni di un foglio sono limitate a circa 1 milione di righe. Con la riga di comando e con le sue utility (come <code>awk</code> e <code>cut</code>), Pasubio  pu\u00f2 filtrare dai suoi file \"grandi\" solo le righe che superano una certa soglia di qualit\u00e0, estrarne soltanto alcune colonne e salvare tutto in un nuovo file. Aassumendo che il punteggio di qualit\u00e0 sia nella colonna 4, che la soglia sia a 50 e le colonne desiderate sono 1\u20134:</p> <pre><code>awk -F, '{ if ($4 &gt; 50) print $0 }' datafile.csv | cut -d, -f1-4 &gt; newdatafile.csv\n</code></pre>"},{"location":"cli/#mettere-in-fila-piu-operazioni-la-magia-del-pipe","title":"Mettere in fila pi\u00f9 operazioni: la magia del \"pipe\"","text":"<p>I comandi della shell nascono per fare una sola cosa e bene. Ad esempio <code>cut</code> estrae una o pi\u00f9 colonne da una tabella, <code>wc</code> conta le parole, le righe o i caratteri di un file, <code>awk</code> applica dei filtri alle righe basati su una o pi\u00f9 condizioni, e <code>sed</code> modifica flussi di stringhe di testo. Questi comandi possono essere messi tutti insieme, utilizzando il <code>|</code> (il \"pipe\", il tubo), una funzionalit\u00e0 della shell che incanala l'output di un comando verso un altro. Giorgio Perozzi, un amministratore di sistema, dice che \"il <code>|</code> d\u00e0 la possibilit\u00e0 di trovare rapidamente soluzioni a problemi quotidiani e prototiparne rapidamente tante\".</p> <p>Il comando di sotto, per contare i nomi univoci di geni da una sequenza, avrebbe fatto comodo a Pasubio:</p> <pre><code>cut -f1 GEOdataset.csv | sed -E 's/^&gt;//' | sort | uniq | wc -l\n</code></pre> <p>Questo \u00e8 quello che fa il comando soprastante: estrae la prima prima colonna (<code>cut</code>), rimuove il carattere <code>&gt;</code> eventualmente presente a inizio riga (<code>sed</code>), ordina il tutto alfabeticamente (<code>sort</code>), estrae i valori univoci (<code>uniq</code>) e stampa il numero di righe di output (<code>wc</code>).</p>"},{"location":"cli/#eseguire-processi-in-parallelo","title":"Eseguire processi in parallelo","text":"<p>Valentina Gherardini, lavora in un centro di calcolo che fornisce accesso remoto a circa 14.000 nodi, verso terabyte di dati. Supponiamo, dice la Gherardini, che un geofisico debba svolgere un flusso di lavoro computazionale per analizzare set di dati sismici, che ogni set richieda un giorno per essere elaborato sul proprio computer e che il ricercatore abbia 60 di questi set di dati. \"Sono due mesi ininterrotti di elaborazione\", dice. Ma, inviando il lavoro a un cluster di computer, in connessione sicura con <code>ssh</code>, aprendo un portale crittografato da remoto, il ricercatore pu\u00f2 parallelizzare i suoi calcoli su 60 computer. \"Invece di due mesi, ci vuole un giorno.\"</p> <p>La parallelizzazione dei processi, si realizza in modo ricco ed efficiente nella shell. Una utility dedicata \u00e8 <code>parallel</code>.</p>"},{"location":"cli/#automatizzare-i-processi","title":"Automatizzare i processi","text":"<p>I comandi della shell possono essere archiviati in file di testo chiamati script, che possono essere salvati, condivisi e sottoposti a versioning, migliorandone la riproducibilit\u00e0. Possono anche essere automatizzati. Utilizzando il comando <code>cron</code>, gli utenti possono pianificare l'esecuzione degli script. Ad esempio, afferma Perozzi, alcuni siti Web suggeriscono agli utenti che intendono fare scraping/download dei loro contenuti, di farlo in orari \"notturni\" - ad esempio tra le 21:00 e le 5:00. \"Puoi far eseguire [lo script] solo negli orari suggeriti\". Un esempio \u00e8 quello del \"NCBI National Center for Biotechnology Information\", in cui nelle linee guida si legge \"Run retrieval scripts on weekends or between 9 pm and 5 am Eastern Time weekdays for any series of more than 100 requests.\"</p>"},{"location":"grazie/","title":"Grazie","text":"<ul> <li>Gabriele</li> <li>John</li> <li>GDAL/OGR</li> <li>Jeroen Janssens</li> </ul>"},{"location":"letture/","title":"Letture consigliate","text":""},{"location":"letture/#libri","title":"Libri","text":"<p>Questi testi sono stati un riferimento per arrivare allo sviluppo di questo progetto; in particolare la I versione del volume di Jeroen Janssens (che ringraziamo molto), di cui nel 2021 \u00e8 stata pubblicata la II versione:</p> <ul> <li>Janssens, Jeroen. Data Science at the Command Line, 2e. https://www.datascienceatthecommandline.com/2e/</li> <li>Vince, Buffalo. Bioinformatics Data Skills. https://www.oreilly.com/library/view/bioinformatics-data-skills/9781449367480/</li> <li>William, Shotts. The Linux Command Line. https://linuxcommand.org/tlcl.php</li> <li>Robert Mesibov. A data cleaner's cookbook. https://www.datafix.com.au/cookbook/</li> </ul>"},{"location":"letture/#siti","title":"Siti","text":"<ul> <li>Bashing data, https://www.datafix.com.au/BASHing/index.html</li> <li>The Programming Historian, https://programminghistorian.org/en/</li> <li>explain shell, https://explainshell.com/</li> </ul>"},{"location":"letture/#spunti","title":"Spunti","text":"<ul> <li>Five reasons why researchers should learn to love the command line (NdR: \u00e8 stato utilizzato per scrivere gran parte della pagina CLI), https://www.nature.com/articles/d41586-021-00263-0</li> <li>Nine simple ways to make it easier to (re)use your data, https://ojs.library.queensu.ca/index.php/IEE/article/view/4608</li> </ul>"},{"location":"monografie/","title":"Monografie","text":"<p>In questa sezione, delle guide dedicate ad alcuni  strumenti per la lettura, trasformazione, descrizione e analisi di file di testo strutturati, molto consigliati:</p> <ul> <li>Miller (\u2692\ufe0f in costruzione);</li> <li>xan (\u2692\ufe0f introduzione);</li> <li>VisiData;</li> <li>frictionless CLI \u2692\ufe0f in costruzione;</li> <li>ckanapi CLI \u2692\ufe0f in costruzione;</li> <li>LLM \u2692\ufe0f in costruzione.</li> </ul>"},{"location":"partecipa/","title":"Partecipa","text":"<p>Se ti piace  a riga di comando e vuoi dare una mano, puoi farlo in queste modalit\u00e0:</p> <ul> <li>segnalare qualcosa da correggere;</li> <li>proporre una nuovo paragrafo, pagina, sezione, ecc..</li> </ul> <p>Lo spazio per farlo sono le discussioni su GitHub .</p>"},{"location":"tags/","title":"\ud83c\udff7\ufe0f Tags","text":"<p>L'elenco dei tag usati</p> <p>[TAGS]</p>"},{"location":"ai/mcp-consigliati/","title":"MCP consigliati","text":"<p>\ud83d\udea7 Pagina in costruzione</p> <p>Questa pagina \u00e8 ancora in lavorazione. Il contenuto \u00e8 incompleto e non revisionato.</p>"},{"location":"ai/mcp-consigliati/#mcp-consigliati","title":"MCP consigliati","text":"<p>Una selezione di server MCP utili in contesti pratici (documentazione, codice, automazione e ricerca).</p> Titolo Breve descrizione Per cosa \u00e8 utile URL DeepWiki MCP Collega l'agente alla documentazione indicizzata di repository e progetti. Capire rapidamente una codebase, trovare funzioni/API e fare onboarding tecnico. https://docs.devin.ai/work-with-devin/deepwiki-mcp Exa MCP Espone la ricerca web e il retrieval di contenuti tramite Exa in formato adatto agli agenti. Ricerca tecnica, raccolta fonti e recupero di pagine/documenti recenti. https://exa.ai/docs/reference/exa-mcp"},{"location":"ai/altri-tool/","title":"Altri strumenti","text":"<p>\ud83d\udea7 Pagina in costruzione</p> <p>Questa pagina \u00e8 ancora in lavorazione. Il contenuto \u00e8 incompleto e non revisionato.</p>"},{"location":"ai/altri-tool/#altri-strumenti","title":"Altri strumenti","text":"<p>In questa sezione una lista di altri strumenti a riga di comando che in modo diretto o indiretto permettono di utilizzare gli LLM e l'AI a riga di comando.</p>"},{"location":"ai/altri-tool/#aider","title":"Aider","text":"<p>Aider \u00e8 uno strumento di programmazione assistita che funziona all'interno del terminale, basato sui modelli di linguaggio di grandi dimensioni (LLM).</p>"},{"location":"ai/altri-tool/#codex-cli","title":"Codex CLI","text":"<p>https://github.com/openai/codex</p>"},{"location":"ai/altri-tool/#crawl4ai","title":"Crawl4AI","text":"<p>https://docs.crawl4ai.com/core/cli/</p>"},{"location":"ai/altri-tool/#docling","title":"Docling","text":"<p>https://docling-project.github.io/docling/</p>"},{"location":"ai/altri-tool/#files-to-prompt","title":"files-to-prompt","text":"<p>https://github.com/simonw/files-to-prompt</p>"},{"location":"ai/altri-tool/#gptme","title":"gptme","text":"<p>https://gptme.org/docs/index.html</p>"},{"location":"ai/altri-tool/#markitdown","title":"MarkItDown","text":"<p>Pagina dedicata</p>"},{"location":"ai/altri-tool/#skillssh","title":"skills.sh","text":"<p>skills.sh \u00e8 una CLI per installare skill sui tool AI a riga di comando che usi.</p> <p>Pagina dedicata</p>"},{"location":"ai/altri-tool/#shot-scraper","title":"shot-scraper","text":"<p>https://github.com/simonw/shot-scraper</p>"},{"location":"ai/altri-tool/#strip-tags","title":"strip-tags","text":"<p>https://github.com/simonw/strip-tags</p>"},{"location":"ai/altri-tool/#yek","title":"yek","text":"<p>https://github.com/bodo-run/yek</p>"},{"location":"ai/altri-tool/markitdown/","title":"MarkItDown","text":"<p>MarkItDown \u00e8 una utility a riga di comando sviluppata da Microsoft per convertire file Markdown in HTML, PDF e altri formati, con supporto avanzato per la formattazione e l'integrazione di contenuti multimediali.</p> <p>E visto che per gli <code>LLM</code> il testo piano \u00e8 il formato di input base, \u00e8 prezioso e propedeutico poter convertire un PDF, un file Word, un Power Point, ecc. in testo piano <code>markdown</code> per poterlo poi processare con gli <code>LLM</code>.</p>"},{"location":"ai/altri-tool/markitdown/#installazione","title":"Installazione","text":"<p>MarkItDown si installa come tool Python. Puoi scegliere tra installazione diretta o ambiente isolato:</p> comando | pip (tutto incluso)comando | uv tool (ambiente isolato) <pre><code>pip install 'markitdown[all]'\n</code></pre> <pre><code>uv tool install 'markitdown[all]'\n</code></pre>"},{"location":"ai/altri-tool/markitdown/#esempi-di-utilizzo","title":"Esempi di utilizzo","text":"<p>Per convertire un file PDF in <code>markdown</code>:</p> <pre><code>markitdown path-to-file.pdf &gt; document.md\n</code></pre> <p>Per convertire una pagina web in <code>markdown</code>:</p> <pre><code>markitdown https://example.com &gt; page.md\n</code></pre>"},{"location":"ai/altri-tool/skills-sh/","title":"skills.sh","text":"<p>skills.sh (di Vercel) \u00e8 una CLI per installare e gestire skill su tutti i tool AI CLI che usi, ad esempio Claude Code, OpenCode, Codex, Cursor e altri.</p> <p>Documentazione ufficiale: https://skills.sh/docs</p>"},{"location":"ai/altri-tool/skills-sh/#installare-una-skill-da-un-repository","title":"Installare una skill da un repository","text":"<p>Esempio:</p> <pre><code>npx skills add ondata/skills --skill openalex\n</code></pre>"},{"location":"ai/altri-tool/skills-sh/#installare-una-skill-da-una-cartella-locale","title":"Installare una skill da una cartella locale","text":"<p>Esempio:</p> <pre><code>npx skills add ./skills/ondata-discussions\n</code></pre>"},{"location":"ai/altri-tool/skills-sh/#durante-linstallazione","title":"Durante l'installazione","text":"<p>In modalit\u00e0 interattiva, durante l'installazione puoi scegliere:</p> <ol> <li>su quale tool installare la skill (per esempio Claude Code, OpenCode, Codex);</li> <li>se installarla come skill di progetto (default) o globale/locale utente (<code>-g</code>).</li> </ol>"},{"location":"ai/claude-code/","title":"Claude Code da riga di comando","text":"<p>Claude Code \u00e8 un assistente per lavorare con file, codice e testi direttamente dal terminale. Qui trovi i comandi essenziali per usarlo in modo veloce quando ti serve \u201cricordare come si fa\u201d.</p>"},{"location":"ai/claude-code/#come-si-lancia","title":"Come si lancia","text":"<pre><code>claude\n</code></pre>"},{"location":"ai/claude-code/#riprendi-lultima-sessione","title":"Riprendi l'ultima sessione","text":"<pre><code>claude -c\n</code></pre>"},{"location":"ai/claude-code/#scegli-una-conversazione-recente","title":"Scegli una conversazione recente","text":"<pre><code>claude -r\n</code></pre> <p>Mostra un picker con le sessioni recenti: puoi scegliere quella che ti interessa dalla lista.</p> <pre><code>  [Request interrupted by user for tool use]\n  23 minutes ago \u00b7 main \u00b7 1.5 MB\n\n  [Request interrupted by user for tool use]\n  50 minutes ago \u00b7 main \u00b7 1.1 MB\n\n  guarda gli aggiornamenti di oggi al codice, e aggiornara docs/evaluation-v1.2.13.md\n  17 hours ago \u00b7 main \u00b7 924.5 KB\n</code></pre> <p>Puoi anche riprendere direttamente una sessione per ID o nome:</p> <pre><code>claude -r \"auth-refactor\" \"Completa questa PR\"\n</code></pre>"},{"location":"ai/claude-code/#esegui-un-prompt-e-esci","title":"Esegui un prompt e esci","text":"<pre><code>claude -p \"analizza questo codice\"\n</code></pre>"},{"location":"ai/claude-code/#output-json-per-script","title":"Output JSON per script","text":"<p>Utile per automazioni e pipeline.</p> <pre><code>claude -p --output-format json \"trova tutti i file CSV\"\n</code></pre> <p>Per specificare un modello diverso:</p> <pre><code>claude -p --model sonnet --output-format json \"trova tutti i file CSV\"\n</code></pre>"},{"location":"ai/claude-code/#altri-parametri-utili","title":"Altri parametri utili","text":"<pre><code># Aggiungi directory di lavoro extra\nclaude --add-dir ../apps ../lib\n\n# Aggiungi istruzioni al prompt di sistema\nclaude --append-system-prompt \"Usa sempre TypeScript\"\n\n# Aggiungi istruzioni al prompt di sistema da file\nclaude --append-system-prompt-file prompt.md\n\n# Sostituisci il prompt di sistema\nclaude --system-prompt \"Sei un esperto Python\"\n\n# Sostituisci il prompt di sistema da file\nclaude --system-prompt-file system.md\n\n# Limita gli strumenti disponibili\nclaude --tools \"Bash,Edit,Read\"\n\n# Log verboso\nclaude --verbose\n\n# Output streaming JSON\nclaude -p --output-format stream-json \"analizza questi log\"\n\n# Output streaming JSON includendo i messaggi parziali\nclaude -p --output-format stream-json --include-partial-messages \"analizza questi log\"\n\n# Usa un agente specifico\nclaude --agent planner\n\n# Elenca gli agenti disponibili\nclaude --agents\n\n# Aggiorna Claude Code\nclaude update\n</code></pre>"},{"location":"ai/claude-code/#processa-input-da-pipe","title":"Processa input da pipe","text":"<pre><code>cat errors.log | claude -p \"analizza questi errori\"\n</code></pre> <p>Esempio minimale:</p> <pre><code>cat logs.txt | claude -p \"explain\"\n</code></pre>"},{"location":"ai/claude-code/configurazione/","title":"Configurazione Claude Code","text":"<p>Qui troverai note utili sulla configurazione di Claude Code.</p>"},{"location":"ai/claude-code/configurazione/#server-mcp-globali","title":"Server MCP globali","text":"<p>I server MCP (Model Context Protocol) globali si configurano, aggiungono e cancellano nel file:</p> <pre><code>~/.claude.json\n</code></pre> <p>Questo file contiene la configurazione di tutti i server MCP disponibili per Claude Code a livello di sistema.</p>"},{"location":"ai/claude-code/configurazione/#aggiunta-globale-da-riga-di-comando","title":"Aggiunta globale da riga di comando","text":"<p>Per aggiungere un server MCP globale (scope utente), puoi usare:</p> <pre><code>claude mcp add -s user -t http mospi-statistics https://mcp.mospi.gov.in\n</code></pre> <p>In questo esempio, <code>-s user</code> indica che il server viene aggiunto alla configurazione globale dell'utente.</p>"},{"location":"ai/claude-code/configurazione/#permessi-consentire-comandi-senza-conferma","title":"Permessi: consentire comandi senza conferma","text":"<p>Per impostazione predefinita, Claude Code chiede conferma prima di eseguire certi comandi. Puoi sbloccare quelli che usi spesso (ad esempio i comandi di lettura) in modo che vengano eseguiti automaticamente.</p>"},{"location":"ai/claude-code/configurazione/#configurazione-globale","title":"Configurazione globale","text":"<p>Modifica (o crea) il file <code>~/.claude/settings.json</code> e aggiungi i comandi nella sezione <code>permissions.allow</code>:</p> <pre><code>{\n  \"permissions\": {\n    \"allow\": [\n      \"Bash(ls:*)\",\n      \"Bash(grep:*)\",\n      \"Bash(wc:*)\"\n    ]\n  }\n}\n</code></pre> <p>Il pattern \u00e8 <code>Bash(comando:*)</code> \u2014 il <code>*</code> significa \"con qualsiasi argomento\".</p> <p>Se il file esiste gi\u00e0, puoi aggiungere le voci da terminale con <code>jq</code>:</p> <pre><code>jq '.permissions.allow += [\"Bash(ls:*)\", \"Bash(grep:*)\"]' ~/.claude/settings.json | sponge ~/.claude/settings.json\n</code></pre> <p>Note</p> <p><code>sponge</code> fa parte del pacchetto <code>moreutils</code>. In alternativa, salva l'output su un file temporaneo e rinomina.</p>"},{"location":"ai/claude-code/configurazione/#configurazione-a-livello-di-progetto","title":"Configurazione a livello di progetto","text":"<p>Puoi creare un file <code>.claude/settings.json</code> nella root del progetto con la stessa struttura:</p> <pre><code>{\n  \"permissions\": {\n    \"allow\": [\n      \"Bash(ls:*)\",\n      \"Bash(grep:*)\"\n    ]\n  }\n}\n</code></pre> <p>Vale solo per quel progetto e pu\u00f2 essere committato nel repository, utile per condividere le impostazioni con il team.</p>"},{"location":"ai/codex/","title":"Codex CLI","text":"<p>Codex \u00e8 uno strumento da riga di comando per interagire con modelli AI direttamente dal terminale.</p>"},{"location":"ai/codex/#installazione","title":"Installazione","text":"<pre><code>npm install -g @microsoft/codex-cli\n</code></pre>"},{"location":"ai/codex/#utilizzo-base","title":"Utilizzo base","text":"<pre><code>codex \"spiega questo comando: tar -xzf file.tar.gz\"\n</code></pre>"},{"location":"ai/codex/#caratteristiche-principali","title":"Caratteristiche principali","text":"<ul> <li>Esecuzione di query dirette a modelli AI</li> <li>Supporto per diverse API (OpenAI, Azure OpenAI, etc.)</li> <li>Integrazione con la shell per automazioni</li> </ul>"},{"location":"ai/codex/configurazione/","title":"Configurazione Codex CLI","text":"<p>Qui troverai note utili sulla configurazione di Codex CLI.</p>"},{"location":"ai/codex/configurazione/#server-mcp-globali","title":"Server MCP globali","text":"<p>I server MCP (Model Context Protocol) globali si configurano, aggiungono e cancellano nel file:</p> <pre><code>~/.codex/config.toml\n</code></pre> <p>Questo file contiene la configurazione di tutti i server MCP disponibili per Codex CLI a livello di sistema.</p>"},{"location":"ai/estrarre-pagine-web/","title":"Estrarre testo da pagine web","text":"<p>\ud83d\udea7 Pagina in costruzione</p> <p>Questa pagina \u00e8 ancora in lavorazione. Il contenuto \u00e8 incompleto e non revisionato.</p>"},{"location":"ai/estrarre-pagine-web/#estrarre-testo-da-pagine-web","title":"Estrarre testo da pagine web","text":""},{"location":"ai/estrarre-pagine-web/#scrape-cli","title":"scrape-cli","text":""},{"location":"ai/estrarre-pagine-web/#strip-tags","title":"strip-tags","text":""},{"location":"ai/estrarre-pagine-web/#trafilatura","title":"Trafilatura","text":""},{"location":"ai/llm_cli/","title":"LLM CLI","text":"<p>\ud83d\udea7 Pagina in costruzione</p> <p>Questa pagina \u00e8 ancora in lavorazione. Il contenuto \u00e8 incompleto e non revisionato.</p>"},{"location":"ai/llm_cli/#introduzione","title":"Introduzione","text":"<p>LLM \u00e8 un'utility a riga di comando e una libreria Python per interagire con modelli di linguaggio di grandi dimensioni (LLM) come quelli di OpenAI, Anthropic, Google, Meta e tanti altri, compresi quelli installati in locale.</p> <p>\u00c8 scritta in <code>Python</code> da quel genio di Simon Willison, il creatore di Datasette.</p> <p>La guida ufficiale \u00e8 scritta molto bene e questa non vuole essere una sua traduzione, ma una raccolta di appunti e suggerimenti per utilizzarla.</p>"},{"location":"ai/llm_cli/#installazione","title":"Installazione","text":"<p>\u00c8 una libreria Python, quindi si pu\u00f2 installare con <code>pip</code>:</p> <pre><code>pip install llm\n</code></pre> <p>\u00c8 molto comodo usare <code>uv</code> per installare librerie e CLI Python:</p> <pre><code>uv tool install llm\n</code></pre>"},{"location":"ai/llm_cli/#primo-utilizzo","title":"Primo utilizzo","text":"<p>Se si vuole utilizzare ad esempio con le API di OpenAI, bisogna recuperare una API Key da OpenAI, aprire la shell e impostarla per <code>llm</code> con il comando:</p> <pre><code>llm keys set openai\n</code></pre> <p>Dopo aver impostato la API Key, si pu\u00f2 iniziare a chattare con il modello di linguaggio:</p> <pre><code>llm 'Crea 5 nomi per un ristorante italiano'\n</code></pre> <p>E in output si otterr\u00e0 qualcosa del genere (notare che il testo \u00e8 in formato Markdown):</p> <pre><code>Ecco 5 nomi per un ristorante italiano, con un piccolo commento su ognuno:\n\n1.  **Trattoria della Nonna Emilia:** (Tradizione, familiarit\u00e0, un nome di una nonna immaginaria evoca calore e autenticit\u00e0). Perfetto per un ristorante con cucina tradizionale e casalinga.\n2.  **Il Giardino Segreto:** (Romantico, misterioso, evoca un'atmosfera intima e speciale). Ottimo per un ristorante con un bel giardino o un'attenzione particolare all'ambiente.\n3.  **Aromi di Sicilia:** (Geografico, specifico, focalizza l'attenzione su una regione ricca di sapori). Ideale se il ristorante si specializza in cucina siciliana.\n4.  **Via Veneto Ristorante:** (Classico, elegante, riferimento a una strada iconica italiana). Adatto per un ristorante che punta a un'esperienza culinaria di lusso e raffinatezza.\n5.  **Pasta e Passione:** (Semplice, diretto, esprime i due elementi fondamentali della cucina italiana). Un nome memorabile e accattivante, adatto a un ristorante con un focus sulla pasta fresca e fatta in casa.\n</code></pre> <p>Per utilizzare <code>llm</code> in modo pi\u00f9 interattivo, si pu\u00f2 avviare una sessione di chat:</p> <pre><code>llm chat\n</code></pre> <p>\ud83d\udc49 Per approfondire l'utilizzo di <code>llm</code>, c'\u00e8 una sezione dedicata.</p>"},{"location":"ai/llm_cli/#aggiornamento","title":"Aggiornamento","text":"<p>Si pu\u00f2 fare in diversi modi. Quello generico \u00e8:</p> <pre><code>llm install -U llm\n</code></pre>"},{"location":"ai/llm_cli/#plugin","title":"Plugin","text":"<p>Esistono diversi plugin per <code>llm</code>. Con questi \u00e8 possibile aggiungere altri modelli di <code>LLM</code> con cui interagire e/o attivare funzionalit\u00e0 aggiuntive.</p>"},{"location":"ai/llm_cli/#installazione_1","title":"Installazione","text":"<pre><code>llm install nome_plugin\n</code></pre>"},{"location":"ai/llm_cli/#aggiornamento-plugin","title":"Aggiornamento plugin","text":"<pre><code>llm install nome_plugin --upgrade\n</code></pre>"},{"location":"ai/llm_cli/#configurazione","title":"Configurazione","text":""},{"location":"ai/llm_cli/#salvataggio-e-utilizzo-delle-api-key","title":"Salvataggio e utilizzo delle API Key","text":"<pre><code>llm keys set nome_provider\n</code></pre> <p>Ad esempio per openai sar\u00e0:</p> <pre><code>llm keys set openai\n</code></pre> <p>Il file con la lista delle Key si ricava in questo modo:</p> <pre><code>llm keys path\n</code></pre> <p>La lista delle Key invece si ottiene con:</p> <pre><code>llm keys\n</code></pre>"},{"location":"ai/llm_cli/#lista-dei-modelli","title":"Lista dei modelli","text":"<pre><code>llm models list\n</code></pre>"},{"location":"ai/llm_cli/#impostare-un-modello-di-default","title":"Impostare un modello di default","text":"<pre><code>llm models default nome_modello\n</code></pre>"},{"location":"ai/llm_cli/modelli/","title":"Modelli","text":"<p>\ud83d\udea7 Pagina in costruzione</p> <p>Questa pagina \u00e8 ancora in lavorazione. Il contenuto \u00e8 incompleto e non revisionato.</p>"},{"location":"ai/llm_cli/modelli/#i-modelli","title":"I modelli","text":"<p>Con <code>llm</code> cli puoi utilizzare la gran parte dei modelli oggi disponibili, sia quelli esposti tramite API su server remoti - quelli di OpenAI, Anthropic, Google, ecc. - sia quelli installati localmente.</p> <p>La lista dei modelli disponibili si ottiene con il comando:</p> <pre><code>llm models list\n</code></pre> <p>Si pu\u00f2 estendere il numero dei modelli disponibili tramite plugin.</p>"},{"location":"ai/llm_cli/plugin/","title":"Plugin","text":"<p>\ud83d\udea7 Pagina in costruzione</p> <p>Questa pagina \u00e8 ancora in lavorazione. Il contenuto \u00e8 incompleto e non revisionato.</p>"},{"location":"ai/llm_cli/plugin/#i-plugin","title":"I plugin","text":"<p>I plugin di <code>llm</code> estendono le sue funzionalit\u00e0. La gran parte viene utilizzata per aggiungere modelli di intelligenza artificiale.</p>"},{"location":"ai/llm_cli/utilizzo/","title":"Utilizzo","text":"<p>\ud83d\udea7 Pagina in costruzione</p> <p>Questa pagina \u00e8 ancora in lavorazione. Il contenuto \u00e8 incompleto e non revisionato.</p>"},{"location":"ai/llm_cli/utilizzo/#utilizzo","title":"Utilizzo","text":"<p>Il modello di default \u00e8 <code>gpt-4o-mini</code> e quindi per il primo utilizzo di <code>llm</code> \u00e8 necessario impostare un'API Key per OpenAI, con il comando:</p> <pre><code>llm keys set openai\n</code></pre> <p>Nota</p> <p>Non \u00e8 obbligatorio usare <code>gpt-4o-mini</code> come modello di default, ma \u00e8 consigliato per iniziare, perch\u00e9 in linea con la documentazione e le guide di <code>llm</code>.</p>"},{"location":"ai/llm_cli/utilizzo/#lanciare-un-prompt","title":"Lanciare un prompt","text":"<p>Un prompt \u00e8 una richiesta, un messaggio, che si invia a un modello LLM per ottenere una risposta. \u00c8 il modo per \"comunicare\" con l'intelligenza artificiale.</p> <p>Per lanciarlo basta scriverlo nella shell, dopo <code>llm</code>, come in questo esempio:</p> <pre><code>llm \"Crea 5 nomi per un ristorante italiano\"\n</code></pre> <p>Guida al Prompt Engineering</p> <p>Il Prompt engineering \u00e8 una disciplina relativamente nuova che consente di sviluppare e ottimizzare i prompt per utilizzare in modo efficiente i modelli linguistici (LM) per un'ampia variet\u00e0 di applicazioni e argomenti di ricerca. Per approfondire puoi iniziare da questa guida dedicata.</p> <p>Se vuoi usare un altro modello, prima guarda la lista dei modelli disponibili sulla tua macchina con <code>llm models list</code>. Avrai in output una lista come questa:</p> <pre><code>Available models:\nAnthropic Messages: anthropic/claude-3-7-sonnet-latest (aliases: claude-3.7-sonnet, claude-3.7-sonnet-latest)\nAnthropic Messages: anthropic/claude-3-haiku-20240307 (aliases: claude-3-haiku)\nGeminiPro: gemini-2.0-flash-thinking-exp-01-21\ngpt4all: all-MiniLM-L6-v2-f16 - SBert, 43.76MB download, needs 1GB RAM\ngpt4all: qwen2-1_5b-instruct-q4_0 - Qwen2-1.5B-Instruct, 894.10MB download, needs 3GB RAM\nLLMGroq: groq/meta-llama/llama-4-scout-17b-16e-instruct\nLLMGroq: groq/whisper-large-v3\nOpenAI Chat: gpt-4.1 (aliases: 4.1)\nPerplexity: r1-1776\nPerplexity: sonar-reasoning-pro\n</code></pre> <p>Se vuoi usare ad esempio <code>gemini-2.0-flash-thinking-exp-01-21</code>, puoi farlo l'opzione <code>-m</code> seguito dal nome del modello:</p> <pre><code>llm -m gemini-2.0-flash-thinking-exp-01-21 \"Crea 5 nomi per un ristorante italiano\"\n</code></pre>"},{"location":"ai/llm_cli/utilizzo/#opzioni-dei-modelli","title":"Opzioni dei modelli","text":"<p>I modelli LLM hanno diverse opzioni che puoi configurare per personalizzare le risposte. Una delle pi\u00f9 comuni \u00e8 la temperatura, che controlla la casualit\u00e0 delle risposte generate.</p> <p>Per visualizzare le opzioni disponibili per i modelli installati, puoi usare il comando:</p> <pre><code>llm models options\n</code></pre> <p>E ad esempio per il modello <code>gemini-2.0-flash-exp</code> sono queste:</p> <pre><code>GeminiPro: gemini-2.0-flash-exp\n  Options:\n    code_execution: boolean\n    temperature: float\n    max_output_tokens: int\n    top_p: float\n    top_k: int\n    json_object: boolean\n    google_search: boolean\n  Attachment types:\n    application/ogg, application/pdf, audio/aac, audio/aiff,\n    audio/flac, audio/mp3, audio/mpeg, audio/ogg, audio/wav,\n    image/heic, image/heif, image/jpeg, image/png, image/webp,\n    text/csv, text/plain, video/3gpp, video/avi, video/mov, video/mp4,\n    video/mpeg, video/mpg, video/quicktime, video/webm, video/wmv,\n    video/x-flv\n  Features:\n  - streaming\n  - schemas\n  - async\n  Keys:\n    key: gemini\n    env_var: LLM_GEMINI_KEY\n</code></pre> <p>Questo modello di intelligenza artificiale supporta diverse opzioni che permettono di personalizzare il comportamento della generazione di testo. Eccone alcune:</p> <ul> <li><code>code_execution</code>: se attivato (<code>true</code>), permette al modello di eseguire codice incluso nel prompt. Utile per test, calcoli o demo tecniche.</li> <li><code>temperature</code> (float): controlla la creativit\u00e0 della risposta (per approfondire).</li> <li><code>max_output_tokens</code> (int): imposta il numero massimo di token (parole/parole spezzate) generabili in output. Il limite massimo \u00e8 8192 token.</li> <li><code>json_object</code> (boolean): se attivo, il modello prova a generare direttamente un oggetto JSON come output.</li> <li><code>google_search</code> (boolean): abilita la consultazione di Google per arricchire la risposta con informazioni aggiornate (se supportato dall'interfaccia).</li> </ul> <p>Il modello pu\u00f2 accettare in input file di tipo:</p> <ul> <li>audio (es. mp3, wav)</li> <li>immagine (es. jpeg, png)</li> <li>video (es. mp4, webm)</li> <li>documenti (es. pdf, csv, txt)</li> </ul> <p>Inoltre ha queste funzionalit\u00e0:</p> <ul> <li>streaming: ricevi l'output man mano che viene generato</li> <li>schemas: supporta output strutturati secondo schemi</li> <li>async: supporta richieste asincrone</li> </ul> <p>Per impostare ad esempio la temperatura, si dovr\u00e0 usare l'opzione <code>-o</code> seguito da <code>temperature</code> e dal suo valore. Ad esempio:</p> <pre><code>llm -o temperature 0.1 \"Suggerisci tre titoli per un articolo che spiega perch\u00e9 il software libero \u00e8 importante nelle scuole.\"\n</code></pre>"},{"location":"ai/llm_cli/utilizzo/#conteggio-dei-token-utilizzati","title":"Conteggio dei token utilizzati","text":"<p>Se si aggiunge al comando l'opzione <code>-u</code> si ha restituito in output anche il conteggio dei token utilizzati per il prompt e per la risposta. Ad esempio:</p> <pre><code>llm \"5 nomi di ristoranti italiani\" -u\n\n1.  **La Tavola del Nonno:** (Classico e accogliente, \"La tavola del nonno\" evoca ricordi di famiglia e cucina tradizionale.)\n2.  **Osteria Acquolina:** (Un po' pi\u00f9 rustico e invitante, \"Acquolina\" significa letteralmente \"l'acquolina in bocca,\" promettendo sapori deliziosi.)\n...\n\nToken usage: 5 input, 209 output, {\"candidatesTokenCount\": 209, \"promptTokensDetails\": [{\"modality\": \"TEXT\", \"tokenCount\": 5}], \"candidatesTokensDetails\": [{\"modality\": \"TEXT\", \"tokenCount\": 209}]}\n</code></pre> <p>Oppure si possono leggere i <code>log</code> dei comandi lanciati, ed in output si avr\u00e0 anche il conteggio dei token:</p> <pre><code>llm logs --short --usage\n</code></pre> <p>L'output sar\u00e0 in <code>yaml</code> e simile a questo (qui sotto soltano uno degli elementi del log):</p> <pre><code>- model: gemini-2.0-flash-exp\n  datetime: '2025-06-02T14:12:15'\n  conversation: 01jwrfrtayx55e4k1c3230smab\n  prompt: 5 nomi di ristoranti italiani\n  prompt_fragments: []\n  system_fragments: []\n  usage:\n    input: 5\n    output: 209\n    details:\n      candidatesTokenCount: 209\n      promptTokensDetails:\n      - modality: TEXT\n        tokenCount: 5\n      candidatesTokensDetails:\n      - modality: TEXT\n        tokenCount: 209\n</code></pre> <p>Utility per il conteggio dei token</p> <p>Simon Willison ha sviluppato anche <code>ttok</code>, un'utility a riga di comando per contare i token di un testo o per troncare quel testo a un certo numero di token.</p>"},{"location":"ai/llm_cli/utilizzo/#file-allegati","title":"File allegati","text":"<p>Alcuni modelli LLM sono multi-modali, cio\u00e8 possono accettare in input non solo testo, ma anche file di altri tipi, come immagini, audio o video. Per questi modelli, puoi allegare file al prompt usando l'opzione <code>-a</code> seguita dal percorso del file.</p>"},{"location":"ai/llm_cli/utilizzo/#leggere-da-stdin","title":"Leggere da <code>stdin</code>","text":"<p>Questa \u00e8 una delle funzionalit\u00e0 pi\u00f9 interessanti di <code>llm</code> nell'utilizzo a riga di comando: puoi leggere da <code>stdin</code> e quindi passare il testo a <code>llm</code> da un file o da un comando. Ad esempio, se hai un file <code>input.txt</code> con del testo, puoi fare:</p> <pre><code>cat input.txt | llm \"mi fai una sintesi del testo che c'\u00e8 in questo file?\"\n</code></pre> <p>Oppure un esempio ispirato a una famosa vignetta di xkcd, che evidenzia come per certi comandi (come <code>tar</code>) non sia sempre semplice ricordarsi le opzioni corrette da usare.</p> La vignetta \"tar\" di xkcd <p>Allora si pu\u00f2 \"passare\" l'output di <code>man tar</code> <sup>1</sup> a <code>llm</code> per chiedere come si usa il comando <code>tar</code> per estrarre il file <code>appunti.tar.gz</code>:</p> <pre><code>man tar | llm \"come si usa il comando tar per estrarre il file appunti.tar.gz.\nScrivi soltanto il comando e commenta le opzioni\"\n</code></pre> <p>Con <code>man tar</code> si ottiene l'output del manuale di <code>tar</code>, che contiene le istruzioni su come usare il comando. Passando questo output a <code>llm</code>, si pu\u00f2 ottenere una risposta chiara e concisa su come eseguire l'operazione richiesta.</p> <p>E l'output sar\u00e0 qualcosa del genere (e la bomba non esploder\u00e0 \ud83d\ude09):</p> <pre><code>tar -xzf appunti.tar.gz\n\n*   `-x`: Indica l'operazione di estrazione (extract).\n*   `-z`: Indica che il file \u00e8 compresso con gzip e che quindi deve essere decompresso durante l'estrazione.\n*   `-f`: Specifica che il successivo argomento \u00e8 il nome del file archivio da estrarre (`appunti.tar.gz`).\n</code></pre> <p>Nota</p> <p>La gran parte dei modelli sarebbero stati in grado di rispondere a questa domanda, senza bisogno di leggere il manuale di <code>tar</code>, ma \u00e8 un esempio interessante per mostrare come si pu\u00f2 usare <code>llm</code> per leggere da <code>stdin</code> e ottenere risposte utili.</p> <ol> <li> <p><code>man</code> \u00e8 un comando per visualizzare i manuali dei comandi Unix/Linux.\u00a0\u21a9</p> </li> </ol>"},{"location":"ai/ollama/","title":"Ollama","text":"<p>Ollama \u00e8 uno strumento per eseguire modelli LLM in locale, direttamente da terminale.</p> <p>In questa pagina trovi il minimo essenziale per iniziare su Linux:</p> <ol> <li>installazione;</li> <li>aggiunta di un modello;</li> <li>primo utilizzo.</li> </ol>"},{"location":"ai/ollama/#installazione-linux","title":"Installazione (Linux)","text":"<p>Per installare Ollama:</p> <pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre> <p>Verifica che sia installato:</p> <pre><code>ollama --version\n</code></pre> <p>Su distribuzioni Linux con <code>systemd</code>, il servizio parte in automatico. In caso contrario, puoi avviarlo manualmente:</p> <pre><code>ollama serve\n</code></pre>"},{"location":"ai/ollama/#aggiungere-un-modello","title":"Aggiungere un modello","text":"<p>Esempio con un modello piccolo e open di Microsoft: <code>phi3:mini</code>.</p> <pre><code>ollama pull phi3:mini\n</code></pre> <p>Per vedere i modelli scaricati:</p> <pre><code>ollama list\n</code></pre>"},{"location":"ai/ollama/#usarlo","title":"Usarlo","text":"<p>Prompt singolo:</p> <pre><code>ollama run phi3:mini \"Spiega in modo semplice cos'\u00e8 una pipe in Linux\"\n</code></pre> <p>Sessione interattiva:</p> <pre><code>ollama run phi3:mini\n</code></pre> <p>Invio di un file via pipe:</p> <pre><code>cat appunti.txt | ollama run phi3:mini \"Riassumi il testo in 5 punti\"\n</code></pre> <p>Per uscire dalla sessione interattiva, usa <code>/bye</code>.</p>"},{"location":"ai/ollama/#lanciare-una-coding-cli-claude-code-o-opencode","title":"Lanciare una coding CLI (Claude Code o OpenCode)","text":"<p>Con <code>ollama launch</code> puoi configurare e avviare tool come Claude Code e OpenCode usando un modello locale oppure un modello cloud gestito da Ollama.</p>"},{"location":"ai/ollama/#1-verifica-versione","title":"1) Verifica versione","text":"<p>Serve Ollama <code>v0.15+</code>:</p> <pre><code>ollama --version\n</code></pre>"},{"location":"ai/ollama/#2-scegli-e-scarica-un-modello","title":"2) Scegli e scarica un modello","text":"<p>Modello locale (esempio):</p> <pre><code>ollama pull glm-4.7-flash\n</code></pre> <p>Modello cloud gestito da Ollama (esempio):</p> <pre><code>ollama pull glm-4.7:cloud\n</code></pre>"},{"location":"ai/ollama/#3-avvia-la-cli","title":"3) Avvia la CLI","text":"<p>Per Claude Code con modello cloud:</p> <pre><code>ollama launch claude --model minimax-m2.5:cloud\n</code></pre> <p>Per Claude Code con modello locale:</p> <pre><code>ollama launch claude --model glm-4.7-flash\n</code></pre> <p>Per OpenCode con modello cloud:</p> <pre><code>ollama launch opencode --model minimax-m2.5:cloud\n</code></pre> <p>Per OpenCode con modello locale:</p> <pre><code>ollama launch opencode --model glm-4.7-flash\n</code></pre> <p>Se non specifichi <code>--model</code>, il comando \u00e8 interattivo e ti fa scegliere il modello (locale o cloud), senza dover impostare variabili d'ambiente o file di configurazione manuali.</p> <p>Se vuoi solo configurare, senza avviare subito il tool:</p> <pre><code>ollama launch opencode --model minimax-m2.5:cloud --config\n</code></pre> <p>Nota pratica</p> <p>Per sessioni di coding lunghe, Ollama consiglia modelli con contesto ampio (almeno <code>64000</code> token) e, se necessario, di usare i modelli cloud.</p>"},{"location":"ai/ttok/","title":"ttok","text":"<p>\ud83d\udea7 Pagina in costruzione</p> <p>Questa pagina \u00e8 ancora in lavorazione. Il contenuto \u00e8 incompleto e non revisionato.</p>"},{"location":"ai/ttok/#ttok","title":"ttok","text":"<p><code>ttok</code> \u00e8 un altro strumento creato da Simon Willison, per contare il numero di token di un testo e/o troncarlo in base a un numero desiderato di questi.</p> <p>Cosa sono i token?</p> <p>I token sono le unit\u00e0 base in cui gli LLM suddividono il testo per elaborarlo. Un token pu\u00f2 essere una parola intera, parte di una parola, un carattere di punteggiatura o anche uno spazio. Ad esempio, la frase \"Ciao mondo!\" potrebbe essere divisa in 3 token: \"Ciao\", \" mondo\", \"!\".</p> <p>\u00c8 fondamentale conoscere il numero di token perch\u00e9 i modelli LLM hanno un limite massimo di token che possono elaborare, in input e output. Inoltre il costo dell'utilizzo di un modello LLM \u00e8 spesso calcolato in base al numero di token elaborati.</p> <p><code>ttok</code> \u00e8 scritto in Python e si pu\u00f2 installare con <code>pip</code> o con <code>uv</code>:</p> <pre><code>pip install ttok\n</code></pre> <pre><code>uv tool install ttok\n</code></pre> <p>Il modello di default utilizzato per il conteggio dei token \u00e8 quello di GPT-3.5 e GPT-4.</p>"},{"location":"ai/ttok/#utilizzo","title":"Utilizzo","text":""},{"location":"ai/ttok/#base","title":"Base","text":"<p>Il comando base si pu\u00f2 lanciare cos\u00ec:</p> <pre><code>ttok Ciao mondo\n</code></pre> <p>In output si ottiene il numero di token:</p> <pre><code>3\n</code></pre> <p>Oppure \u00e8 possibile fargli leggere l'output di un comando, come ad esempio <code>curl</code> e conteggiare il numero di token della pagina web su Wikipedia dedicata a Aaron Swartz:</p> <pre><code>curl -kL 'https://en.wikipedia.org/wiki/Aaron_Swartz' | ttok\n</code></pre> <p>In questo caso si hanno circa <code>230.000</code> token di output. Questo si pu\u00f2 passare a un <code>LLM</code> tramite <code>llm</code> cli per chiedere di riassumere il testo per un messaggio da inviare in chat:</p> <pre><code>curl -kL 'https://en.wikipedia.org/wiki/Aaron_Swartz' | \\\nllm \"mi scrivi un messaggio breve per stimolare le persone a scoprire chi \u00e8 Aaron. Lo invier\u00f2 in chat\" | ttok\n</code></pre> <p>In output si ottiene qualcosa come il testo di sotto, che \u00e8 lungo circa <code>70</code> token:</p> <pre><code>Hai mai sentito parlare di Aaron Swartz? \ud83e\udd14 Era un genio, un attivista e un pioniere del web. La sua storia \u00e8 incredibile e ti far\u00e0 riflettere sul futuro di internet e della conoscenza. Scoprilo anche tu! \ud83d\ude09\"\n</code></pre> <p>Conoscendo il numero di token di input e di output, in base al modello di LLM scelto, si pu\u00f2 calcolare il costo del servizio.</p>"},{"location":"ai/ttok/#troncare-il-testo-a-un-certo-numero-di-token","title":"Troncare il testo a un certo numero di token","text":"<p>Se si vuole troncare il testo a un certo numero di token, si pu\u00f2 usare l'opzione <code>-t</code> seguita dal numero di token desiderato. Ad esempio, per troncare il testo a <code>10</code> token:</p> <pre><code>ttok -t 10 \"Ciao mondo, questo \u00e8 un testo di esempio che voglio troncare a dieci token.\"\n</code></pre> <p>In output si ottiene il testo troncato:</p> <pre><code>Ciao mondo, questo \u00e8 un testo di\n</code></pre>"},{"location":"ai/ttok/#visualizzare-i-token","title":"Visualizzare i token","text":"<p>Per visualizzare i token di un testo, si pu\u00f2 usare l'opzione <code>--encode</code>:</p> <pre><code>ttok \"Ciao mondo, questo \u00e8 un testo di esempio\" --encode\n</code></pre> <p>In output si ottengono gli ID dei token (dei 13 token del testo di esempio):</p> <pre><code>34 23332 70809 11 34560 11676 653 1296 78 1891 1560 3342 822\n</code></pre> <p>Per convertire questi ID in testo leggibile, si pu\u00f2 usare l'opzione <code>--decode</code>:</p> <pre><code>ttok \"34 23332 70809 11 34560 11676 653 1296 78 1891 1560 3342 822\" --decode\n</code></pre>"},{"location":"ai/ttok/#costo-dei-modelli-llm-per-numero-di-token","title":"Costo dei modelli LLM per numero di token","text":"<p>Simon Willison - \u00e8 sempre bene rinnovare una volta al giorno grazie Simon -  ha realizzato un calcolatore di costi per token e modello, che permette di calcolare il costo del servizio.</p> <p></p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2023/09/02/creata-sezione-blog/","title":"Creata sezione blog","text":"<p>Attivata la sezione blog, che verr\u00e0 usata per inserire aggiornamenti su questo progetto.</p> <p>Il 21 agosto 2023, il grande Martin Donath ha pubblicato la release <code>9.2.0</code> di Material for MkDocs.</p> <p>Material for MkDocs \u00e8 il motore di questo sito; con questa release, il plug-in per la creazione e gestione di blog viene reso disponibile nella community edition, ovvero nella versione gratuita.</p> <p>E allora \u00e8 stato introdotto anche qui.</p> <p>Grazie infinite Martin</p> <p>Il lavoro di Martin per lo sviluppo di questa piattaforma \u00e8 straordinario. C'\u00e8 una cura e un'efficacia che si vede raramente. Grazie, grazie, grazie!!</p>"},{"location":"blog/2026/02/21/miller-6170-yaml-finalmente-supportato/","title":"Miller 6.17.0: YAML finalmente supportato","text":"<p>Ci sono release che aggiungono funzioni \"belle da leggere\", e release che ti fanno risparmiare tempo da subito. La release Miller 6.17.0 (pubblicata il 21 febbraio 2026) \u00e8 del secondo tipo: prende problemi reali emersi nelle issue e li trasforma in comandi pi\u00f9 diretti.</p> <p>Il risultato \u00e8 semplice: meno passaggi intermedi, meno script collaterali, pipeline pi\u00f9 corte.</p> <p></p>","tags":["miller","release","yaml","dcf"]},{"location":"blog/2026/02/21/miller-6170-yaml-finalmente-supportato/#yaml-io-nativo","title":"YAML I/O nativo","text":"<p>Chi lavora con configurazioni, metadati, OpenAPI o cataloghi dati si trova spesso file YAML da esplorare al volo. Prima, lo YAML non era un formato supportato da Miller: bisognava convertirlo in JSON (o CSV) con uno strumento esterno prima di poterlo elaborare.</p> <p>Prima (YAML non era un formato di input/output di Miller)</p> <p>Input (<code>input.yaml</code>):</p> <pre><code>- id: 1\n  city: Roma\n  score: 10\n- id: 2\n  city: Milano\n  score: 20\n</code></pre> <pre><code># Serviva una conversione esterna, ad esempio con yq\nyq -o=json input.yaml | mlr --ijson --ocsv cat\n</code></pre> <p>Output:</p> <pre><code>city,id,score\nRoma,1,10\nMilano,2,20\n</code></pre> <p>Dopo (YAML \u00e8 ora un formato nativo di Miller)</p> <pre><code>mlr --iyaml --ocsv cat input.yaml\n</code></pre> <p>Output:</p> <pre><code>city,id,score\nRoma,1,10\nMilano,2,20\n</code></pre> <p>Puoi trattare YAML come CSV/JSON nella stessa pipeline, senza passaggi manuali extra.</p> <p>Esempio d'uso minimo con creazione campo derivato:</p> <pre><code>- first_name: Ada\n  last_name: Lovelace\n- first_name: Alan\n  last_name: Turing\n</code></pre> <pre><code>mlr --yaml put '$full_name = $first_name . \" \" . $last_name' persone.yaml\n</code></pre> <p>Output:</p> <pre><code>- first_name: Ada\n  full_name: Ada Lovelace\n  last_name: Lovelace\n- first_name: Alan\n  full_name: Alan Turing\n  last_name: Turing\n</code></pre> <p>\ud83c\udf89 Nota personale: questa feature mi rende particolarmente orgoglioso, perch\u00e9 la richiesta iniziale \u00e8 partita proprio da me con l'issue #614.</p>","tags":["miller","release","yaml","dcf"]},{"location":"blog/2026/02/21/miller-6170-yaml-finalmente-supportato/#supporto-dcf-debian-control-format","title":"Supporto DCF (Debian Control Format)","text":"<p>DCF (Debian Control Format) \u00e8 un formato testuale usato storicamente nei metadati dei pacchetti Debian e, pi\u00f9 in generale, in file descrittivi composti da record. Ogni record contiene campi <code>chiave: valore</code>, pu\u00f2 avere righe di continuazione per i campi lunghi, e i record sono separati da una riga vuota. Prima si ricorreva spesso a workaround con <code>grep</code>/<code>sed</code> prima di passare i dati a Miller.</p> <p>Prima</p> <p>Input (<code>sample.dcf</code>):</p> <pre><code>Package: foo\nVersion: 1.0\nDepends: libc6 (&gt;= 2.0), libfoo (&gt;= 1.2)\nDescription: A test package.\n\nPackage: bar\nVersion: 2.0\nRecommends: foo\nDescription: Another package.\n</code></pre> <p>Dopo</p> <pre><code>mlr --idcf --ojson cat sample.dcf\n</code></pre> <p>Output:</p> <pre><code>[\n  {\n    \"Package\": \"foo\",\n    \"Version\": \"1.0\",\n    \"Depends\": [\"libc6 (&gt;= 2.0)\", \"libfoo (&gt;= 1.2)\"],\n    \"Description\": \"A test package.\"\n  },\n  {\n    \"Package\": \"bar\",\n    \"Version\": \"2.0\",\n    \"Recommends\": [\"foo\"],\n    \"Description\": \"Another package.\"\n  }\n]\n</code></pre> <p>\u00c8 un miglioramento pratico quando i file hanno continuation lines e campi non sempre uguali tra record.</p> <p>Verbi correlati: <code>cat</code>.</p>","tags":["miller","release","yaml","dcf"]},{"location":"blog/2026/02/21/miller-6170-yaml-finalmente-supportato/#reorder-r-riordinare-colonne-con-regex","title":"<code>reorder -r</code>: riordinare colonne con regex","text":"<p>Quando lo schema \u00e8 variabile ma i prefissi sono stabili (<code>YYY_*</code>, <code>XXX_*</code>), elencare i campi uno per uno \u00e8 fragile.</p> <p>Prima</p> <p>Input (<code>input.csv</code>):</p> <pre><code>id,YYY_city,foo,YYY_region,XXX_score,bar\n1,Roma,a,Lazio,10,x\n2,Milano,b,Lombardia,20,y\n</code></pre> <pre><code>mlr --icsv --ocsv reorder -f YYY_city,YYY_region,XXX_score input.csv\n</code></pre> <p>Output:</p> <pre><code>YYY_city,YYY_region,XXX_score,id,foo,bar\nRoma,Lazio,10,1,a,x\nMilano,Lombardia,20,2,b,y\n</code></pre> <p>Dopo</p> <pre><code>mlr --icsv --ocsv reorder -r '^YYY,^XXX' input.csv\n</code></pre> <p>Output:</p> <pre><code>YYY_city,YYY_region,XXX_score,id,foo,bar\nRoma,Lazio,10,1,a,x\nMilano,Lombardia,20,2,b,y\n</code></pre> <p>Utile soprattutto nei dataset dove le colonne cambiano spesso ma seguono convenzioni di naming.</p> <p>Verbi correlati: <code>reorder</code>.</p>","tags":["miller","release","yaml","dcf"]},{"location":"blog/2026/02/21/miller-6170-yaml-finalmente-supportato/#nest-r-selezione-regex-dei-campi-target","title":"<code>nest -r</code>: selezione regex dei campi target","text":"<p><code>nest</code> serve a esplodere o implodere campi con valori multipli delimitati: ad esempio trasforma un campo <code>tags=a;b;c</code> in tre campi separati (<code>tags_1</code>, <code>tags_2</code>, <code>tags_3</code>), oppure fa l'operazione inversa. \u00c8 utile ogni volta che dati strutturati sono compressi in un singolo campo con un separatore interno.</p> <p>Nei dati reali capita di avere famiglie di colonne con nomi molto simili tra loro (<code>tags_1</code>, <code>tags_2</code>, <code>tags_3</code>, ...). Prima, quando volevi usare nest, eri costretto a elencare hardcoded ogni campo per nome, uno per uno con <code>-f</code>. Ora puoi selezionarli tutti in una sola chiamata usando una regex con <code>-r</code>: se domani compare <code>tags_5</code>, non devi toccare il comando.</p> <p>Prima</p> <p>Input (<code>input.csv</code>):</p> <pre><code>id,tags_1,tags_2,tags_3,tags_4,other\n1,alpha;beta,gamma,delta,uno;due,z\n2,red;blue,green,black,tre;quattro,k\n</code></pre> <pre><code>mlr --icsv --ocsv nest --explode --values --across-fields -f tags_1 -f tags_2 -f tags_3 input.csv\n</code></pre> <p>Output:</p> <pre><code>id,tags_1,tags_2,tags_3_1,tags_4,other\n1,alpha;beta,gamma,delta,uno;due,z\n2,red;blue,green,black,tre;quattro,k\n</code></pre> <p>Dopo</p> <pre><code>mlr --icsv --ocsv nest --explode --values --across-fields -r '^tags_' input.csv\n</code></pre> <p>Output:</p> <pre><code>id,tags_1_1,tags_1_2,tags_2_1,tags_3_1,tags_4_1,tags_4_2,other\n1,alpha,beta,gamma,delta,uno,due,z\n2,red,blue,green,black,tre,quattro,k\n</code></pre> <p>Meno manutenzione quando aumentano o cambiano i campi della stessa famiglia.</p> <p>\ud83c\udf89 Nota personale: anche questa mi rende felice: la richiesta iniziale era mia, nell'issue #381.</p> <p>Verbi correlati: <code>nest</code>.</p>","tags":["miller","release","yaml","dcf"]},{"location":"blog/2026/02/21/miller-6170-yaml-finalmente-supportato/#split-folder-output-diretto-in-cartella","title":"<code>split --folder</code>: output diretto in cartella","text":"<p>Qui la novit\u00e0 non \u00e8 nel risultato finale (i file prodotti), ma nel modo in cui lo esprimi. Prima si usava spesso <code>--prefix</code> con path incorporato: funzionava, ma mescolava directory di destinazione e nome base dei file in un solo parametro. Con <code>--folder</code> le due cose sono separate e il comando \u00e8 pi\u00f9 leggibile/manutenibile.</p> <p>Prima</p> <p>Input (<code>input.csv</code>):</p> <pre><code>id,city\n1,Roma\n2,Milano\n3,Torino\n</code></pre> <pre><code>mlr --icsv split -m 10 --prefix /tmp/test --suffix csv input.csv\n</code></pre> <p>Output (file creati):</p> <pre><code>/tmp/test_1.csv\n/tmp/test_2.csv\n/tmp/test_3.csv\n</code></pre> <p>Dopo</p> <pre><code>mlr --icsv split -m 10 --folder /tmp --prefix test --suffix csv input.csv\n</code></pre> <p>Output (file creati):</p> <pre><code>/tmp/test_1.csv\n/tmp/test_2.csv\n/tmp/test_3.csv\n</code></pre> <p>Quindi: stesso output, ma semantica migliore del comando. Separare destinazione (<code>--folder</code>) e nome file (<code>--prefix</code>) rende script e automazioni pi\u00f9 leggibili e meno ambigui.</p> <p>Verbi correlati: <code>split</code>.</p> <p>Questa non \u00e8 una release \"solo tecnica\": aggiunge due formati richiesti da tempo (YAML, DCF) e migliora tre verbi usati ogni giorno (<code>reorder</code>, <code>nest</code>, <code>split</code>) nei punti dove prima servivano workaround.</p> <p>Per chi lavora con dati da riga di comando, \u00e8 una release che si sente subito. E oltre alle novit\u00e0 raccontate qui, include anche diversi bugfix e altre modifiche interne che rendono Miller ancora pi\u00f9 solido.</p>","tags":["miller","release","yaml","dcf"]},{"location":"blog/2026/02/21/miller-6170-yaml-finalmente-supportato/#fonti","title":"Fonti","text":"<ul> <li>Release <code>v6.17.0</code>: https://github.com/johnkerl/miller/releases/tag/v6.17.0</li> <li>PR YAML: https://github.com/johnkerl/miller/pull/1963</li> <li>Issue YAML: https://github.com/johnkerl/miller/issues/614</li> <li>PR DCF: https://github.com/johnkerl/miller/pull/1970</li> <li>Issue DCF: https://github.com/johnkerl/miller/issues/1804</li> <li>PR <code>reorder -r</code>: https://github.com/johnkerl/miller/pull/1960</li> <li>Issue <code>reorder -r</code>: https://github.com/johnkerl/miller/issues/1325</li> <li>PR <code>nest -r</code>: https://github.com/johnkerl/miller/pull/1961</li> <li>Issue <code>nest -r</code>: https://github.com/johnkerl/miller/issues/381</li> <li>PR <code>split --folder</code>: https://github.com/johnkerl/miller/pull/1962</li> <li>Issue <code>split --folder</code>: https://github.com/johnkerl/miller/issues/1754</li> <li>Discussion origine <code>split --folder</code>: https://github.com/johnkerl/miller/discussions/1402</li> </ul>","tags":["miller","release","yaml","dcf"]},{"location":"blog/2023/09/03/release-69-di-miller/","title":"Release 6.9 di Miller","text":"<p>Rilasciata una nuova versione di Miller con il supporto alla compressione ZSTD, una gestione migliore degli errori, il supporto ai nanosecondi nel timestamp e tanto altro.</p> <p>Grazie infinite John</p> <p>Il grande John Kerl \u00e8 l'autore di Miller, uno strumento che ha cambiato in meglio la mia vita lavorativa, facendomi imparare tanto e stimolandomi sempre ad approfondire temi vecchi e nuovi. Grazie, grazie, grazie!</p> <p>Miller \u00e8 lo strumento che ha ispirato di pi\u00f9 la creazione di questo spazio e per la quale c'\u00e8 una sezione dedicata, ancora per\u00f2 in costruzione. Il 31 agosto 2023, \u00e8 stata pubblicata la release <code>6.9</code> di Miller.</p> <p>A seguire alcune delle novit\u00e0</p>","tags":["miller","release"]},{"location":"blog/2023/09/03/release-69-di-miller/#timestamp","title":"Timestamp","text":"<p>Per i <code>timestamp</code> sono state introdotte numerose funzioni <code>DSL</code> per gestire i nanosecondi come numero intero a partire dalla data epoch: <code>gmt2nsec</code>, <code>localtime2nsec</code>, <code>nsec2gmt</code>, <code>nsec2gmtdate</code>, <code>nsec2localdate</code>, <code>nsec2localtime</code>, <code>strfntime</code>, <code>strfntime_local</code>, <code>strpntime</code>, <code>strpntime_local</code>, <code>sysntime</code> e <code>upntime</code>.</p>","tags":["miller","release"]},{"location":"blog/2023/09/03/release-69-di-miller/#funzioni-statistiche","title":"Funzioni statistiche","text":"<p>Sono state introdotte le singole funzioni statistiche, prima presenti soltanto nel verbo <code>stats</code>: ora possono essere usate su array e map.</p>","tags":["miller","release"]},{"location":"blog/2023/09/03/release-69-di-miller/#verbi-per-trova-e-sostituisci","title":"Verbi per \"trova e sostituisci\"","text":"<p>Aggiunti i nuovi verbi <code>sub</code>, <code>gsub</code> e <code>ssub</code>, per applicare comandi di trova e sostituisci, in modo molto pi\u00f9 diretto e comodo della vecchia modalit\u00e0 via verbo <code>put</code>. Sono supportate le espressioni regolari.</p> <p>Se ad esempio ho questa tabella di input:</p> nome dataNascita altezza peso andy 1973-05-08 176 86.5 chiara 1993-12-13 162 58.3 guido 2001-01-22 196 90.4 <p>E voglio sostituire <code>1973</code> con <code>2021</code> nella colonna <code>dataNascita</code>, si pu\u00f2 lanciare questo comando:</p> <pre><code>mlr --csv sub -f dataNascita 1973 2021 input.csv\n</code></pre> <p>Ed ottenere:</p> nome dataNascita altezza peso andy 2021-05-08 176 86.5 chiara 1993-12-13 162 58.3 guido 2001-01-22 196 90.4 <p>In qualche modo correlata \u00e8 l'introduzione della funzione <code>contains</code>, che restituisce <code>true</code>, se il primo argomento contiene il secondo.</p>","tags":["miller","release"]},{"location":"blog/2023/09/03/release-69-di-miller/#supporto-nativo-alla-compressione-zstd","title":"Supporto nativo alla compressione ZSTD","text":"<p>La compressione <code>ZSTD</code> \u00e8 ideata per essere efficiente sia in termini di velocit\u00e0 che di rapporto di compressione. Ne ho scritto da poco nel post \"Gestire file CSV grandi, brutti e cattivi\".</p> <p>Con la release <code>6.9</code> di Miller si potr\u00e0 operare direttamente su file compresso in questa modalit\u00e0, senza decomprimerlo. Ad esempio potr\u00f2 leggere le prime 10 righe con:</p> <pre><code>mlr --csv --ifs \";\" head input.csv.zst\n</code></pre> <p>Di questa novit\u00e0 sono un po' orgoglioso, perch\u00e9 deriva da una mia proposta, nata proprio mentre scrivevo il suddetto articolo. Prima c'era il supporto per <code>GZIP</code>, <code>BZIP2</code> e <code>ZLIB</code>.</p>","tags":["miller","release"]},{"location":"ckanapi/","title":"ckanapi cli","text":"<p>\u00c8 un'utility per per utilizzare le API di CKAN. <code>CKAN</code> \u00e8 la piattaforma open source pi\u00f9 usata per la realizzazione di portali open data: imparare a usare questo strumento \u00e8 molto comodo, per interagire \"a riga di comando\" con tantissimi portali di questo tipo.</p>"},{"location":"ckanapi/#installazione","title":"Installazione","text":"<p>Si installa rapidamente con </p> <p><pre><code>pip install ckanapi\n</code></pre> oppure con</p> <pre><code>uv tool install ckanapi\n</code></pre>"},{"location":"ckanapi/#esempi-di-query","title":"Esempi di query","text":"<p>Le ricerche sono basate su <code>solr</code>, e per diversi campi non \u00e8 una ricerca per stringa esatta (vedi nota).</p>"},{"location":"ckanapi/#ricerca-per-stringa","title":"Ricerca per stringa","text":"<p>Ad esempio la stringa <code>furgoni</code>, in qualsiasi campo:</p> <pre><code>ckanapi -r https://dati.gov.it/opendata/ action package_search q='furgoni'\n</code></pre> <p>Numero di risultati</p> <p>Di default il numero di risultati presentati in un output di <code>package_search</code> \u00e8 pari a 10. Per modificarlo bisogna introdurre il parametro <code>rows</code> (il valore massimo di default \u00e8 pari a 1000).</p>"},{"location":"ckanapi/#il-numero-di-risultati-di-una-ricerca-per-dataset","title":"Il numero di risultati di una ricerca per dataset","text":"<p>L'azione <code>package_search</code> restituisce sempre in output, il parametro <code>count</code>. Si pu\u00f2 allora impostare a <code>0</code> il numero di record di output da visualizzare (perch\u00e9 non si vuole qui leggere i dettagli dei risultati) ed estrarre dall'output del JSON soltanto <code>count</code>:</p> <pre><code>ckanapi -r https://dati.gov.it/opendata/ action package_search q='scuole' rows=0 | jq '.count'\n</code></pre> <p>La risposta \u00e8 immediata, perch\u00e9 non vengono listate le decine di dataset che contengono la stringa <code>scuole</code>, ma soltanto il conteggio degli stessi.</p>"},{"location":"ckanapi/#ricerca-per-titolo","title":"Ricerca per titolo","text":"<p>Tutti i dataset che hanno nel titolo la parola <code>farmacie</code>:</p> <pre><code>ckanapi -r https://dati.gov.it/opendata/ search datasets q='(title:\"farmacie\")'\n</code></pre> <p>Tutti i dataset che hanno nel titolo le parola <code>farmacie</code> e <code>anno</code>:</p> <pre><code>ckanapi -r https://dati.gov.it/opendata/ search datasets q='(title:\"farmacie\" title:\"anno\")'\n</code></pre> <p>Tutti i dataset che hanno nel titolo le parola <code>farmacie</code> o <code>distributori</code>:</p> <pre><code>ckanapi -r https://dati.gov.it/opendata/ search datasets q='(title:\"farmacie\" OR title:\"distributori\")'\n</code></pre>"},{"location":"ckanapi/#ricerca-dataset-basata-su-field-extra","title":"Ricerca dataset basata su field extra","text":"<p>Nei dataset spesso ci sono diversi campi descrittivi, nell'array <code>extras</code>. Due di esempio:</p> <pre><code>{\n  \"key\": \"contact_email\",\n  \"value\": \"dirigenteat@comune.copertino.le.it\"\n},\n{\n  \"key\": \"contact_name\",\n  \"value\": \"comune-di-copertino\"\n},\n{\n  \"key\": \"identifier\",\n  \"value\": \"c_c978:openpnrr-comune-di-copertino\"\n}\n</code></pre> <p>Per cercare tramite questi campi, si pu\u00f2 usare il nome della <code>key</code>. Ad esempio:</p> <pre><code>ckanapi -r https://dati.gov.it/opendata/ action package_search fq='identifier:*pnrr*' rows=10\n</code></pre> <p>Carattere jolly</p> <p><code>*</code> \u00e8 il carattere jolly per le ricerche. Ad esempio con <code>identifier:*prova*</code> si avranno tutti i risultati che hanno come <code>identifier</code> \"approvate\", ma anche \"approvadibomba\".</p>"},{"location":"ckanapi/#ricerca-per-data-di-creazione-dei-metadati","title":"Ricerca per data di creazione dei metadati","text":"<pre><code>ckanapi -r  https://dati.regione.sicilia.it action package_search fq='metadata_created:[2017-04-12T00:00:00Z TO 2022-06-30T23:59:05Z]' rows:1\n</code></pre> <p>Con <code>rows:1</code> prendo soltanto il primo risultato.</p> <p>Oppure (\u00e8 preferibile perch\u00e9 \u00e8 un output jsonl)</p> <pre><code>ckanapi -r  https://dati.regione.sicilia.it search datasets fq='metadata_created:[2017-04-12T00:00:00Z TO 2022-06-30T23:59:05Z]' rows:1\n</code></pre>"},{"location":"ckanapi/#ricerca-dataset-per-formato-delle-risorse-contenute","title":"Ricerca dataset per formato delle risorse contenute","text":"<p>La ricerca di dataset per formato delle risorse contenute, dalla documentazione ufficiale non sembra possibile. \u00c8 per\u00f2 possibile farlo, usando un parametro non documentato, di cui c'\u00e8 traccia nella documentazione sulle viste: <code>res_format</code>.</p> <p>Per avere restituito ad esempio i dataset che contengono almeno una risorsa in formato <code>CSV</code>, il comando \u00e8:</p> <pre><code>ckanapi -r https://dati.gov.it/opendata/ search datasets fq='(res_format:\"CSV\")' rows=1\n</code></pre> <p>\u00c8 possibile ovviamente arricchire la query, aggiungendo al filtro per formato, quello per organizzazione:</p> <pre><code>ckanapi -r https://dati.gov.it/opendata/ search datasets fq='(res_format:\"CSV\" AND organization:\"regione-toscana\")' rows=1\n</code></pre> <p>Ad agosto del 2022, la cosa risulta ancora non documentata: https://github.com/ckan/ckan/discussions/7013.</p>"},{"location":"ckanapi/#utilizzare-piu-parametri-di-ricerca-di-un-dataset-e-combinarli-in-and","title":"Utilizzare pi\u00f9 parametri di ricerca di un dataset e combinarli in AND","text":"<pre><code>ckanapi search datasets -r https://dati.gov.it/opendata/ fq='(organization:\"comune-di-torino\" AND groups:\"ambiente\")' rows=1\n</code></pre> <p>Nota bene: <code>rows=1</code> limita l'output a un solo risultato. Qui \u00e8 inserito soltanto per testare la query.</p>"},{"location":"ckanapi/#esempi-di-filtri","title":"Esempi di filtri","text":""},{"location":"ckanapi/#tutti-i-dataset-con-30-tag","title":"Tutti i dataset con 30 tag","text":"<pre><code>ckanapi -r https://dati.gov.it/opendata/ search datasets fq='(num_tags:30)'\n</code></pre>"},{"location":"ckanapi/#tutti-i-dataset-di-unorganizzazione","title":"Tutti i dataset di un'organizzazione","text":"<pre><code>ckanapi -r https://dati.toscana.it/ search datasets fq='(organization:\"comune-di-firenze\")' -O datasets.jsonl.gz -z\n</code></pre> <p>Note:</p> <ul> <li><code>-O</code> per definire il file di output. \u00c8 un JSON Lines;</li> <li><code>-z</code> per avere l'output JSON Lines compresso in gzip.</li> </ul>"},{"location":"ckanapi/#varie","title":"Varie","text":""},{"location":"ckanapi/#numero-di-dataset-per-organizzazione","title":"Numero di dataset per organizzazione","text":"<pre><code>ckanapi -r https://dati.toscana.it/ action package_search facet.field:'[\"organization\"]' rows:0\n</code></pre>"},{"location":"ckanapi/#numero-di-dataset-per-un-dato-gruppo-tematico","title":"Numero di dataset per un dato gruppo tematico","text":"<pre><code>ckanapi -r https://dati.gov.it/opendata/ action package_search fq='(groups:\"ambiente\")' facet.field:'[\"organization\"]' rows=0\n</code></pre>"},{"location":"ckanapi/#download-dei-risultati-di-una-ricerca-in-loop","title":"Download dei risultati di una ricerca in loop","text":"<pre><code>URL=\"https://mysample.url\"\nckanapi -r \"$URL\" search datasets fq='(title:\"taxes\")' | \\\njq -r '.id' | \\\nxargs -I _ ckanapi -r \"$URL\" dump datasets _ --datapackages=./out\n</code></pre> <p>Vedi https://github.com/ckan/ckanapi/issues/197#issuecomment-1069173069</p>"},{"location":"ckanapi/#note-sulle-ricerche-fatte-su-ckan","title":"Note sulle ricerche fatte su CKAN","text":"<p>Alcune delle ricerche fatte su CKAN, sfruttano come motore <code>solr</code>. Di <code>default</code>, su <code>solr</code>, i campi su cui \u00e8 possibile eseguire una ricerca sono impostati come <code>type=text</code>, e su questi non \u00e8 attiva una ricerca \"esatta\", ma sempre una ricerca fuzzy. Per la ricerca esatta i campi devono essere impostati come <code>type=string</code>.</p> <p>Nello schema <code>solr</code> di CKAN \u00e8 possibile leggere il <code>type</code> dei vari campi. Ad esempio <code>res_format</code> \u00e8 impostato come <code>string</code> e su questo sar\u00e0 possibile eseguire ricerche esatte.</p>"},{"location":"csvkit/","title":"csvkit","text":"<p>Attenzione</p> <p>La sezione per csvkit non \u00e8 ancora pronta. \u00c8 possibile fare riferimento al sito ufficiale.</p> <p>\u00c8 un toolkit dedicato alla lettura, interrogazione, analisi, trasformazione di file CSV, scritto in <code>Python</code>.Ha fatto un po' la storia ed \u00e8 un riferimento.</p> <p>Mette a disposizione diversi comandi.</p>"},{"location":"csvkit/csvstat/","title":"csvkit - csvstat","text":"<p>Il comando <code>csvstat</code> restituisce delle statistiche di base, per ogni campo:</p> <ul> <li><code>column_id</code>, il numero della colonna (1, se \u00e8 la prima e a seguire);</li> <li><code>column_name</code>, il nome;</li> <li><code>type</code>, il tipo di campo (\u00e8 un valore dedotto, non letto, quindi ci potrebbe essere qualche errore);</li> <li><code>nulls</code>, se il campo contiene valori nulli (\u00e8 un booleano, con <code>True</code> e <code>False</code>);</li> <li><code>unique</code>, il numero di valori univoci;</li> <li><code>min</code>, il valore minimo (se numerico);</li> <li><code>max</code>, il valore massimo (se numerico);</li> <li><code>sum</code>, la somma (se numerico);</li> <li><code>mean</code>, la media (se numerico);</li> <li><code>median</code>, la mediana (se numerico);</li> <li><code>stdev</code>, la deviazione standard (se numerico);</li> <li><code>len</code>, la lunghezza (se \u00e8 un campo di testo).</li> <li><code>freq</code>, la lista dei valori pi\u00f9 frequenti.</li> </ul> <p>Ad esempio applicato a questo file</p> <p>csvstat --csv  --everything colored-shapes.csv</p> <p>\u00c8 stata aggiunta l'opzione <code>--csv</code>, per forzare l'output del comando in formato <code>CSV</code>.</p> <p>restituisce</p> column_id column_name type nulls unique min max sum mean median stdev len freq 1 color Text False 6 6 red, blue, yellow, purple, green 2 shape Text False 3 8 square, triangle, circle 3 flag Boolean False 2 False, True 4 i Number False 10056 56 499,974 2,526,603,201 250,704.822 251,091 144,935.141 18685, 26495, 36075, 49190, 68145 5 u Number False 10027 0 1 5,022.133 0.498 0.498 0.29 0.854026, 0.546961, 0.017264, 0.171977, 0.170643 6 v Number False 10026 -0.093 1.072 5,016.697 0.498 0.497 0.289 0.971983, 0.090678, 0.221560, 0.004933, 0.464153 7 w Number False 9434 0.043 0.901 5,034.491 0.5 0.5 0.034 0.506155, 0.521158, 0.499156, 0.494010, 0.520913 8 x Number False 10071 1.113 8.921 50,464.129 5.007 4.997 1.166 5.458618, 4.011646, 5.332571, 4.315572, 5.178077 <p>Nota</p> <p>Se avesse pure il conteggio dei valori nulli, sarebbe perfetto.</p>"},{"location":"dati/converti/","title":"Converti","text":""},{"location":"dati/esplora/","title":"Esplora","text":"<p>In questo spazio facciamo riferimento a dati che sono archiviati come testo, una modalit\u00e0 molto diffusa (si pensi ai formati <code>JSON</code>, <code>XML</code>, <code>CSV</code>, <code>TTL</code>, <code>YAML</code>, ecc.) e per la quale gli strumenti a riga di comando hanno supporto nativo: si aspettano stringhe di testo come <code>input</code>.</p> <p>Tanti strumenti di base e nativi sono impareggiabili nell'esplorare file di testo, in termini di rapidit\u00e0, opzioni e modalit\u00e0 per farlo. Sono spesso (in modo evidente) pi\u00f9 comodi della gran parte di quelli con interfaccia grafica.</p>"},{"location":"dati/esplora/#visualizzare","title":"Visualizzare","text":"<p>Uno dei modi per conoscere un file \u00e8 quello di visualizzarne i contenuti. Queste sono alcune delle utility classiche per farlo nel terminale.</p>"},{"location":"dati/esplora/#cat","title":"<code>cat</code>","text":"<p>L'utility tipica \u00e8 <code>cat</code> che \"stampa\" a schermo uno o pi\u00f9 file (in questo caso concatena e stampa).</p> <p>Se si vuole visualizzare il file <code>colored-shapes.csv</code>, il comando \u00e8 semplicemente:</p> <pre><code>cat colored-shapes.csv\n</code></pre> <p>Vengono per\u00f2 stampate a schermo tutte le righe, e con file molto grandi pu\u00f2 essere un po' lento e sopratutto poco informativo, perch\u00e9 non si legger\u00e0 l'intestazione che in alcuni formati (come i <code>CSV</code>) \u00e8 un elemento di esplorazione dei dati prezioso.</p> <p>Tip</p> <p>Esiste anche la versione di <code>cat</code>, che consente di leggere anche file di testo compressi: <code>zcat</code>.</p>"},{"location":"dati/esplora/#head","title":"<code>head</code>","text":"<p>Per visualizzare soltanto le prime righe c'\u00e8 l'utility <code>head</code>, che in maniere predefinita stampa le prime 10 righe:</p> <p>head colored-shapes.csv</p> <pre><code>color,shape,flag,i,u,v,w,x\nyellow,triangle,1,56,0.632170,0.988721,0.436498,5.798188\nred,square,1,80,0.219668,0.001257,0.792778,2.944117\nred,circle,1,84,0.209017,0.290052,0.138103,5.065034\nred,square,0,243,0.956274,0.746720,0.775542,7.117831\npurple,triangle,0,257,0.435535,0.859129,0.812290,5.753095\nred,square,0,322,0.201551,0.953110,0.771991,5.612050\npurple,triangle,0,328,0.684281,0.582372,0.801405,5.805148\nyellow,circle,1,370,0.603365,0.423708,0.639785,7.006414\nyellow,circle,1,440,0.285656,0.833516,0.635058,6.350036\n</code></pre> <p>Con l'opzione <code>-n numeroDiRighe</code> \u00e8 possibile scegliere il numero di righe da visualizzare:</p> <p>head -n 3 colored-shapes.csv</p> <pre><code>color,shape,flag,i,u,v,w,x\nyellow,triangle,1,56,0.632170,0.988721,0.436498,5.798188\nred,square,1,80,0.219668,0.001257,0.792778,2.944117\n</code></pre> <p>Per un formato come il <code>CSV</code> il comando <code>head</code> \u00e8 prezioso, perch\u00e9 nella gran parte dei casi restituisce una buona visione del file: qual \u00e8 il separatore dei campi, quali sono i campi, qual \u00e8 il separatore dei decimali (se presenti), se ci sono caratteri speciali (come le <code>\"</code>), ecc..</p>"},{"location":"dati/esplora/#tail","title":"<code>tail</code>","text":"<p>Ma \u00e8 bene poter vedere anche le ultime righe, per valutare se la struttura \u00e8 identica a quella di intestazione e se ci sono \"strani\" contenuti (alle volte purtroppo ci sono note di testo, campi calcolati). Il comando \u00e8 <code>tail</code>:</p> <pre><code>tail colored-shapes.csv\n</code></pre> <p>Di default, le ultime 10 righe; si pu\u00f2 definire quante se ne desiderano sempre con l'opzione <code>-n numeroDiRighe</code>.</p>"},{"location":"dati/esplora/#less","title":"<code>less</code>","text":"<p>Un altro strumento consigliato (ce sono tanti altri), per visualizzare ed esplorare il contenuto di un file di testo \u00e8 <code>less</code>, che consente di sfogliare il contenuto schermata dopo schermata, man mano che lo schermo viene riempito:</p> <pre><code>less colored-shapes.csv\n</code></pre> <p>Alcune informazioni di base su <code>less</code></p> <ul> <li>con l'o\"pzione <code>-S</code> si disabilita il world wrap</li> <li>con Space si scorrono le schermate (vedi immagine sotto);</li> <li>con / si attiva la ricerca di una stringa;</li> <li>con n il risultato successivo della ricerca;</li> <li>con N il risultato successivo della ricerca;</li> <li>con G si va alla fine del file;</li> <li>con g si va all'inizio;</li> <li>con Q si esce da <code>less</code>.</li> </ul> <p></p> <p>Tip</p> <p>In questo modo - con <code>head</code>, <code>tail</code> e <code>less</code> - \u00e8 possibile esplorare anche file di testo di grandi dimensioni.</p> <p>Le utility di questa sezione per\u00f2 fanno soltanto visualizzare contenuti, non estraggono informazioni (numero di righe, colonne, encoding, dimensioni, ecc..).</p>"},{"location":"dati/esplora/#informazioni-sui-file","title":"Informazioni sui file","text":""},{"location":"dati/esplora/#file","title":"<code>file</code>","text":"<p>L'utility di base, preinstallata in tutti i sistemi Linux \u00e8 <code>file</code>, che \u00e8 utile per avere informazioni sul tipo di file.</p> <p>file base_category.csv</p> <pre><code>base_category.csv: CSV text\n</code></pre> <p>Con l'opzione <code>-i</code> si ottengono informazioni sull'encoding.</p> <p>file -i encoding_iso-8859-1.csv</p> <pre><code>encoding_iso-8859-1.csv: application/csv; charset=iso-8859-1\n</code></pre> <p>Conoscere l'encoding di un file di testo strutturato, da usare per fare analisi e trasformazione di dati \u00e8 un elemento essenziale, perch\u00e9 se \"mappato\" scorrettamente porta a una lettura errata dei contenuti del file.</p>"},{"location":"dati/esplora/#stat","title":"<code>stat</code>","text":"<p><code>stat</code> fornisce moltissime informazioni sui file, come le dimensioni, i permessi, la data di modifica, ecc.:</p> <p>stat encoding_iso-8859-1.csv</p> <pre><code>File: encoding_iso-8859-1.csv\nSize: 33              Blocks: 0          IO Block: 512    regular file\nDevice: 2dh/45d Inode: 266275327968286903  Links: 1\nAccess: (0644/-rw-r--r--)  Uid: ( 1000/userName)   Gid: ( 1000/userName)\nAccess: 2022-02-19 09:17:54.648775600 +0100\nModify: 2022-02-19 09:17:54.637142300 +0100\nChange: 2022-02-19 12:44:32.134296200 +0100\nBirth: -\n</code></pre>"},{"location":"dati/esplora/#encoding","title":"Encoding","text":"<p>Conoscere la codifica dei caratteri \u00e8 un elemento chiave per la loro lettura. Specie con formati di testo come il <code>CSV</code>, in cui le informazioni sull'encoding non sono scritte all'interno del file (se va bene sono riportate in un testo che descrive il file).</p> <p>Si pu\u00f2 provare - come visto sopra - a estrapolare con <code>file</code> (o con <code>chardet</code>).</p> <p>Un'esperienza comune di barriera all'uso di un file di testo, di cui non si conosce l'encoding, \u00e8 quella ad esempio della errata lettura dei caratteri accentati, in cui un file come questo, viene letto male (<code>Cefal\ufffd</code>, sarebbe <code>Cefal\u00f9</code>):</p> encoding_iso-8859-1.csv<pre><code>id,nomeComune\n1,Cefal\ufffd\n2,Milano\n3,Orr\ufffd\n</code></pre> <p>Tip</p> <p>Spesso le utility hanno impostato come encoding standard di lettura l'<code>UTF-8</code>. Per leggere correttamente il file di sopra se ne pu\u00f2 provare a estrapolare la codifica corretta con <code>file</code> (o con <code>chardet</code>) e poi se serve trasformarla in quella di output desiderata.</p>"},{"location":"dati/esplora/#le-misure","title":"Le \"misure\"","text":"<p>A un file testuale di dati \u00e8 importante \"prendere le misure\", come il numero di righe, il numero di caratteri, numero di colonne (se \u00e8 un file a griglia tabellare), ecc..</p>"},{"location":"dati/esplora/#wc","title":"<code>wc</code>","text":"<p><code>wc</code> \u00e8 una delle utility pi\u00f9 importanti per \"esplorare\" file di testo.Questo il file di input di esempio:</p> wc-01.csv<pre><code>id,titolo\n1,La spada nella roccia\n2,Il mago di oz\n</code></pre> comando output descrizione <code>wc wc-01.csv</code> <code>3 9 50 wc-01.csv</code> numero di righe, parole e byte <code>wc -l wc-01.csv</code> <code>3 wc-01.csv</code> numero di righe <code>wc -w wc-01.csv</code> <code>9 wc-01.csv</code> numero di parole  (separate da spazio) <code>wc -c wc-01.csv</code> <code>50 wc-01.csv</code> numero di byte <code>wc --max-line-length wc-01.csv</code> <code>23 wc-01.csv</code> il numero di caratteri  della riga pi\u00f9 lunga <p>Tip</p> <p>Se si vuole soltanto il numero di righe, senza il nome del file, il comando \u00e8 <code>&lt;wc-01.csv wc -l</code></p> <p><code>wc</code> non \u00e8 per\u00f2 un comando che interpreta il tipo di formato, e quindi in presenza di un file di input come questo sottostante - un CSV di due record (pi\u00f9 intestazione) e due colonne - il conteggio delle righe sar\u00e0 pari a 8.</p> <pre><code>FieldA,FieldB\n\"Come\nQuando\nFuori\nPiove\",Ciao\nMiao,\"Uno\nDue\nTre\"\n</code></pre> <p>Questo file - vedi anteprima sottostante - contiene dei ritorni a capo nelle celle, e per ognuno viene conteggiata una riga.</p> FieldA FieldB ComeQuandoFuoriPiove Ciao Miao UnoDueTre <p>Attenzione</p> <p>Per poter conteggiare il numero di record di un <code>CSV</code> come questo, \u00e8 necessario usare una utility che tenga conto delle caratteristiche del formato (CSV aware), come ad esempio il fatto che in una cella ci possono essere pi\u00f9 righe.</p>"},{"location":"dati/esplora/#miller","title":"Miller","text":"<p>Miller \u00e8 una straordinaria utility per leggere, analizzare e trasformare file di testo strutturati. \u00c8 capace di interpretare le caratteristiche dei formati che supporta (<code>CSV</code>, <code>TSV</code>, <code>JSON</code>, ecc.) e quindi ad esempio contare correttamente il numero di record di un file <code>CSV</code>, anche in presenza di celle con ritorni a capo (vedi esempio soprastante).</p> <p>mlr --csv tail -n 1 then put '$righe=NR' then cut -f righe wc-02.csv</p> <pre><code>righe\n2\n</code></pre> <p>Alcune note:</p> <ul> <li><code>--csv</code> per impostare formato di input e output;</li> <li><code>tail -n 1</code> per estrarre l'ultima riga;</li> <li><code>put '$righe=NR'</code> per creare il campo <code>righe</code> e valorizzarlo con il numero di riga della riga corrente - <code>NR</code> - che qui \u00e8 l'ultima;</li> <li><code>cut -f righe</code>, per avere in output soltanto il campo precedentemente creato.</li> </ul> <p>Se si vuole in output anche il numero di colonne, il comando si pu\u00f2 modificare in questo modo:</p> <pre><code>mlr --csv tail -n 1 then put '$righe=NR;$colonne=NF-1' then cut -f righe,colonne wc-02.csv\n</code></pre> <p>Con <code>$colonne=NF-1</code> \u00e8 stato aggiunto un campo che conta il numero di campi - <code>NF</code> - presenti nella riga corrente, a cui \u00e8 stato sottratto 1, perch\u00e9 \u00e8 stata aggiunta la colonna che d\u00e0 conto del numero di righe, che non fa parte delle colonne pre esistenti nel file di input.</p> <p>In output:</p> <pre><code>righe,colonne\n2,2\n</code></pre> <p>Un esempio pi\u00f9 eclatante \u00e8 quando il file di input \u00e8 un <code>JSON</code> come quello sottostante, composto da 2 \"record\" (qui \u00e8 improprio come termine, ma per dare l'idea), ognuno con due campi.</p> <pre><code>[\n  {\n    \"id\": 1,\n    \"titolo\": \"La spada nella roccia\"\n  },\n  {\n    \"id\": 2,\n    \"titolo\": \"Il mago di oz\"\n  }\n]\n</code></pre> <p>Il comando</p> <pre><code>mlr --j2c tail -n 1 then put '$righe=NR;$colonne=NF-1' then cut -f righe,colonne input.json\n</code></pre> <p>restituir\u00e0 sempre (<code>--j2c</code> \u00e8 per trasformare l'input in <code>JSON</code> in <code>CSV</code>)</p> <pre><code>righe,colonne\n2,2\n</code></pre> <p>Info</p> <p>A Miller \u00e8 dedicata una monografia di questo sito.</p>"},{"location":"dati/esplora/#csvkit","title":"<code>csvkit</code>","text":"<p><code>csvkit</code> mette a disposizione il comando <code>csvstat</code>, che restituisce tante \"misure\" della sorgente dati di input.</p> <p>Ad esempio a partire dal file sottostante</p> base.csv<pre><code>nome,dataNascita,altezza,peso\nandy,1973-05-08,176,86.5\nchiara,1993-12-13,162,58.3\nguido,2001-01-22,196,90.4\n</code></pre> <p>con il comando</p> <pre><code>csvstat --csv ./base.csv\n</code></pre> <p>restituisce</p> column_id column_name type nulls unique min max sum mean median stdev len freq 1 nome Text False 3 6 andy, chiara, guido 2 dataNascita Date False 3 1973-05-08 2001-01-22 1973-05-08, 1993-12-13, 2001-01-22 3 altezza Number False 3 162 196 534 178 176 17.088 176, 162, 196 4 peso Number False 3 58.3 90.4 235.2 78.4 86.5 17.516 86.5, 58.3, 90.4 <p>Di interesse</p> <p>In questo output, a proposito di misure, anche i valori univoci, i valori nulli e i parametri statistici di base.</p>"},{"location":"dati/esplora/#xsv","title":"<code>xsv</code>","text":"<p><code>xsv</code> mette a disposizione il comando <code>stats</code>, che restituisce tante \"misure\" della sorgente dati di input.</p> <p>Ad esempio a partire dal file sottostante</p> base.csv<pre><code>nome,dataNascita,altezza,peso\nandy,1973-05-08,176,86.5\nchiara,1993-12-13,162,58.3\nguido,2001-01-22,196,90.4\n</code></pre> <p>con il comando</p> <pre><code>xsv stats  --everything ./base.csv\n</code></pre> <p>restituisce</p> field type sum min max min_length max_length mean stddev median mode cardinality nome Unicode andy guido 4 6 N/A 3 dataNascita Unicode 1973-05-08 2001-01-22 10 10 N/A 3 altezza Integer 534 162 196 3 3 178 13.9522996909709 176 N/A 3 peso Float 235.20000000000002 58.3 90.4 4 4 78.4 14.30174814489474 86.5 N/A 3 <p>Di interesse</p> <p>In questo output, a proposito di misure, anche i valori univoci e i parametri statistici di base.</p>"},{"location":"dati/esplora/#le-forme-la-descrizione-dei-dati","title":"Le \"forme\" (la descrizione dei dati)","text":"<p>Intro</p> <p>Quale \u00e8 lo schema dati? Quale \u00e8 il formato? Qual \u00e8 il separatore dei campi? Poter leggere o estrarre da una sorgente dati questi elementi descrittivi \u00e8 prezioso, delle volte \u00e8 propedeutico al loro uso.</p> <p>Alcuni dei formati di file di testo strutturato (<code>CSV</code>, <code>TSV</code>, quelli a larghezza fissa, ecc.), non sono associati alla definizione dei campi da cui sono composti, n\u00e9 rendono disponibile informazioni sul loro <code>encoding</code> o sul separatore di campi.</p> <p>N\u00e8 tantomeno quale sia il formato. Perch\u00e9 \u00e8 vero che spesso lo possiamo dedurre dall'estensione (<code>.csv</code>, <code>.json</code>), ma questa non \u00e8 detto che sia sempre associata al file (un file <code>CSV</code> si potrebbe chiamare semplicemente <code>input</code> e non <code>input.csv</code>), e non \u00e8 ci sono controlli di coerenza tra un'estensione e il contenuto del file (un file con estensione <code>.tsv</code>, potrebbe contenere un <code>JSON</code>). Un'estensione \u00e8 essenzialmente un'etichetta.</p> <p>Da dati come quelli di sotto (questo <code>CSV</code>), non \u00e8 possibile leggere che si tratta di campi rispettivamente, con stringhe di testo, date, numeri interi e numeri decimali. Sono tutte stringhe. Che il separatore sia il <code>;</code> si deduce visualizzandolo, ma non \u00e8 un'informazione che accompagna il file.</p> base-semicolon.csv<pre><code>nome;dataNascita;altezza;peso\nandy;1973-05-08;176;86.5\nchiara;1993-12-13;162;58.3\nguido;2001-01-22;196;90.4\n</code></pre> <p>Con il formato <code>JSON</code>  - per schema e tipo campi - va un po' meglio ma le date sono sempre stringhe e non c'\u00e8 differenza tra numeri interi e decimali. I tipi di campo possibili in questo formato sono infatti: <code>string</code>, <code>number</code>, <code>boolean</code> (<code>\"sposato\":true</code>) e <code>null</code> (<code>\"secondoNome\":null</code>).</p> <pre><code>[\n  {\n    \"nome\": \"andy\",\n    \"dataNascita\": \"1973-05-08\",\n    \"altezza\": 176,\n    \"peso\": 86.5\n  },\n  {\n    \"nome\": \"chiara\",\n    \"dataNascita\": \"1993-12-13\",\n    \"altezza\": 162,\n    \"peso\": 58.3\n  },\n  {\n    \"nome\": \"guido\",\n    \"dataNascita\": \"2001-01-22\",\n    \"altezza\": 196,\n    \"peso\": 90.4\n  }\n]\n</code></pre> <p>Quello che \u00e8 possibile fare \u00e8 il cosiddetto inferencing, ovvero dedurre tipo di campi, encoding, separatore e formato, leggendo il contenuto dei dati di input.</p> <p>Nota</p> <p>Come visto sopra, <code>csvstat</code> e <code>xsv stats</code> estraggono il tipo di campo, e qui a seguire non verranno di nuovi riportati</p>"},{"location":"dati/esplora/#frictionless","title":"<code>frictionless</code>","text":"<p><code>frictionless</code> con il comando <code>describe</code>, consente di estrarre lo schema dati, l'encoding, il separatore di campo (per i formati in cui \u00e8 applicabile) e il formato, a partire da una sorgente di input.</p> <p>A partire ad esempio da questo file</p> base-semicolon.csv<pre><code>nome;dataNascita;altezza;peso\nandy;1973-05-08;176;86.5\nchiara;1993-12-13;162;58.3\nguido;2001-01-22;196;90.4\n</code></pre> <p>con il comando <code>frictionless describe ./base-semicolon.csv</code>, verranno restituiti (tra le altre cose) il tipo per ogni campo di input, l'encoding, il formato e il separatore:</p> <pre><code># --------\n# metadata: ./base-semicolon.csv\n# --------\n\ndialect:\n  delimiter: ;\nencoding: utf-8\nformat: csv\nhashing: md5\nname: base-semicolon\npath: ./base-semicolon.csv\nprofile: tabular-data-resource\nschema:\n  fields:\n    - name: nome\n      type: string\n    - name: dataNascita\n      type: date\n    - name: altezza\n      type: integer\n    - name: peso\n      type: number\nscheme: file\n</code></pre>"},{"location":"dati/trasforma/","title":"Trasforma","text":"<p>Attenzione</p> <p>Questa pagina \u00e8 una bozza, \u00e8 ancora quasi tutta da scrivere.</p>"},{"location":"dati/trasforma/#stuttura","title":"Stuttura","text":""},{"location":"dati/trasforma/#da-wide-a-long","title":"Da wide a long","text":""},{"location":"dati/trasforma/#da-long-a-wide","title":"Da long a wide","text":""},{"location":"dati/trasforma/#pivot","title":"Pivot","text":""},{"location":"dati/trasforma/#encoding","title":"Encoding","text":"<p>Tante volte \u00e8 necessario trasformare i file da una codifica di caratteri a un'altra. Un caso classico \u00e8 quello di un lavoro in cui la gran parte dei dati \u00e8 in <code>UTF-8</code>, ma alcuni file \"esterni\" sono in <code>Windows-1252</code> e sono da uniformare ai precedenti.</p> <p>\u00c8 disponibile l'utility <code>iconv</code>, che si occupa proprio di questo. Ad esempio per trasformare un file da <code>Windows-1252</code> a <code>UTF-8</code>, il comando \u00e8:</p> <pre><code>iconv -f Windows-1252 -t UTF-8 input &gt;output\n</code></pre>"},{"location":"frictionless/","title":"frictionless","text":"<p>Attenzione</p> <p>La sezione per <code>frictionless</code> non \u00e8 ancora pronta. \u00c8 possibile fare riferimento al sito ufficiale.</p> <p>\u00c8 l'applicazione a riga di comando del framework Frictionless, basata su <code>Python</code>. Si installa con il comando:</p> <pre><code>pip install frictionless\n</code></pre> <p>Ha 4 comandi principali:</p> <ul> <li>describe, per descrivere dati;</li> <li>extract, per estrarli;</li> <li>validate, per validarli;</li> <li>transform, per trasformarli.</li> </ul>"},{"location":"frictionless/descrivere/","title":"Descrivere dati","text":"<p>Il comando <code>describe</code> restituisce la descrizione di una sorgente dati. Il file sottostante verr\u00e0 usato come input di esempio.</p> nome dataNascita altezza peso andy 1973-05-08 176 86.5 chiara 1993-12-13 162 58.3 guido 2001-01-22 196 90.4 <p>Si lancia come opzione, del programma principale:</p> <pre><code>frictionless describe ./base.csv\n</code></pre> <p>L'output (il formato predefinito \u00e8 lo <code>YAML</code>) sar\u00e0:</p> <pre><code># --------\n# metadata: ./base.csv\n# --------\n\nencoding: utf-8\nformat: csv\nhashing: md5\nname: base\npath: ./base.csv\nprofile: tabular-data-resource\nschema:\n  fields:\n    - name: nome\n      type: string\n    - name: dataNascita\n      type: date\n    - name: altezza\n      type: integer\n    - name: peso\n      type: number\nscheme: file\n</code></pre> <p>Per ogni sorgente analizzata:</p> <ul> <li>il separatore di campo. Quando non appare nell'output (come qui sopra), vuol dire che \u00e8 quello predefinito, ovvero in questo caso la <code>,</code> (sotto maggiori dettagli);</li> <li>l'encoding;</li> <li>il formato;</li> <li>il nome;</li> <li>il path;</li> <li>lo schema dati, con nome e tipo di ogni campo. Il tipo di campo, per formati in cui non \u00e8 definito (come il <code>CSV</code>) \u00e8 dedotto e quindi pu\u00f2 non sempre essere corretto.</li> </ul> <p>Se il separatore non \u00e8 la <code>,</code>, ovvero il carattere considerato standard per un <code>CSV</code>, questo verr\u00e0 documentato in output.</p> <p>Se ad esempio in input si ha questo file:</p> base-semicolon.csv<pre><code>nome;dataNascita;altezza;peso\nandy;1973-05-08;176;86.5\nchiara;1993-12-13;162;58.3\nguido;2001-01-22;196;90.4\n</code></pre> <p>Il comando <code>describe</code> restituir\u00e0 in pi\u00f9 la sezione <code>dialect</code>, con valorizzata la propriet\u00e0 <code>delimiter</code>:</p> <pre><code># --------\n# metadata: ./base-semicolon.csv\n# --------\n\ndialect:\n  delimiter: ;\nencoding: utf-8\nformat: csv\nhashing: md5\nname: base-semicolon\npath: ./base-semicolon.csv\nprofile: tabular-data-resource\nschema:\n  fields:\n    - name: nome\n      type: string\n    - name: dataNascita\n      type: date\n    - name: altezza\n      type: integer\n    - name: peso\n      type: number\nscheme: file\n</code></pre> <p>\u00c8 possibile avere l'output in <code>JSON</code> aggiungendo l'opzione <code>--json</code>.</p> <p>Descrivere un file JSON</p> <p>Per descrivere un file JSON si deve usare l'opzione <code>path</code>:</p> <pre><code>frictionless describe --path input.json\n</code></pre>"},{"location":"frictionless/descrivere/#aggiungere-le-statistiche-della-risorsa","title":"Aggiungere le statistiche della risorsa","text":"<p>L'opzione <code>--stats</code> del comando <code>describe</code> \u00e8 molto utile, perch\u00e9 restituisce alcune informazioni numeriche alla descrizione della risorsa:</p> <ul> <li>le dimensioni in byte;</li> <li>il numero di campi;</li> <li>il numero di righe;</li> <li>l'impronta hash.</li> </ul> <p>Si lancia cos\u00ec (\u00e8 applicato al file <code>base-semicolon.csv</code>)</p> <pre><code>frictionless describe --stats base-semicolon.csv\n</code></pre> <p>e restituisce in output le propriet\u00e0 di <code>stats</code> (vedi sotto).</p> <pre><code># --------\n# metadata: base-semicolon.csv\n# --------\n\npath: base-semicolon.csv\nname: base-semicolon\nprofile: tabular-data-resource\nscheme: file\nformat: csv\nhashing: md5\nstats:\n  hash: 7ed86c23c432fd1b6586ce6399d1ac1c\n  bytes: 108\n  fields: 4\n  rows: 3\nencoding: utf-8\ndialect:\n  delimiter: ;\nschema:\n  fields:\n    - type: string\n      name: nome\n    - type: date\n      name: dataNascita\n    - type: integer\n      name: altezza\n    - type: number\n      name: peso\n</code></pre>"},{"location":"frictionless/ricette/","title":"Ricette frictionless","text":""},{"location":"frictionless/ricette/#validare","title":"Validare","text":""},{"location":"frictionless/ricette/#impostare-i-valori-da-considerare-come-nulli","title":"Impostare i valori da considerare come nulli","text":"<p>Ad esempio, per dire di considerare come valori nulli, le celle che contengono <code>NULL</code> o <code>N</code>:</p> <pre><code>frictionless validate --field-missing-values \"NULL,N\" input.csv\n</code></pre>"},{"location":"frictionless/ricette/#non-validare-una-o-piu-colonne","title":"Non validare una o pi\u00f9 colonne","text":"<p>\u00c8 possibile farlo sia tramite nome colonna, che numero di colonna.</p> <pre><code>frictionless validate --skip-fields \"fieldA,fieldB\" input.csv\n</code></pre> <pre><code>frictionless validate --skip-fields \"22,46\" input.csv\n</code></pre>"},{"location":"frictionless/ricette/#eseguire-soltanto-certi-tipi-di-test","title":"Eseguire soltanto certi tipi di test","text":"<pre><code>frictionless validate --pick-errors \"type-error,missing-cell\" input.csv\n</code></pre>"},{"location":"frictionless/ricette/#non-eseguire-uno-specifico-test-di-validazione","title":"Non eseguire uno specifico test di validazione","text":"<p>Ad esempio non eseguire il controllo (<code>missing-label</code>) che verifica che tutte le colonne abbiano un'etichetta:</p> <pre><code>frictionless validate --skip-errors missing-label input.csv\n</code></pre>"},{"location":"frictionless/ricette/#definire-il-separatore-di-campo-senza-inferencing","title":"Definire il separatore di campo, senza inferencing","text":"<p><code>frictionless</code>, quando legge un file in formato <code>CSV</code>, prova a estrarne automaticamente il separatore di campo. Se si vuole forzarne la definizione:</p> <pre><code>frictionless validate --dialect '{\"delimiter\": \";\"}' input.csv\n</code></pre>"},{"location":"frictionless/ricette/#aumentare-lampiezza-di-informazioni-da-usare-per-fare-inferencing-su-un-file","title":"Aumentare l'ampiezza di informazioni da usare per fare inferencing su un file","text":"<p><code>frictionless</code> per dedurre ad esempio l'encoding di un file <code>CSV</code> (e tante altre informazioni) usa un byte sample di 10.000. Se si vuole ampliare, c'\u00e8 l'opzione <code>--buffer-size</code>.</p> <pre><code>frictionless validate --buffer-size 250000 input.csv\n</code></pre>"},{"location":"frictionless/ricette/#validare-un-file-json","title":"Validare un file JSON","text":"<p>Per validare un file JSON si deve usare l'opzione <code>path</code>:</p> <pre><code>frictionless validate --path input.json\n</code></pre>"},{"location":"frictionless/ricette/#estrarre","title":"Estrarre","text":""},{"location":"frictionless/ricette/#come-estrarre-uno-specifico-foglio-di-un-foglio-elettronico","title":"Come estrarre uno specifico foglio, di un foglio elettronico","text":"<pre><code>frictionless extract input.ods --dialect '{\"sheet\": \"Sheet2\"}'\n</code></pre> <p>Oppure utilizzando l'opzione <code>--sheet</code></p> <pre><code>frictionless extract input.ods --sheet \"sheetName\"\n</code></pre>"},{"location":"frictionless/ricette/#estrarre-soltanto-le-righe-che-passano-la-validazione","title":"Estrarre soltanto le righe che passano la validazione","text":"<pre><code>frictionless extract --valid  input.csv --csv\n</code></pre> <p>Note</p> <p>Da un'idea di <code>aborruso</code>, proposta tramite le issue <code>#963</code> e <code>#1004</code>.</p>"},{"location":"frictionless/ricette/#estrarre-soltanto-le-righe-che-non-passano-la-validazione","title":"Estrarre soltanto le righe che non passano la validazione","text":"<pre><code>frictionless extract --invalid  input.csv --csv\n</code></pre>"},{"location":"frictionless/tipi-errore/","title":"Tipi di errore","text":"<p>A seguire i tipi di errore gestiti da <code>frictionless</code>.</p>"},{"location":"frictionless/tipi-errore/#general-error","title":"General Error","text":"<p>Code: <code>general-error</code>  Tags: <code>-</code>  Template: <code>General error: {note}</code>  Description: <code>There is an error.</code> </p>"},{"location":"frictionless/tipi-errore/#row-error","title":"Row Error","text":"<p>Code: <code>row-error</code>  Tags: <code>#table #row</code>  Template: <code>Row Error</code>  Description: <code>Row Error</code> </p>"},{"location":"frictionless/tipi-errore/#cell-error","title":"Cell Error","text":"<p>Code: <code>cell-error</code>  Tags: <code>#table #row #cell</code>  Template: <code>Cell Error</code>  Description: <code>Cell Error</code> </p>"},{"location":"frictionless/tipi-errore/#extra-cell","title":"Extra Cell","text":"<p>Code: <code>extra-cell</code>  Tags: <code>#table #row #cell</code>  Template: <code>Row at position \"{rowPosition}\" has an extra value in field at position \"{fieldPosition}\"</code>  Description: <code>This row has more values compared to the header row (the first row in the data source). A key concept is that all the rows in tabular data must have the same number of columns.</code> </p>"},{"location":"frictionless/tipi-errore/#missing-cell","title":"Missing Cell","text":"<p>Code: <code>missing-cell</code>  Tags: <code>#table #row #cell</code>  Template: <code>Row at position \"{rowPosition}\" has a missing cell in field \"{fieldName}\" at position \"{fieldPosition}\"</code>  Description: <code>This row has less values compared to the header row (the first row in the data source). A key concept is that all the rows in tabular data must have the same number of columns.</code> </p>"},{"location":"frictionless/tipi-errore/#type-error","title":"Type Error","text":"<p>Code: <code>type-error</code>  Tags: <code>#table #row #cell</code>  Template: <code>Type error in the cell \"{cell}\" in row \"{rowPosition}\" and field \"{fieldName}\" at position \"{fieldPosition}\": {note}</code>  Description: <code>The value does not match the schema type and format for this field.</code> </p>"},{"location":"frictionless/tipi-errore/#constraint-error","title":"Constraint Error","text":"<p>Code: <code>constraint-error</code>  Tags: <code>#table #row #cell</code>  Template: <code>The cell \"{cell}\" in row at position \"{rowPosition}\" and field \"{fieldName}\" at position \"{fieldPosition}\" does not conform to a constraint: {note}</code>  Description: <code>A field value does not conform to a constraint.</code> </p>"},{"location":"frictionless/tipi-errore/#unique-error","title":"Unique Error","text":"<p>Code: <code>unique-error</code>  Tags: <code>#table #row #cell</code>  Template: <code>Row at position \"{rowPosition}\" has unique constraint violation in field \"{fieldName}\" at position \"{fieldPosition}\": {note}</code>  Description: <code>This field is a unique field but it contains a value that has been used in another row.</code> </p>"},{"location":"frictionless/tipi-errore/#truncated-value","title":"Truncated Value","text":"<p>Code: <code>truncated-value</code>  Tags: <code>#table #row #cell</code>  Template: <code>The cell {cell} in row at position {rowPosition} and field {fieldName} at position {fieldPosition} has an error: {note}</code>  Description: <code>The value is possible truncated.</code> </p>"},{"location":"frictionless/tipi-errore/#forbidden-value","title":"Forbidden Value","text":"<p>Code: <code>forbidden-value</code>  Tags: <code>#table #row #cell</code>  Template: <code>The cell {cell} in row at position {rowPosition} and field {fieldName} at position {fieldPosition} has an error: {note}</code>  Description: <code>The value is forbidden.</code> </p>"},{"location":"frictionless/tipi-errore/#sequential-value","title":"Sequential Value","text":"<p>Code: <code>sequential-value</code>  Tags: <code>#table #row #cell</code>  Template: <code>The cell {cell} in row at position {rowPosition} and field {fieldName} at position {fieldPosition} has an error: {note}</code>  Description: <code>The value is not sequential.</code> </p>"},{"location":"frictionless/tipi-errore/#error","title":"Error","text":"<p>Code: <code>error</code>  Tags: <code>-</code>  Template: <code>{note}</code>  Description: <code>Error</code> </p>"},{"location":"frictionless/tipi-errore/#file-error","title":"File Error","text":"<p>Code: <code>file-error</code>  Tags: <code>#file</code>  Template: <code>General file error: {note}</code>  Description: <code>There is a file error.</code> </p>"},{"location":"frictionless/tipi-errore/#hash-count-error","title":"Hash Count Error","text":"<p>Code: <code>hash-count-error</code>  Tags: <code>#file</code>  Template: <code>The data source does not match the expected hash count: {note}</code>  Description: <code>This error can happen if the data is corrupted.</code> </p>"},{"location":"frictionless/tipi-errore/#byte-count-error","title":"Byte Count Error","text":"<p>Code: <code>byte-count-error</code>  Tags: <code>#file</code>  Template: <code>The data source does not match the expected byte count: {note}</code>  Description: <code>This error can happen if the data is corrupted.</code> </p>"},{"location":"frictionless/tipi-errore/#package-error","title":"Package Error","text":"<p>Code: <code>package-error</code>  Tags: <code>-</code>  Template: <code>The data package has an error: {note}</code>  Description: <code>A validation cannot be processed.</code> </p>"},{"location":"frictionless/tipi-errore/#resource-error","title":"Resource Error","text":"<p>Code: <code>resource-error</code>  Tags: <code>-</code>  Template: <code>The data resource has an error: {note}</code>  Description: <code>A validation cannot be processed.</code> </p>"},{"location":"frictionless/tipi-errore/#pipeline-error","title":"Pipeline Error","text":"<p>Code: <code>pipeline-error</code>  Tags: <code>-</code>  Template: <code>Pipeline is not valid: {note}</code>  Description: <code>Provided pipeline is not valid.</code> </p>"},{"location":"frictionless/tipi-errore/#inquiry-error","title":"Inquiry Error","text":"<p>Code: <code>inquiry-error</code>  Tags: <code>-</code>  Template: <code>Inquiry is not valid: {note}</code>  Description: <code>Provided inquiry is not valid.</code> </p>"},{"location":"frictionless/tipi-errore/#control-error","title":"Control Error","text":"<p>Code: <code>control-error</code>  Tags: <code>-</code>  Template: <code>Control is not valid: {note}</code>  Description: <code>Provided control is not valid.</code> </p>"},{"location":"frictionless/tipi-errore/#dialect-error","title":"Dialect Error","text":"<p>Code: <code>dialect-error</code>  Tags: <code>-</code>  Template: <code>Dialect is not valid: {note}</code>  Description: <code>Provided dialect is not valid.</code> </p>"},{"location":"frictionless/tipi-errore/#layout-error","title":"Layout Error","text":"<p>Code: <code>layout-error</code>  Tags: <code>-</code>  Template: <code>Layout is not valid: {note}</code>  Description: <code>Provided layout is not valid.</code> </p>"},{"location":"frictionless/tipi-errore/#schema-error","title":"Schema Error","text":"<p>Code: <code>schema-error</code>  Tags: <code>-</code>  Template: <code>Schema is not valid: {note}</code>  Description: <code>Provided schema is not valid.</code> </p>"},{"location":"frictionless/tipi-errore/#field-error","title":"Field Error","text":"<p>Code: <code>field-error</code>  Tags: <code>-</code>  Template: <code>Field is not valid: {note}</code>  Description: <code>Provided field is not valid.</code> </p>"},{"location":"frictionless/tipi-errore/#report-error","title":"Report Error","text":"<p>Code: <code>report-error</code>  Tags: <code>-</code>  Template: <code>Report is not valid: {note}</code>  Description: <code>Provided report is not valid.</code> </p>"},{"location":"frictionless/tipi-errore/#status-error","title":"Status Error","text":"<p>Code: <code>status-error</code>  Tags: <code>-</code>  Template: <code>Status is not valid: {note}</code>  Description: <code>Provided status is not valid.</code> </p>"},{"location":"frictionless/tipi-errore/#check-error","title":"Check Error","text":"<p>Code: <code>check-error</code>  Tags: <code>-</code>  Template: <code>Check is not valid: {note}</code>  Description: <code>Provided check is not valid</code> </p>"},{"location":"frictionless/tipi-errore/#step-error","title":"Step Error","text":"<p>Code: <code>step-error</code>  Tags: <code>-</code>  Template: <code>Step is not valid: {note}</code>  Description: <code>Provided step is not valid</code> </p>"},{"location":"frictionless/tipi-errore/#source-error","title":"Source Error","text":"<p>Code: <code>source-error</code>  Tags: <code>-</code>  Template: <code>The data source has not supported or has inconsistent contents: {note}</code>  Description: <code>Data reading error because of not supported or inconsistent contents.</code> </p>"},{"location":"frictionless/tipi-errore/#scheme-error","title":"Scheme Error","text":"<p>Code: <code>scheme-error</code>  Tags: <code>-</code>  Template: <code>The data source could not be successfully loaded: {note}</code>  Description: <code>Data reading error because of incorrect scheme.</code> </p>"},{"location":"frictionless/tipi-errore/#format-error","title":"Format Error","text":"<p>Code: <code>format-error</code>  Tags: <code>-</code>  Template: <code>The data source could not be successfully parsed: {note}</code>  Description: <code>Data reading error because of incorrect format.</code> </p>"},{"location":"frictionless/tipi-errore/#encoding-error","title":"Encoding Error","text":"<p>Code: <code>encoding-error</code>  Tags: <code>-</code>  Template: <code>The data source could not be successfully decoded: {note}</code>  Description: <code>Data reading error because of an encoding problem.</code> </p>"},{"location":"frictionless/tipi-errore/#hashing-error","title":"Hashing Error","text":"<p>Code: <code>hashing-error</code>  Tags: <code>-</code>  Template: <code>The data source could not be successfully hashed: {note}</code>  Description: <code>Data reading error because of a hashing problem.</code> </p>"},{"location":"frictionless/tipi-errore/#compression-error","title":"Compression Error","text":"<p>Code: <code>compression-error</code>  Tags: <code>-</code>  Template: <code>The data source could not be successfully decompressed: {note}</code>  Description: <code>Data reading error because of a decompression problem.</code> </p>"},{"location":"frictionless/tipi-errore/#storage-error","title":"Storage Error","text":"<p>Code: <code>storage-error</code>  Tags: <code>-</code>  Template: <code>The storage has an error: {note}</code>  Description: <code>A storage's operation cannot be performed</code> </p>"},{"location":"frictionless/tipi-errore/#task-error","title":"Task Error","text":"<p>Code: <code>task-error</code>  Tags: <code>-</code>  Template: <code>The task has an error: {note}</code>  Description: <code>General task-level error.</code> </p>"},{"location":"frictionless/tipi-errore/#table-error","title":"Table Error","text":"<p>Code: <code>table-error</code>  Tags: <code>#table</code>  Template: <code>General table error: {note}</code>  Description: <code>There is a table error.</code> </p>"},{"location":"frictionless/tipi-errore/#header-error","title":"Header Error","text":"<p>Code: <code>header-error</code>  Tags: <code>#table #header</code>  Template: <code>Cell Error</code>  Description: <code>Cell Error</code> </p>"},{"location":"frictionless/tipi-errore/#blank-header","title":"Blank Header","text":"<p>Code: <code>blank-header</code>  Tags: <code>#table #header</code>  Template: <code>Header is completely blank</code>  Description: <code>This header is empty. A header should contain at least one value.</code> </p>"},{"location":"frictionless/tipi-errore/#label-error","title":"Label Error","text":"<p>Code: <code>label-error</code>  Tags: <code>#table #header #label</code>  Template: <code>Label Error</code>  Description: <code>Label Error</code> </p>"},{"location":"frictionless/tipi-errore/#extra-label","title":"Extra Label","text":"<p>Code: <code>extra-label</code>  Tags: <code>#table #header #label</code>  Template: <code>There is an extra label \"{label}\" in header at position \"{fieldPosition}\"</code>  Description: <code>The header of the data source contains label that does not exist in the provided schema.</code> </p>"},{"location":"frictionless/tipi-errore/#missing-label","title":"Missing Label","text":"<p>Code: <code>missing-label</code>  Tags: <code>#table #header #label</code>  Template: <code>There is a missing label in the header's field \"{fieldName}\" at position \"{fieldPosition}\"</code>  Description: <code>Based on the schema there should be a label that is missing in the data's header.</code> </p>"},{"location":"frictionless/tipi-errore/#blank-label","title":"Blank Label","text":"<p>Code: <code>blank-label</code>  Tags: <code>#table #header #label</code>  Template: <code>Label in the header in field at position \"{fieldPosition}\" is blank</code>  Description: <code>A label in the header row is missing a value. Label should be provided and not be blank.</code> </p> <p>Campo senza etichetta (sotto \u00e8 il secondo).</p> <pre><code>fieldA,\n1,bene\n4,male\n</code></pre>"},{"location":"frictionless/tipi-errore/#duplicate-label","title":"Duplicate Label","text":"<p>Code: <code>duplicate-label</code>  Tags: <code>#table #header #label</code>  Template: <code>Label \"{label}\" in the header at position \"{fieldPosition}\" is duplicated to a label: {note}</code>  Description: <code>Two columns in the header row have the same value. Column names should be unique.</code> </p> <p>Nome campo duplicato.</p> <pre><code>fieldA,fieldA\n1,bene\n4,male\n</code></pre>"},{"location":"frictionless/tipi-errore/#incorrect-label","title":"Incorrect Label","text":"<p>Code: <code>incorrect-label</code>  Tags: <code>#table #header #label</code>  Template: <code>Label \"{label}\" in field {fieldName} at position \"{fieldPosition}\" does not match the field name in the schema</code>  Description: <code>One of the data source header does not match the field name defined in the schema.</code> </p>"},{"location":"frictionless/tipi-errore/#blank-row","title":"Blank Row","text":"<p>Code: <code>blank-row</code>  Tags: <code>#table #row</code>  Template: <code>Row at position \"{rowPosition}\" is completely blank</code>  Description: <code>This row is empty. A row should contain at least one value.</code> </p> <p>Riga vuota.</p> <pre><code>fieldA,fieldB\n1,bene\n,\n4,male\n</code></pre>"},{"location":"frictionless/tipi-errore/#primarykey-error","title":"PrimaryKey Error","text":"<p>Code: <code>primary-key-error</code>  Tags: <code>#table #row</code>  Template: <code>Row at position \"{rowPosition}\" violates the primary key: {note}</code>  Description: <code>Values in the primary key fields should be unique for every row</code> </p>"},{"location":"frictionless/tipi-errore/#foreignkey-error","title":"ForeignKey Error","text":"<p>Code: <code>foreign-key-error</code>  Tags: <code>#table #row</code>  Template: <code>Row at position \"{rowPosition}\" violates the foreign key: {note}</code>  Description: <code>Values in the foreign key fields should exist in the reference table</code> </p>"},{"location":"frictionless/tipi-errore/#duplicate-row","title":"Duplicate Row","text":"<p>Code: <code>duplicate-row</code>  Tags: <code>#table #row</code>  Template: <code>Row at position {rowPosition} is duplicated: {note}</code>  Description: <code>The row is duplicated.</code> </p>"},{"location":"frictionless/tipi-errore/#row-constraint","title":"Row Constraint","text":"<p>Code: <code>row-constraint</code>  Tags: <code>#table #row</code>  Template: <code>The row at position {rowPosition} has an error: {note}</code>  Description: <code>The value does not conform to the row constraint.</code> </p>"},{"location":"frictionless/tipi-errore/#field-count-error","title":"Field Count Error","text":"<p>Code: <code>field-count-error</code>  Tags: <code>#table</code>  Template: <code>The data source does not match the expected field count: {note}</code>  Description: <code>This error can happen if the data is corrupted.</code> </p>"},{"location":"frictionless/tipi-errore/#row-count-error","title":"Row Count Error","text":"<p>Code: <code>row-count-error</code>  Tags: <code>#table</code>  Template: <code>The data source does not match the expected row count: {note}</code>  Description: <code>This error can happen if the data is corrupted.</code> </p>"},{"location":"frictionless/tipi-errore/#table-dimensions-error","title":"Table dimensions error","text":"<p>Code: <code>table-dimensions-error</code>  Tags: <code>#table</code>  Template: <code>The data source does not have the required dimensions: {note}</code>  Description: <code>This error can happen if the data is corrupted.</code> </p>"},{"location":"frictionless/tipi-errore/#deviated-value","title":"Deviated Value","text":"<p>Code: <code>deviated-value</code>  Tags: <code>#table</code>  Template: <code>There is a possible error because the value is deviated: {note}</code>  Description: <code>The value is deviated.</code> </p>"},{"location":"frictionless/validare/","title":"Validare dati","text":"<p>Il comando <code>validate</code> esegue la validazione di risorse e/o liste di risorse contenute in un datapackage.</p> <p>In input ad esempio una risorsa tabellare di questo tipo (nella tabella di sotto, i caratteri <code>\u00b7\u00b7\u00b7</code> rappresentano una cella vuota):</p> text number date type Lorem 37.256 2022-01-13 012543 A \u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 Ciao 2023-01-13 C <p>In formato <code>CSV</code> \u00e8:</p> input_00.csv<pre><code>text,number,date,,type\nLorem,37.256,2022-01-13,012543,A\n\n,Ciao,2023-01-13,C\n</code></pre> <p>Questo input ha diversi problemi:</p> <ul> <li>la colonna numero 4, non ha nome;</li> <li>la riga 3 \u00e8 completamente vuota;</li> <li>nella riga 4, manca la colonna 5.</li> </ul> <p>Il comando <code>validate</code> di <code>frictionless</code> \u00e8 in grado di riconoscerli e restituirli all'utente.</p> <p>Utilizzando il file <code>input_00.csv</code> come input e lanciando</p> <pre><code>frictionless validate input_00.csv\n</code></pre> <p>in output si avr\u00e0</p> <pre><code># -------\n# invalid: input_00.csv\n# -------\n\n## Summary\n\n+-----------------------------+-------------------+\n| Description                 | Size/Name/Count   |\n+=============================+===================+\n| File name                   | input_00.csv      |\n+-----------------------------+-------------------+\n| File size (bytes)           | 76                |\n+-----------------------------+-------------------+\n| Total Time Taken (sec)      | 0.028             |\n+-----------------------------+-------------------+\n| Total Errors                | 3                 |\n+-----------------------------+-------------------+\n| Blank Label (blank-label)   | 1                 |\n+-----------------------------+-------------------+\n| Blank Row (blank-row)       | 1                 |\n+-----------------------------+-------------------+\n| Missing Cell (missing-cell) | 1                 |\n+-----------------------------+-------------------+\n\n## Errors\n\n+-------+---------+-----------+-------------------------------------------------+\n| row   | field   | code      | message                                         |\n+=======+=========+===========+=================================================+\n|       | 4       | blank-    | Label in the header in field at position \"4\" is |\n|       |         | label     | blank                                           |\n+-------+---------+-----------+-------------------------------------------------+\n| 3     |         | blank-row | Row at position \"3\" is completely blank         |\n+-------+---------+-----------+-------------------------------------------------+\n| 4     | 5       | missing-  | Row at position \"4\" has a missing cell in field |\n|       |         | cell      | \"type\" at position \"5\"                          |\n+-------+---------+-----------+-------------------------------------------------+\n</code></pre> <p>La prima parte - <code>Summary</code> - restituisce un riepilogo. Tra le altre cose il nome del file, le sue dimensioni, il numero totale di errori, e il numero di errori per tipo. La seconda - <code>Errors</code> - restituisce i dettagli di ciascun errore (sopra evidenziati in giallo): numero di riga (la riga di intestazione \u00e8 la <code>1</code>) e di colonna in cui \u00e8 presente l'errore, e la descrizione di dettaglio dello stesso.</p> <p>\u00c8 interessante notare come gli errori estratti automaticamente, coincidano con quelli descritti \"a mano\".</p> <p>Validare un file JSON</p> <p>Per validare un file JSON si deve usare l'opzione <code>path</code>:</p> <pre><code>frictionless validate --path input.json\n</code></pre>"},{"location":"frictionless/validare/#output-di-validazione-in-formato-json-o-yaml","title":"Output di validazione in formato JSON o YAML","text":"<p>\u00c8 molto importante potere avere l'output di validazione in un formato machine readable, in modo da poter ad esempio fare una validazione giornaliera di uno o pi\u00f9 file, archiviare l'esito in un file di log e infine avvisare automaticamente l'utente se il processo non \u00e8 andato a buon fine.</p> <p>Per farlo ci sono i flag <code>--json</code> e <code>--yaml</code>. Ad esempio per l'output in <code>JSON</code>, il comando sar\u00e0:</p> <pre><code>frictionless validate input_00.csv --json\n</code></pre> Esempio di output in JSON <pre><code>{\n  \"version\": \"4.40.5\",\n  \"time\": 0.032,\n  \"errors\": [],\n  \"tasks\": [\n    {\n      \"resource\": {\n        \"path\": \"input_00.csv\",\n        \"name\": \"input_00\",\n        \"profile\": \"tabular-data-resource\",\n        \"scheme\": \"file\",\n        \"format\": \"csv\",\n        \"hashing\": \"md5\",\n        \"stats\": {\n          \"hash\": \"cf404937d0c1a8001585d5593640c7c3\",\n          \"bytes\": 76,\n          \"fields\": 5,\n          \"rows\": 3\n        },\n        \"encoding\": \"utf-8\",\n        \"schema\": {\n          \"fields\": [\n            {\n              \"type\": \"string\",\n              \"name\": \"text\"\n            },\n            {\n              \"type\": \"string\",\n              \"name\": \"number\"\n            },\n            {\n              \"type\": \"date\",\n              \"name\": \"date\"\n            },\n            {\n              \"type\": \"string\",\n              \"name\": \"field4\"\n            },\n            {\n              \"type\": \"string\",\n              \"name\": \"type\"\n            }\n          ]\n        }\n      },\n      \"time\": 0.032,\n      \"scope\": [\n        \"hash-count-error\",\n        \"byte-count-error\",\n        \"field-count-error\",\n        \"row-count-error\",\n        \"blank-header\",\n        \"extra-label\",\n        \"missing-label\",\n        \"blank-label\",\n        \"duplicate-label\",\n        \"incorrect-label\",\n        \"blank-row\",\n        \"primary-key-error\",\n        \"foreign-key-error\",\n        \"extra-cell\",\n        \"missing-cell\",\n        \"type-error\",\n        \"constraint-error\",\n        \"unique-error\"\n      ],\n      \"partial\": false,\n      \"errors\": [\n        {\n          \"label\": \"\",\n          \"fieldName\": \"field4\",\n          \"fieldNumber\": 4,\n          \"fieldPosition\": 4,\n          \"labels\": [\n            \"text\",\n            \"number\",\n            \"date\",\n            \"\",\n            \"type\"\n          ],\n          \"rowPositions\": [\n            1\n          ],\n          \"code\": \"blank-label\",\n          \"name\": \"Blank Label\",\n          \"tags\": [\n            \"#table\",\n            \"#header\",\n            \"#label\"\n          ],\n          \"note\": \"\",\n          \"message\": \"Label in the header in field at position \\\"4\\\" is blank\",\n          \"description\": \"A label in the header row is missing a value. Label should be provided and not be blank.\"\n        },\n        {\n          \"cells\": [],\n          \"rowNumber\": 2,\n          \"rowPosition\": 3,\n          \"code\": \"blank-row\",\n          \"name\": \"Blank Row\",\n          \"tags\": [\n            \"#table\",\n            \"#row\"\n          ],\n          \"note\": \"\",\n          \"message\": \"Row at position \\\"3\\\" is completely blank\",\n          \"description\": \"This row is empty. A row should contain at least one value.\"\n        },\n        {\n          \"cell\": \"\",\n          \"fieldName\": \"type\",\n          \"fieldNumber\": 5,\n          \"fieldPosition\": 5,\n          \"cells\": [\n            \"\",\n            \"Ciao\",\n            \"2023-01-13\",\n            \"C\"\n          ],\n          \"rowNumber\": 3,\n          \"rowPosition\": 4,\n          \"code\": \"missing-cell\",\n          \"name\": \"Missing Cell\",\n          \"tags\": [\n            \"#table\",\n            \"#row\",\n            \"#cell\"\n          ],\n          \"note\": \"\",\n          \"message\": \"Row at position \\\"4\\\" has a missing cell in field \\\"type\\\" at position \\\"5\\\"\",\n          \"description\": \"This row has less values compared to the header row (the first row in the data source). A key concept is that all the rows in tabular data must have the same number of columns.\"\n        }\n      ],\n      \"stats\": {\n        \"errors\": 3\n      },\n      \"valid\": false\n    }\n  ],\n  \"stats\": {\n    \"errors\": 3,\n    \"tasks\": 1\n  },\n  \"valid\": false\n}\n</code></pre>"},{"location":"frictionless/validare/#validare-un-file-tabellare-a-partire-da-uno-schema","title":"Validare un file tabellare a partire da uno schema","text":"<p>In input ad esempio questi dati</p> text number date code type Lorem 37.256 2022-01-13 012543 A Ciao 2023-01-13 78956 C <p>che in CSV sono</p> input_01.csv<pre><code>text,number,date,code,type\nLorem,37.256,2022-01-13,012543,A\n,Ciao,2023-01-13,78956,C\n</code></pre> <p>A seguire l'elenco dei campi/colonne del file, e i vari vincoli che si vogliono imporre e poter verificare, sul tipo di campo, valori consentiti, obbligatoriet\u00e0, ecc:</p> <ul> <li><code>text</code>, un campo stringa, obbligatorio</li> <li><code>number</code>, un campo numerico</li> <li><code>date</code>, un campo di tipo data, con valori che non possono essere successivi al 15 maggio 2022</li> <li><code>code</code>, un campo stringa, che deve essere composto da 6 caratteri numerici (pu\u00f2 iniziare per <code>0</code>, quindi non \u00e8 un numero)</li> <li><code>type</code>, un campo stringa, che pu\u00f2 avere come valori soltanto <code>A</code> e <code>B</code>.</li> </ul> <p>Il file di sopra - <code>input_01.csv</code> - rispetto a questo schema ha diversi problemi:</p> <ol> <li>riga <code>3</code>, colonna <code>1</code>, il campo obbligatorio <code>text</code> non \u00e8 valorizzato;</li> <li>riga <code>3</code>, colonna <code>2</code>, c'\u00e8 una stringa (<code>Ciao</code>) in un campo numerico;</li> <li>riga <code>3</code>, colonna <code>3</code>, c'\u00e8 una data che va oltre il 15 maggio 2022 (<code>2023-01-13</code>);</li> <li>riga <code>3</code>, colonna <code>4</code>, c'\u00e8 una stringa che non \u00e8 composta da 6 numeri (<code>78956</code>);</li> <li>riga <code>3</code>, colonna <code>5</code>, c'\u00e8 un valore che non \u00e8 n\u00e9 <code>A</code>, ne <code>B</code>, ma \u00e8 <code>C</code>.</li> </ol> <p>Questo schema si pu\u00f2 descrivere in formato <code>YAML</code>, secondo queste specifiche. Il file di descrizione si potr\u00e0 usare per validare il file e verificare che rispetti lo schema.</p> <p>Qui sotto il contenuto del file <code>schema_01.yml</code>, per descrivere e validare in formato frictionless lo schema del file di sopra.</p> <pre><code>path: input_01.csv\nname: input_01\nprofile: tabular-data-resource\nscheme: file\nformat: csv\nhashing: md5\nencoding: utf-8\nschema:\n  missingValues:\n    - \"NA\"\n    - \"NaN\"\n    - \"null\"\n    - \"\"\n  fields:\n  - name: text\n    type: string\n    title: \"label text\"\n    constraints:\n      required: true # (1)\n  - name: number\n    type: number\n    title: \"label number\"\n  - name: date\n    type: date\n    title: \"label date\"\n    description: \"La data di accesso dell'utente nel sistema\"\n    example: '2020-05-15'\n    constraints:\n      maximum: '2022-05-15' # (2)\n  - name: code\n    type: string\n    title: \"label code\"\n    constraints:\n      pattern: ^[0-9]{6}$ # (3)\n  - name: type\n    type: string\n    title: \"label type\"\n    constraints:\n      enum: # (4)\n        - A\n        - B\n</code></pre> <ol> <li>Il vincolo <code>required: true</code> impone che tutte le celle della colonna siano valorizzate</li> <li>Il valore massimo di data inseribile in questo campo</li> <li>L'espressione regolare che i valori delle celle di questo campo devono rispettare</li> <li>L'enumerazione, l'elenco, dei valori ammessi per le celle di questo campo</li> </ol> <p>Creare automaticamente un file <code>YAML</code></p> <p>Utilizzando il comando <code>describe</code>, puoi creare la struttura di base di un file <code>frictionless</code> che descrive una risorsa, a cui aggiungere poi \"a mano\" ulteriori propriet\u00e0.</p> <p>Per ogni campo \u00e8 definito il tipo di campo, con il parametro <code>type</code>, e sono definiti altri vincoli specifici tramite il parametro <code>constraints</code> (documentazione ufficiale). Tra questi ad esempio:</p> <ul> <li><code>required</code>, per indicare che i valori di quel campo sono obbligatori;</li> <li><code>maximum</code>, per indicare qual \u00e8 il valore massimo utilizzabile per un campo (non \u00e8 applicabile a tutti i tipi di campo)</li> <li><code>pattern</code>, per indicare, per un campo di tipo <code>string</code>, qual \u00e8 l'espressione regolare da rispettare;</li> <li><code>enum</code>, per indicare, per un campo di tipo <code>string</code>, qual \u00e8 l'elenco dei valori ammissibili.</li> </ul> <p>Utilizzando questo file <code>YAML</code>, che fa riferimento al file <code>CSV</code> di input (<code>path: input_01.csv</code>) \u00e8 possibile lanciare la validazione:</p> <pre><code>frictionless validate schema_01.yml\n</code></pre> <p>In output, estratto automaticamente, l'elenco di errori annotato poco sopra.</p> <pre><code>## Errors\n\n+-------+---------+------------+----------------------------------------------------+\n|   row |   field | code       | message                                            |\n+=======+=========+============+====================================================+\n|     3 |       1 | constraint | The cell \"\" in row at position \"3\" and field       |\n|       |         | -error     | \"text\" at position \"1\" does not conform to a       |\n|       |         |            | constraint: constraint \"required\" is \"True\"        |\n+-------+---------+------------+----------------------------------------------------+\n|     3 |       2 | type-error | Type error in the cell \"Ciao\" in row \"3\" and field |\n|       |         |            | \"number\" at position \"2\": type is \"number/default\" |\n+-------+---------+------------+----------------------------------------------------+\n|     3 |       3 | constraint | The cell \"2023-01-13\" in row at position \"3\" and   |\n|       |         | -error     | field \"date\" at position \"3\" does not conform to a |\n|       |         |            | constraint: constraint \"maximum\" is \"2022-05-15\"   |\n+-------+---------+------------+----------------------------------------------------+\n|     3 |       4 | constraint | The cell \"78956\" in row at position \"3\" and field  |\n|       |         | -error     | \"code\" at position \"4\" does not conform to a       |\n|       |         |            | constraint: constraint \"pattern\" is \"^[0-9]{6}$\"   |\n+-------+---------+------------+----------------------------------------------------+\n|     3 |       5 | constraint | The cell \"C\" in row at position \"3\" and field      |\n|       |         | -error     | \"type\" at position \"5\" does not conform to a       |\n|       |         |            | constraint: constraint \"enum\" is \"['A', 'B']\"      |\n+-------+---------+------------+----------------------------------------------------+\n</code></pre>"},{"location":"frictionless/validare/#applicare-uno-schema-definito-per-un-file-ad-altri","title":"Applicare uno schema definito per un file, ad altri","text":"<p>Per farlo basta usare l'opzione <code>--path</code> e lanciare il comando inserendo il percorso del file su cui fare la validazione.</p> <pre><code>frictionless validate schema_01.yml --path /percorso/file.csv\n</code></pre>"},{"location":"frictionless/validare/#opzioni-del-comando","title":"Opzioni del comando","text":"<pre><code>Options:\n  --type TEXT                     Specify type e.g. \"package\"\n  --path TEXT                     Specify the data path explicitly (e.g. you\n                                  need to use it if your data is JSON)\n\n  --scheme TEXT                   Specify scheme  [default: inferred]\n  --format TEXT                   Specify format  [default: inferred]\n  --hashing TEXT                  Specify hashing algorithm  [default:\n                                  inferred]\n\n  --encoding TEXT                 Specify encoding  [default: inferred]\n  --innerpath TEXT                Specify in-archive path  [default: first]\n  --compression TEXT              Specify compression  [default: inferred]\n  --control TEXT                  An inline JSON object or a path to a JSON\n                                  file that provides the control\n                                  (configuration for the data Loader)\n\n  --dialect TEXT                  An inline JSON object or a path to a JSON\n                                  file that provides the dialect\n                                  (configuration for the parser)\n\n  --sheet TEXT                    The sheet to use from the input data (only\n                                  with XLS and ODS files/plugins)\n\n  --table TEXT                    The table to use from the SQL database (SQL\n                                  plugin)\n\n  --keys TEXT                     The keys to use as column names for the\n                                  Inline or JSON data plugins\n\n  --keyed / --no-keyed            Whether the input data is keyed for the\n                                  Inline or JSON data plugins\n\n  --header-rows TEXT              Comma-separated row numbers [default:\n                                  inferred]\n\n  --header-join TEXT              Multiline header joiner [default: inferred]\n  --pick-fields TEXT              Comma-separated fields to pick e.g.\n                                  \"1,name1\"\n\n  --skip-fields TEXT              Comma-separated fields to skip e.g.\n                                  \"2,name2\"\n\n  --limit-fields INTEGER          Limit fields by this integer e.g. \"10\"\n  --offset-fields INTEGER         Offset fields by this integer e.g \"5\"\n  --pick-rows TEXT                Comma-separated rows to pick e.g.\n                                  \"1,&lt;blank&gt;\"\n\n  --skip-rows TEXT                Comma-separated rows to skip e.g. \"2,3,4,5\"\n  --limit-rows INTEGER            Limit rows by this integer e.g \"100\"\n  --offset-rows INTEGER           Offset rows by this integer e.g. \"50\"\n  --schema TEXT                   Specify a path to a schema\n  --stats-hash TEXT               Expected hash based on hashing option\n  --stats-bytes INTEGER           Expected size in bytes\n  --stats-fields INTEGER          Expected amount of fields\n  --stats-rows INTEGER            Expected amount of rows\n  --buffer-size INTEGER           Limit the amount of bytes to be extracted as\n                                  a buffer  [default: 10000]\n\n  --sample-size INTEGER           Limit the number of rows to be extracted as\n                                  a sample  [default: 100]\n\n  --field-type TEXT               Force all the fields to have this type\n  --field-names TEXT              Comma-separated list of field names\n  --field-confidence FLOAT        Infer confidence. A float from 0 to 1. If 1,\n                                  (sampled) data is guaranteed to be valid\n                                  against the inferred schema  [default: 0.9]\n\n  --field-float-numbers / --no-field-float-numbers\n                                  Make number floats instead of decimals\n                                  [default: False]\n\n  --field-missing-values TEXT     Comma-separated list of missing values\n                                  [default: \"\"]\n\n  --schema-sync / --no-schema-sync\n                                  Sync the schema based on the data's header\n                                  row\n\n  --basepath TEXT                 Basepath of the resource/package\n  --pick-errors TEXT              Comma-separated errors to pick e.g. \"type-\n                                  error\"\n\n  --skip-errors TEXT              Comma-separated errors to skip e.g. \"blank-\n                                  row\"\n\n  --limit-errors INTEGER          Limit errors by this integer\n  --limit-memory INTEGER          Limit memory by this integer in MB\n  --original / --no-original      Don't call infer on resources\n  --parallel / --no-parallel      Enable multiprocessing\n  --yaml / --no-yaml              Return in pure YAML format  [default: False]\n  --json / --no-json              Return in JSON format  [default: False]\n  --resource-name TEXT            Name of resource to validate\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"gdal-ogr/","title":"GDAL/OGR","text":"<p>Senza questa GDAL/OGR, forse non sarei mai caduto dentro il buco nero della \"riga di comando\".</p> <p>\u00c8 la pi\u00f9 straordinaria utility per legger e trasformare dati spaziali.</p> <p>\u27a1\ufe0f Installazione e/o compilazione</p>"},{"location":"gdal-ogr/installazione/","title":"Installazione e/o Compilazione di GDAL","text":""},{"location":"gdal-ogr/installazione/#compilazione","title":"Compilazione","text":""},{"location":"gdal-ogr/installazione/#ambiente-linux","title":"Ambiente LINUX","text":"<p>Si inizia da installare le dipendenze:</p> <pre><code>sudo apt-get install swig\nsudo apt-get install build-essential\nsudo apt-get install cmake\nsudo apt-get install libproj-dev proj-data proj-bin unzip -y\nsudo apt install curl &amp;&amp; sudo apt-get install libcurl4-openssl-dev\nsudo apt-get install libcrypto++-dev libcrypto++-doc libcrypto++-utils\nsudo apt install libgeos-dev\n</code></pre> <p>Poi, a partire da una cartella di lavoro, si clona il repository ufficiale (\u00e8 necessario avere <code>git</code> installato) e si compila seguendo questi passi:</p> <pre><code>git clone https://github.com/OSGeo/gdal.git\ncd gdal\nmkdir build\ncd build\n\ncmake  GDAL_USE_GEOS=ON ..\ncmake --build .\nsudo cmake --build . --target install\n</code></pre> <p>Nota bene</p> <p>Qui si d\u00e0 per scontato che nel sistema sia installato Python. Se non lo \u00e8, \u00e8 bene installare la versione stabile per il sistema in uso.</p>"},{"location":"gdal-ogr/installazione/#installazione","title":"Installazione","text":""},{"location":"gdal-ogr/installazione/#windows","title":"Windows","text":"<p>Un modo per farlo \u00e8 quello di utilizzare i pacchetti precompilati disponibili su GISInternals.</p> <p>Questi i passi:</p> <ul> <li>si sceglie la release desiderata (ad esempio la stabile);</li> <li>poi architettura e \"compilatore\" (quindi ad esempio, <code>x64</code> e <code>MSVC 2022</code>);</li> <li>si apre pagina relativa (ad esempio <code>release-1930-x64-gdal-3-6-3-mapserver-8-0-0</code>) e si sceglie la voce che in descrizione riporta \"Compiled binaries in a single .zip package\" e si fa il relativo download;</li> <li>si estrae il file zip scaricato in una cartella del PC (ad esempio <code>C:\\gdal</code>).</li> </ul> <p></p> <p>L'installazione \u00e8 terminata. Per usare GDAL, baster\u00e0 lanciare la shell relativa, facendo click sul file <code>SDKShell.bat</code> (presente nella radice della cartella dei file estratti).</p>"},{"location":"gdal-ogr/ricette/","title":"Ricette GDAL/OGR","text":""},{"location":"gdal-ogr/ricette/#scaricare-dati-da-un-webservice-arcgis","title":"Scaricare dati da un webservice ArcGIS","text":"<p>Prima lo interrogo, per avere restituito il nome del layer e altri metadati:</p> <pre><code>ogrinfo -so -ro -al \"https://map.sitr.regione.sicilia.it/gis/rest/services/catasto/cartografia_catastale/MapServer/6/query?where=objectid+%3D+objectid&amp;outfields=*&amp;f=json\"\n</code></pre> <p>L'output sar\u00e0 qualcosa come:</p> <pre><code>Layer name: ESRIJSON\nGeometry: Polygon\nFeature Count: 1000\nExtent: (13.097299, 37.166626) - (13.976281, 37.694915)\nLayer SRS WKT:\nPROJCS[\"ETRS89 / UTM zone 33N\",\n    GEOGCS[\"ETRS89\",\n        DATUM[\"European_Terrestrial_Reference_System_1989\",\n            SPHEROID[\"GRS 1980\",6378137,298.257222101,\n                AUTHORITY[\"EPSG\",\"7019\"]],\n            TOWGS84[0,0,0,0,0,0,0],\n            AUTHORITY[\"EPSG\",\"6258\"]],\n        PRIMEM[\"Greenwich\",0,\n            AUTHORITY[\"EPSG\",\"8901\"]],\n        UNIT[\"degree\",0.0174532925199433,\n            AUTHORITY[\"EPSG\",\"9122\"]],\n        AUTHORITY[\"EPSG\",\"4258\"]],\n    PROJECTION[\"Transverse_Mercator\"],\n    PARAMETER[\"latitude_of_origin\",0],\n    PARAMETER[\"central_meridian\",15],\n    PARAMETER[\"scale_factor\",0.9996],\n    PARAMETER[\"false_easting\",500000],\n    PARAMETER[\"false_northing\",0],\n    UNIT[\"metre\",1,\n        AUTHORITY[\"EPSG\",\"9001\"]],\n    AXIS[\"Easting\",EAST],\n    AXIS[\"Northing\",NORTH],\n    AUTHORITY[\"EPSG\",\"25833\"]]\nOBJECTID: Integer (0.0)\nCOMUNE: String (4.0)\nSEZIONE: String (1.0)\nFOGLIO: String (4.0)\nALLEGATO: String (1.0)\nSVILUPPO: String (1.0)\nORIGINE: String (20.0)\nShape_Length: Real (0.0)\nShape_Area: Real (0.0)\n</code></pre> <p>E poi, lo scarico paginando:</p> <pre><code>ogr2ogr -overwrite -f SQLite -dsco SPATIALITE=YES -nlt MULTIPOLYGON -nln particelle particelle.sqlite  \"https://map.sitr.regione.sicilia.it/gis/rest/services/catasto/cartografia_catastale/MapServer/6/query?where=objectid+%3D+objectid&amp;outfields=*&amp;f=json\" ESRIJSON -oo FEATURE_SERVER_PAGING=\"YES\"\n</code></pre> <p>Nota</p> <ul> <li><code>-nlt MULTIPOLYGON</code> perch\u00e9 l'oggetto di input qui \u00e8 un <code>MULTIPOLYGON</code>;</li> <li>la paginazione della query \u00e8 possibile quando <code>ArcGIS server &gt;= 10.3</code> e i layer da interrogare hanno l'impostazione <code>supportsPagination=true</code>.</li> </ul>"},{"location":"gdal-ogr/ricette/#usare-gdal-via-docker","title":"Usare gdal via docker","text":"<p>Si inizia con il prelevare l'immagine da un repository. Ad esempio questa di osgeo:</p> <pre><code>docker pull osgeo/gdal:alpine-normal-latest\n</code></pre> <p>E poi si pu\u00f2 lanciare in modo non interattivo dalla propria shell:</p> <pre><code>docker run --rm -v \"$(pwd)\":/data osgeo/gdal:alpine-normal-latest gdalinfo data/input.tif\n</code></pre> <ul> <li><code>--rm</code> per rimuovere il container non appena si esce da esso;</li> <li><code>-v</code> per specificare i volumi da montare nell'host e nel container <code>/from/host/:/on/container</code></li> </ul> <p>Nell'esempio di sopra sto montando la cartella corrente dell'host e la cartella <code>data</code> del container.</p> <p>In modo interattivo, invece il comando diventa:</p> <pre><code>docker run -ti --rm -v \"$(pwd)\":/data osgeo/gdal:alpine-normal-latest /bin/sh\n</code></pre> <p>Si usa in questo esempio <code>/bin/sh</code>, perch\u00e9 lanciando l'<code>inspect</code> dell'immagine - <code>docker inspect osgeo/gdal:alpine-normal-latest</code> - si ha (vedi qui):</p> <pre><code>\"Cmd\": [\n    \"/bin/sh\",\n    \"-c\",\n    \"#(nop) COPY dir:bfa76ede215e381fc0e06a919358cf3fe603fbc832802559c2e82eeec03e484d in /usr/ \"\n]\n</code></pre>"},{"location":"gdal-ogr/ricette/#interrogare-un-server-csw","title":"Interrogare un server CSW","text":"<p>Se si vuole interrogare ad esempio il server CSW del Repertorio Nazionale dei Dati Territoriali, l'URL di riferimento \u00e8 http://geodati.gov.it/RNDT/csw</p> <p>Se si vuole l'elenco di tutti gli item, che contengono nel <code>subject</code> il valore <code>salute</code>, si pu\u00f2 lanciare questo comando:</p> <pre><code>ogrinfo -ro -al \"CSW:http://geodati.gov.it/RNDT/csw\" -where \"subject LIKE 'salute'\"\n</code></pre> <p>Si avr\u00e0 a schermo l'elenco degli item.</p> <p>Nota</p> <p>La ricerca viene eseguita non per stringa esatta, ma per tutto ci\u00f2 che contiene nel <code>subject</code> la stringa <code>salute</code>. Per ricerche di stringhe esatte, leggere il paragrafo di sotto.</p> <p>Se si vuole l'output di questa query in CSV, si pu\u00f2 usare <code>ogr2ogr</code>:</p> <pre><code>ogr2ogr -F csv tmp.csv \"CSW:http://geodati.gov.it/RNDT/csw\" -where \"subject LIKE 'salute'\" -oo ELEMENTSETNAME=full -oo FULL_EXTENT_RECORDS_AS_NON_SPATIAL=YES -oo MAX_RECORDS=500 --config GML_SKIP_CORRUPTED_FEATURES YES\n</code></pre> <p>Se si vuole leggere il comando che ogr invia al server, bisogna attivare il debug, con <code>--debug on</code>:</p> <pre><code>ogrinfo --debug on -ro -al \"CSW:http://geodati.gov.it/RNDT/csw\" -where \"subject LIKE 'salute'\"\n</code></pre> <p>In output si leggeranno i dettagli delle chiamate HTTP:</p> <pre><code>HTTP: Fetch(http://geodati.gov.it/RNDT/csw)\nHTTP: These POSTFIELDS were sent:&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;csw:GetRecords resultType=\"results\" service=\"CSW\" version=\"2.0.2\" startPosition=\"73\" maxRecords=\"500\" xmlns:csw=\"http://www.opengis.net/cat/csw/2.0.2\" xmlns:gml=\"http://www.opengis.net/gml\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:dct=\"http://purl.org/dc/terms/\" xmlns:ogc=\"http://www.opengis.net/ogc\" xmlns:ows=\"http://www.opengis.net/ows\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.opengis.net/cat/csw/2.0.2 http://schemas.opengis.net/csw/2.0.2/CSW-discovery.xsd\"&gt;&lt;csw:Query typeNames=\"csw:Record\"&gt;&lt;csw:ElementSetName&gt;full&lt;/csw:ElementSetName&gt;&lt;csw:Constraint version=\"1.1.0\"&gt;&lt;ogc:Filter&gt;&lt;ogc:PropertyIsLike wildCard='*' singleChar='_' escapeChar='!'&gt;&lt;ogc:PropertyName&gt;dc:subject&lt;/ogc:PropertyName&gt;&lt;ogc:Literal&gt;salute&lt;/ogc:Literal&gt;&lt;/ogc:PropertyIsLike&gt;&lt;/ogc:Filter&gt;&lt;/csw:Constraint&gt;&lt;/csw:Query&gt;&lt;/csw:GetRecords&gt;\nHTTP: These HTTP headers were set: Content-Type: application/xml; charset=UTF-8\n</code></pre> <p>La query \u00e8 descritta in XML:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;csw:GetRecords resultType=\"results\" service=\"CSW\" version=\"2.0.2\" startPosition=\"1\" maxRecords=\"500\" xmlns:csw=\"http://www.opengis.net/cat/csw/2.0.2\" xmlns:gml=\"http://www.opengis.net/gml\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:dct=\"http://purl.org/dc/terms/\" xmlns:ogc=\"http://www.opengis.net/ogc\" xmlns:ows=\"http://www.opengis.net/ows\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.opengis.net/cat/csw/2.0.2 http://schemas.opengis.net/csw/2.0.2/CSW-discovery.xsd\"&gt;\n  &lt;csw:Query typeNames=\"csw:Record\"&gt;\n    &lt;csw:ElementSetName&gt;full&lt;/csw:ElementSetName&gt;\n    &lt;csw:Constraint version=\"1.1.0\"&gt;\n      &lt;ogc:Filter&gt;\n        &lt;ogc:PropertyIsLike wildCard='*' singleChar='_' escapeChar='!'&gt;\n          &lt;ogc:PropertyName&gt;dc:subject&lt;/ogc:PropertyName&gt;\n          &lt;ogc:Literal&gt;salute&lt;/ogc:Literal&gt;\n        &lt;/ogc:PropertyIsLike&gt;\n      &lt;/ogc:Filter&gt;\n    &lt;/csw:Constraint&gt;\n  &lt;/csw:Query&gt;\n&lt;/csw:GetRecords&gt;\n</code></pre> <p>Questa consente di costruire una semplice chiamata <code>HTTP</code> in <code>curl</code>:</p> <pre><code>curl -s -k -X POST -H \"Content-Type: application/xml; charset=UTF-8\" -d '&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;csw:GetRecords resultType=\"results\" service=\"CSW\" version=\"2.0.2\" startPosition=\"1\" maxRecords=\"500\" xmlns:csw=\"http://www.opengis.net/cat/csw/2.0.2\" xmlns:gml=\"http://www.opengis.net/gml\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:dct=\"http://purl.org/dc/terms/\" xmlns:ogc=\"http://www.opengis.net/ogc\" xmlns:ows=\"http://www.opengis.net/ows\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.opengis.net/cat/csw/2.0.2 http://schemas.opengis.net/csw/2.0.2/CSW-discovery.xsd\"&gt;&lt;csw:Query typeNames=\"csw:Record\"&gt;&lt;csw:ElementSetName&gt;full&lt;/csw:ElementSetName&gt;&lt;csw:Constraint version=\"1.1.0\"&gt;&lt;ogc:Filter&gt;&lt;ogc:PropertyIsLike wildCard=\"*\" singleChar=\"_\" escapeChar=\"!\"&gt;&lt;ogc:PropertyName&gt;dc:subject&lt;/ogc:PropertyName&gt;&lt;ogc:Literal&gt;salute&lt;/ogc:Literal&gt;&lt;/ogc:PropertyIsLike&gt;&lt;/ogc:Filter&gt;&lt;/csw:Constraint&gt;&lt;/csw:Query&gt;&lt;/csw:GetRecords&gt;' https://geodati.gov.it/RNDT/csw\n</code></pre> <p>Alcune note su questo comando:</p> <ul> <li>nel debug di ogr si legge <code>&lt;ogc:PropertyIsLike wildCard='*' singleChar='_' escapeChar='!'&gt;</code>. Nel comando curl di sopra, bisogna modificare gli <code>'</code> in <code>\"</code>;</li> <li><code>startPosition</code> \u00e8 impostato a <code>\"1\"</code> per avere tutti i risultati a partire dal primo;</li> <li><code>resultType</code> impostato a <code>\"results\"</code> d\u00e0 in output le informazioni su tutti gli item di output.</li> </ul> <p>Se si vuole avere in output soltanto un riepilogo dei risultati, bisogner\u00e0 cambiare <code>resultType</code> in <code>\"hits\"</code>:</p> <pre><code>curl -s -k -X POST -H \"Content-Type: application/xml; charset=UTF-8\" -d '&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;csw:GetRecords resultType=\"hits\" service=\"CSW\" version=\"2.0.2\" startPosition=\"1\" maxRecords=\"500\" xmlns:csw=\"http://www.opengis.net/cat/csw/2.0.2\" xmlns:gml=\"http://www.opengis.net/gml\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:dct=\"http://purl.org/dc/terms/\" xmlns:ogc=\"http://www.opengis.net/ogc\" xmlns:ows=\"http://www.opengis.net/ows\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.opengis.net/cat/csw/2.0.2 http://schemas.opengis.net/csw/2.0.2/CSW-discovery.xsd\"&gt;&lt;csw:Query typeNames=\"csw:Record\"&gt;&lt;csw:ElementSetName&gt;full&lt;/csw:ElementSetName&gt;&lt;csw:Constraint version=\"1.1.0\"&gt;&lt;ogc:Filter&gt;&lt;ogc:PropertyIsLike wildCard=\"*\" singleChar=\"_\" escapeChar=\"!\"&gt;&lt;ogc:PropertyName&gt;dc:subject&lt;/ogc:PropertyName&gt;&lt;ogc:Literal&gt;salute&lt;/ogc:Literal&gt;&lt;/ogc:PropertyIsLike&gt;&lt;/ogc:Filter&gt;&lt;/csw:Constraint&gt;&lt;/csw:Query&gt;&lt;/csw:GetRecords&gt;' https://geodati.gov.it/RNDT/csw\n</code></pre> <p>In output</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;csw:GetRecordsResponse xmlns:csw=\"http://www.opengis.net/cat/csw/2.0.2\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:dcmiBox=\"http://dublincore.org/documents/2000/07/11/dcmi-box/\" xmlns:dct=\"http://purl.org/dc/terms/\" xmlns:gml=\"http://www.opengis.net/gml\" xmlns:ows=\"http://www.opengis.net/ows\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\"&gt;\n  &lt;csw:SearchStatus timestamp=\"2022-11-17T11:34:46+01:00\" /&gt;\n  &lt;csw:SearchResults elementSet=\"full\" nextRecord=\"1\" numberOfRecordsMatched=\"72\" numberOfRecordsReturned=\"0\" recordSchema=\"http://www.opengis.net/cat/csw/2.0.2\" /&gt;\n&lt;/csw:GetRecordsResponse&gt;\n</code></pre> <p>\u00c8 possibile anche usare un carattere jolly e fare ricerche per tutti gli item, che contengono la stringa <code>salut</code>. Il caratter <code>jolly</code> \u00e8 <code>*</code>.</p> <p>Il comando</p> <pre><code>ogrinfo -ro -al \"CSW:http://geodati.gov.it/RNDT/csw\" -where \"subject LIKE '*salut*'\"\n</code></pre> <p>restituir\u00e0 quindi anche gli item con subject <code>rischio per la salute</code>, <code>Centri salute mentale</code>, <code>ambiente e salute</code>, ecc..</p>"},{"location":"gdal-ogr/ricette/#elenco-dei-campi-interrogabili","title":"Elenco dei campi interrogabili","text":"<p>Si ottiene lanciando <code>ogrinfo CSW:http://www.pcn.minambiente.it/geoportal/csw -al -so</code>. In output, in basso, l'elenco dei campi interrogabili e il loro tipo:</p> <pre><code>INFO: Open of `CSW:http://www.pcn.minambiente.it/geoportal/csw'\n      using driver `CSW' successful.\n\nLayer name: records\nGeometry: Polygon\nFeature Count: 329\nExtent: (-180.000000, -90.000000) - (180.000000, 90.000000)\nLayer SRS WKT:\nGEOGCS[\"WGS 84\",\n    DATUM[\"WGS_1984\",\n        SPHEROID[\"WGS 84\",6378137,298.257223563,\n            AUTHORITY[\"EPSG\",\"7030\"]],\n        AUTHORITY[\"EPSG\",\"6326\"]],\n    PRIMEM[\"Greenwich\",0,\n        AUTHORITY[\"EPSG\",\"8901\"]],\n    UNIT[\"degree\",0.0174532925199433,\n        AUTHORITY[\"EPSG\",\"9122\"]],\n    AUTHORITY[\"EPSG\",\"4326\"]]\nGeometry Column = boundingbox\nidentifier: String (0.0)\nother_identifiers: StringList (0.0)\ntype: String (0.0)\nsubject: String (0.0)\nother_subjects: StringList (0.0)\nreferences: String (0.0)\nother_references: StringList (0.0)\nmodified: String (0.0)\nabstract: String (0.0)\ndate: String (0.0)\nlanguage: String (0.0)\nrights: String (0.0)\nformat: String (0.0)\nother_formats: StringList (0.0)\ncreator: String (0.0)\nsource: String (0.0)\nanytext: String (0.0)\n</code></pre>"},{"location":"gdal-ogr/ricette/#query-in-and-e-or","title":"Query in AND e OR","text":"<p>Ad esempio tutti gli item che hanno come subject <code>salute</code> e <code>strutture</code>:</p> <pre><code>ogrinfo -ro -al \"CSW:http://geodati.gov.it/RNDT/csw\" -where \"subject LIKE 'salute' AND subject LIKE 'strutture'\"\n</code></pre> <p>Si pu\u00f2 applicare anche la condizione <code>OR</code>.</p>"},{"location":"gdal-ogr/ricette/#query-basate-su-elenco-di-parole-mappate-come-stringa-esatta","title":"Query basate su elenco di parole, mappate come stringa esatta","text":"<p>Se il subject che si vuole filtrare \u00e8 <code>strutture sociali</code>, bisogna inserire questa stringa tra <code>\"</code>.</p> <p>Se non lo si fa, la ricerca verr\u00e0 fatta in <code>OR</code>, ovvero tutti gli item con <code>subject=\"strutture\"</code> o <code>subject=\"sociali\"</code>.</p> <p>Al momento (17 novembre 2022) c'\u00e8 un bug, per cui non si pu\u00f2 usare direttamente ogr, e quindi bisogna fare la cosa tramite chiamata HTTP diretta:</p> <pre><code>curl -s -k -X POST -H \"Content-Type: application/xml; charset=UTF-8\" -d '&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;csw:GetRecords resultType=\"hits\" service=\"CSW\" version=\"2.0.2\" startPosition=\"1\" maxRecords=\"500\" xmlns:csw=\"http://www.opengis.net/cat/csw/2.0.2\" xmlns:gml=\"http://www.opengis.net/gml\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:dct=\"http://purl.org/dc/terms/\" xmlns:ogc=\"http://www.opengis.net/ogc\" xmlns:ows=\"http://www.opengis.net/ows\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.opengis.net/cat/csw/2.0.2 http://schemas.opengis.net/csw/2.0.2/CSW-discovery.xsd\"&gt;&lt;csw:Query typeNames=\"csw:Record\"&gt;&lt;csw:ElementSetName&gt;full&lt;/csw:ElementSetName&gt;&lt;csw:Constraint version=\"1.1.0\"&gt;&lt;ogc:Filter&gt;&lt;ogc:PropertyIsLike wildCard=\"*\" singleChar=\"_\" escapeChar=\"!\"&gt;&lt;ogc:PropertyName&gt;dc:subject&lt;/ogc:PropertyName&gt;&lt;ogc:Literal&gt;\"strutture sociali\"&lt;/ogc:Literal&gt;&lt;/ogc:PropertyIsLike&gt;&lt;/ogc:Filter&gt;&lt;/csw:Constraint&gt;&lt;/csw:Query&gt;&lt;/csw:GetRecords&gt;' https://geodati.gov.it/RNDT/csw\n</code></pre> <p>Notare che qui sopr\u00e0 \u00e8 stato inserito <code>&lt;ogc:Literal&gt;\"strutture sociali\"&lt;/ogc:Literal&gt;</code>.</p>"},{"location":"gdal-ogr/ricette/#cercare-per-titolo-title","title":"Cercare per titolo (title)","text":"<p>Nota bene</p> <p>Quanto scritto sotto, da dicembre 2022, non \u00e8 pi\u00f9 valido: a seguito di questa segnalazione e di questa modifica al codice in GDAL/OGR \u00e8 possibile fare ricerche CSW per titolo. Si lascia in ogni caso questa sezione, perch\u00e9 \u00e8 un buon esempio di query HTTP.</p> <p>Come si vede sopra il campo <code>title</code> non \u00e8 di quelli ricercabili tramite OGR (vedi issue 6718).</p> <p>Quindi la ricerca per titolo \u00e8 fattibile soltanto tramite una query HTTP sul server CSW, utilizzando il nome del campo disponibile. Nella guida al servizio CSW usato per questi esempi \u00e8 presenta la lista dei campi, e si pu\u00f2 fare riferimento al campo <code>title</code> usando <code>apiso:title</code> o <code>dc:title</code>.</p> <p></p> <p>La query di sotto cerca la stringa <code>salute</code> nel <code>title</code> (che in questo server \u00e8 mappato come <code>dc:title</code>):</p> <pre><code>curl -s -k -X POST -H \"Content-Type: application/xml; charset=UTF-8\" -d '&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;csw:GetRecords resultType=\"results\" service=\"CSW\" version=\"2.0.2\" startPosition=\"1\" maxRecords=\"500\" xmlns:csw=\"http://www.opengis.net/cat/csw/2.0.2\" xmlns:gml=\"http://www.opengis.net/gml\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:dct=\"http://purl.org/dc/terms/\" xmlns:ogc=\"http://www.opengis.net/ogc\" xmlns:ows=\"http://www.opengis.net/ows\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.opengis.net/cat/csw/2.0.2 http://schemas.opengis.net/csw/2.0.2/CSW-discovery.xsd\"&gt;&lt;csw:Query typeNames=\"csw:Record\"&gt;&lt;csw:ElementSetName&gt;full&lt;/csw:ElementSetName&gt;&lt;csw:Constraint version=\"1.1.0\"&gt;&lt;ogc:Filter&gt;&lt;ogc:PropertyIsLike wildCard=\"*\" singleChar=\"_\" escapeChar=\"!\"&gt;&lt;ogc:PropertyName&gt;dc:title&lt;/ogc:PropertyName&gt;&lt;ogc:Literal&gt;salute&lt;/ogc:Literal&gt;&lt;/ogc:PropertyIsLike&gt;&lt;/ogc:Filter&gt;&lt;/csw:Constraint&gt;&lt;/csw:Query&gt;&lt;/csw:GetRecords&gt;' https://geodati.gov.it/RNDT/csw\n</code></pre> <p>Nota</p> <p>La ricerca senza virgolette d\u00e0 in output l'elenco di tutti gli item che contengono la parola <code>salute</code>, quindi restituisce anche <code>Centri salute mentale</code>. Se si vuole fare una ricerca esatta, bisogna mettere la stringa tra virgolette. Ad esempio <code>&lt;ogc:Literal&gt;\"salute\"&lt;/ogc:Literal&gt;</code></p>"},{"location":"gdal-ogr/ricette/#comprimere-una-base-raster-con-perdita-ma-senza-che-sia-visibile","title":"Comprimere una base raster, con perdita, ma senza che sia \"visibile\"","text":"<p>\u00c8 una compressione lossy, di cui la \"perdita\" \u00e8 pressoch\u00e9 impercettibile ad occhio. Da applicare soltanto se la raster \u00e8 da usare come sfondo, non da sottoporre ad analisi, in cui una varizione dei valori dei pixel non \u00e8 un problema.</p> <pre><code>gdal_translate \\\n-b 1 -b 2 -b 3 \\\n-co COMPRESS=JPEG \\\n-co JPEG_QUALITY=75 \\\n-co PHOTOMETRIC=YCBCR \\\n-co TILED=YES \\\ninput.tif output.tif\n</code></pre> <p>E per rendere la visualizzazione molto pi\u00f9 rapida \u00e8 essenziale aggiungere i tasselli compressi, a varie scale di zoom, con <code>gdaladdo</code>:</p> <pre><code>gdaladdo \\\n--config COMPRESS_OVERVIEW JPEG \\\n--config JPEG_QUALITY_OVERVIEW 50 \\\n--config PHOTOMETRIC_OVERVIEW YCBCR \\\n--config INTERLEAVE_OVERVIEW PIXEL \\\n-r average \\\noutput.tif\n</code></pre> <p>Fonte: http://blog.cleverelephant.ca/2015/02/geotiff-compression-for-dummies.html</p> <p>Se sai che la TIFF di output sar\u00e0 molto grande</p> <p>Il limite dimensionale di un file TIFF \u00e8 4 GB. Per fare in modo che GDAL produca output pi\u00f9 grandi, bisogna aggiungere l'opzione <code>-co BIGTIFF=YES</code> in <code>gdal_translate</code>; se si vuole applicare anche nella piramidazione invece \u00e8 <code>--config BIGTIFF_OVERVIEW YES</code> in <code>gdaladdo</code>.</p>"},{"location":"gdal-ogr/ricette/#usare-tutti-i-processori-disponibili","title":"Usare tutti i processori disponibili","text":"<p>Basta aggiungere il parametro <code>-co NUM_THREADS=ALL_CPUS</code>.</p>"},{"location":"gdal-ogr/ricette/#fare-una-piramidazione-su-file-esterno","title":"Fare una piramidazione su file esterno","text":"<p>Di default la piramidazione crea le piramidi all'interno del file a cui \u00e8 applicata.</p> <p>Se si vogliono inserire in un file esterno bisogno aggiungere l'opzione <code>-ro</code>, che imposta il read only sul file di input (se il file non \u00e8 \"scrivibile\" da <code>gdaladdo</code>, <code>-ro</code> si pu\u00f2 omettere, e la piramidiazione avr\u00e0 luogo esternamente).</p> <pre><code>gdaladdo -ro --config COMPRESS_OVERVIEW DEFLATE input.ecw\n</code></pre>"},{"location":"gdal-ogr/ricette/#attivare-il-debug","title":"Attivare il debug","text":"<p>\u00c8 molto comodo attivare il debug, sopratutto per leggere i comandi che GDAL esegue in background. Per attivarlo basta aggiungere ai comandi <code>--config CPL_DEBUG ON</code>.</p> <p>Ad esempio questo comando <code>gdalinfo</code> sul secondo sub dataset del WMS del catasto dell'Agenzia delle entrate:</p> <pre><code>gdalinfo --config CPL_DEBUG ON \\\nwms:\"https://wms.cartografia.agenziaentrate.gov.it/inspire/wms/ows01.php?VERSION=1.3.0\" \\\n-sd 2\n</code></pre>"},{"location":"gdal-ogr/ricette/#creare-file-xml-che-descrive-una-sorgente-wms","title":"Creare file XML che descrive una sorgente WMS","text":"<p>Per farlo torna utile la modalit\u00e0 di <code>debug</code>:</p> <ul> <li>si interroga con <code>gdalinfo</code> il sub dataset di interesse con il debug attivo;</li> <li>si legge la chiamata WMS che fa GDAL per raccogliere le info;</li> <li>si usa la chiamata GDAL come layer di input in <code>gdal_translate</code>, per produrre il file <code>XML</code>.</li> </ul> <p>Ad esempio qualcosa come</p> <pre><code>gdal_translate -of WMS \\\nWMS:\"https://wms.cartografia.agenziaentrate.gov.it/inspire/wms/ows01.php?language=ita&amp;SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=province&amp;CRS=EPSG:6706&amp;BBOX=33,2,48,19\" \\\nprovince.xml\n</code></pre> <p>In output</p> <pre><code>&lt;GDAL_WMS&gt;\n  &lt;Service name=\"WMS\"&gt;\n    &lt;Version&gt;1.3.0&lt;/Version&gt;\n    &lt;ServerUrl&gt;https://wms.cartografia.agenziaentrate.gov.it/inspire/wms/ows01.php?language=ita&amp;amp;SERVICE=WMS&lt;/ServerUrl&gt;\n    &lt;Layers&gt;province&lt;/Layers&gt;\n    &lt;CRS&gt;EPSG:6706&lt;/CRS&gt;\n    &lt;ImageFormat&gt;image/jpeg&lt;/ImageFormat&gt;\n    &lt;Transparent&gt;FALSE&lt;/Transparent&gt;\n    &lt;BBoxOrder&gt;yxYX&lt;/BBoxOrder&gt;\n  &lt;/Service&gt;\n  &lt;DataWindow&gt;\n    &lt;UpperLeftX&gt;2&lt;/UpperLeftX&gt;\n    &lt;UpperLeftY&gt;48&lt;/UpperLeftY&gt;\n    &lt;LowerRightX&gt;19&lt;/LowerRightX&gt;\n    &lt;LowerRightY&gt;33&lt;/LowerRightY&gt;\n    &lt;SizeX&gt;1073741824&lt;/SizeX&gt;\n    &lt;SizeY&gt;947419256&lt;/SizeY&gt;\n  &lt;/DataWindow&gt;\n  &lt;BandsCount&gt;3&lt;/BandsCount&gt;\n  &lt;BlockSizeX&gt;1024&lt;/BlockSizeX&gt;\n  &lt;BlockSizeY&gt;1024&lt;/BlockSizeY&gt;\n  &lt;OverviewCount&gt;20&lt;/OverviewCount&gt;\n&lt;/GDAL_WMS&gt;\n</code></pre>"},{"location":"maipiusenza/","title":"ciao mondo","text":""},{"location":"miller/","title":"Miller","text":"<p>Il suo eccezionale autore -  John Kerl - definisce Miller come <code>awk</code>, <code>sed</code>, <code>cut</code>, <code>join</code> e <code>sort</code> per file di testo strutturati come <code>CSV</code>, <code>TSV</code> e <code>JSON</code>.</p> <p>Ha una ottima documentazione ufficiale in inglese, consultabile qui.</p>"},{"location":"miller/#installazione","title":"Installazione","text":"<p>Miller 6 si pu\u00f2 installare in diversi modi:</p> <ul> <li>utilizzando i file binari precompilati, oresenti nella pagina delle release;</li> <li>compilandolo a partire dal codice sorgente;</li> <li>utilizzando un gestore di pacchetti:<ul> <li>Linux, <code>yum install miller</code> o <code>apt-get install miller</code> in dipenendenza della versione di Linux, o Homebrew</li> <li>MacOS, <code>brew update</code> e <code>brew install miller</code>, o <code>sudo port selfupdate</code> e <code>sudo port install miller</code>, in dipendenza delle tue preferenze di Homebrew o MacPorts.</li> <li>Windows, <code>choco install miller</code> utilizzando Chocolatey.</li> </ul> </li> </ul>"},{"location":"miller/#primi-passi","title":"Primi passi","text":"<p>Miller fa spesso riferimento nei suoi sub comandi al toolkit di <code>UNIX</code> e ai suoi esegubili come <code>cat</code>, <code>tail</code>, <code>cut</code>, <code>sort</code>, etc..  Un esempio per iniziare: stampare a schermo il contenuto di un file:</p> comandooutput <pre><code>mlr --csv cat base.csv\n</code></pre> <pre><code>nome,dataNascita,altezza,peso\nandy,1973-05-08,176,86.5\nchiara,1993-12-13,162,58.3\nguido,2001-01-22,196,90.4\n</code></pre> <p>Nel comando di sopra <code>cat</code> \u00e8 uno dei verbi di Miller.</p> <p>Esistono altri tipi di sub comandi, che invece replicano alcune delle caratteristiche di <code>awk</code>, come <code>filter</code> e <code>put</code>.</p> <p>I sub comandi di Miller si chiamano verbi</p> <ul> <li>formati;</li> <li>verbi;</li> <li>script (<code>DSL</code>)</li> </ul>"},{"location":"miller/contributi/","title":"Contributi","text":"<p>Da quando abbiamo iniziato a usare Miller alcuni anni fa, abbiamo aperto diverse issue nel repository ufficiale, con idee, domande, proposte e segnalazioni di bug. Alcune sono diventate nuove caratteristiche di Miller e ci fa un particolare piacere tenerne nota.</p> <p>Tutto questo \u00e8 possibile grazie al suo straordinario autore, John Kerl, che cura in modo esemplare il rapporto con la comunit\u00e0 che utilizza questa applicazione.</p>"},{"location":"miller/contributi/#2022-02-08-aggiunto-il-natural-sorting","title":"2022-02-08 | Aggiunto il natural sorting","text":"<p>\u00c8 possibile usare il natural sorting<sup>1</sup>, sia per il verbo <code>sort</code>, che per la funzione DSL <code>sort</code>. Grazie a Salvatore Fiandaca per l'ispirazione.</p> <p>Vedi <code>#874</code> e <code>#872</code>.</p>"},{"location":"miller/contributi/#2022-02-07-espansa-sintassi-di-strptime","title":"2022-02-07 | Espansa sintassi di strptime","text":"<p>Prima non era possibile utilizzare <code>%j</code>.</p> <p>Vedi <code>#914</code>.</p>"},{"location":"miller/contributi/#2022-02-06-supporto-piu-rigoroso-al-formato-tsv","title":"2022-02-06 | Supporto pi\u00f9 rigoroso al formato TSV","text":"<p>Prima era gestito come uno speciale <code>CSV</code>, in realt\u00e0 sono formati differenti.</p> <p>Vedi <code>#922</code> e <code>#923</code>.</p>"},{"location":"miller/contributi/#2022-01-09-supporto-al-formato-json-lines","title":"2022-01-09 | Supporto al formato JSON Lines","text":"<p>Prima non era supportato. Vedi <code>#755</code> e <code>#844</code>.</p>"},{"location":"miller/contributi/#2022-01-09-json-di-output-corretto","title":"2022-01-09 | JSON di output corretto","text":"<p>Prima non era supportato di default. Vedi <code>#755</code> e <code>#844</code>.</p>"},{"location":"miller/contributi/#2021-10-10-gestione-corretta-del-carattere-pipe-nella-creazione-di-markdown","title":"2021-10-10 | Gestione corretta del carattere pipe nella creazione di Markdown","text":"<p>Prima un carettere <code>|</code> in una cella di input, faceva produrre Markdown di output scorretti.</p> <p>Vedi <code>#610</code>.</p> <ol> <li> <p>https://blog.codinghorror.com/sorting-for-humans-natural-sort-order/ \u21a9</p> </li> </ol>"},{"location":"miller/eterogeneita_record/","title":"Eterogeneit\u00e0 dei record","text":"<p>Per Miller il modo nativo di \"pensare\" ai dati \u00e8 per record eterogenei: ogni record non deve necessariamente avere lo stesso numero di campi degli altri.</p> <p>I record non hanno lo stesso numero di campi</p> <pre><code>nome=andy,dataNascita=1973-05-08,altezza=176,peso=86.5,comuneNascita=Roma\nnome=chiara,dataNascita=1993-12-13,altezza=162,peso=58.3,comuneNascita=Milano\nnome=guido,altezza=196,peso=90.4,comuneNascita=Roma\nnome=sara,dataNascita=2000-02-22,altezza=166,peso=70.4,comuneNascita=Roma\nnome=giulia,dataNascita=1997-08-13,altezza=169,peso=68.3\n</code></pre>"},{"location":"miller/eterogeneita_record/#terminologia","title":"Terminologia","text":"<p>Esistono tre tipi di eterogeneit\u00e0: <code>ragged</code>, <code>irregular</code> e <code>sparse</code>.</p>"},{"location":"miller/eterogeneita_record/#dati-rettangolari","title":"Dati rettangolari","text":"<p>Qui un esempio di dati \"rettangolari\", in formato <code>CSV</code>: 3 i campi previsti, e ogni record ha valorizzato 3 campi.</p> eterogenita.csv<pre><code>a,b,c\n1,2,3\n4,5,6\n7,8,9\n</code></pre>"},{"location":"miller/eterogeneita_record/#dati-rettangolari-ma-con-celle-vuote","title":"Dati rettangolari, ma con celle vuote","text":"<p>Questo \u00e8 un file che non ha record eterogenei, ma ha alcune celle vuote (per il campo <code>b</code> a riga 2 e per il campo <code>a</code> a riga 3).</p> eterogeneita_vuoti.csv<pre><code>a,b,c\n1,2,3\n4,,6\n,8,9\n</code></pre> <p>In Miller \u00e8 possibile usare il verbo <code>fill-empty</code>, per attribuirgli un valore (ad esempio <code>NA</code>). La gestione dei valori sconosciuti o nulli \u00e8 tra la  buone pratiche della pubblicazione di dati in formato CSV.</p> <p>mlr --icsv --opprint fill-empty -v NA ./eterogeneita_vuoti.csv</p> <pre><code>a  b  c\n1  2  3\n4  NA 6\nNA 8  9\n</code></pre> <p>Il verbo <code>fill-empty</code> richiede Miller &gt;= 6.0</p>"},{"location":"miller/eterogeneita_record/#ragged","title":"Ragged","text":"<p>Questo \u00e8 il caso di eterogeneit\u00e0, dovuta a errori di struttura del file. Come questo di sotto.</p> eterogeneita_ragged.csv<pre><code>a,b,c\n1,2,3\n4,5\n7,8,9,10\n</code></pre> <p>Se si prova semplicemente a stamparlo a schermo, Miller restituisce un errore:</p> <p>mlr --csv cat ./eterogeneita_ragged.csv</p> <pre><code>a,b,c\n1,2,3\nmlr :  mlr: CSV header/data length mismatch 3 != 2 at filename eterogeneita_ragged.csv row 3.\n</code></pre> <p>Questi gli errori del file, che ha un'intestazione composta da 3 campi:</p> <ol> <li>nella riga 2, ci sono 2 campi e non 3;</li> <li>nella riga 4, ci sono 4 campi e non 3.</li> </ol> <p>Miller \u00e8 in grado di gestire anche questa eterogeneit\u00e0, che deriva da errori, utilizzando il flag <code>--allow-ragged-csv-input</code>:</p> <p>mlr --csv --allow-ragged-csv-input ./eterogeneita_ragged.csv</p> <pre><code>a,b,c,4\n1,2,3,\n4,5,,\n7,8,9,10\n</code></pre> <p>Per il primo errore \u00e8 stata aggiunta una cella vuota; per il secondo \u00e8 stato aggiunto un campo a cui \u00e8 stato assegnato un'etichetta numerica - <code>4</code> - corrispondente al suo ordine nella lista dei campi.</p>"},{"location":"miller/eterogeneita_record/#irregular","title":"Irregular","text":"<p>Un altro tipo di eterogeneit\u00e0 \u00e8 legata a campi ordinati diversamente per ogni riga:</p> eterogeneita_irregular.json<pre><code>{\"a\": 1, \"b\": 2, \"c\": 3}\n{\"c\": 6, \"a\": 4, \"b\": 5}\n{\"b\": 8, \"c\": 9, \"a\": 7}\n</code></pre> <p>Se fosse necessario uniformare l'ordine si possono usare i verbi <code>regularize</code> o <code>sort-within-records</code>.</p> <p>Il verbo <code>regularize</code> riordina le righe nello stesso ordine della prima (qualunque sia l'ordine); il verbo <code>sort-in-records</code> usa semplicemente l'ordine alfabetico.</p>"},{"location":"miller/eterogeneita_record/#sparse","title":"Sparse","text":"<p>In ultimo c'\u00e8 l'eterogeneit\u00e0 pi\u00f9 frequente, legata a record che non sono composti tutti dagli stessi campi. Come ad esempio il <code>JSON</code> sottostante.</p> eterogeneita_sparse.json<pre><code>{\n  \"host\": \"xy01.east\",\n  \"status\": \"running\",\n  \"volume\": \"/dev/sda1\"\n}\n{\n  \"host\": \"xy92.west\",\n  \"status\": \"running\"\n}\n{\n  \"purpose\": \"failover\",\n  \"host\": \"xy55.east\",\n  \"volume\": \"/dev/sda1\",\n  \"reimaged\": true\n}\n</code></pre> <p>Si pu\u00f2 utilizzare il verbo <code>unsparsify</code> per fare in modo che tutti i record abbiano gli stessi campi.</p> <p>mlr --json unsparsify ./eterogeneita_sparse.json</p> <pre><code>{\n  \"host\": \"xy01.east\",\n  \"status\": \"running\",\n  \"volume\": \"/dev/sda1\",\n  \"purpose\": \"\",\n  \"reimaged\": \"\"\n}\n{\n  \"host\": \"xy92.west\",\n  \"status\": \"running\",\n  \"volume\": \"\",\n  \"purpose\": \"\",\n  \"reimaged\": \"\"\n}\n{\n  \"host\": \"xy55.east\",\n  \"status\": \"\",\n  \"volume\": \"/dev/sda1\",\n  \"purpose\": \"failover\",\n  \"reimaged\": true\n}\n</code></pre>"},{"location":"miller/flag/","title":"Flag","text":"<p>A seguire sono riportati i flag che si possono utilizzare in Miller. Ad esempio, nel comando</p> <p>mlr --icsv --ojson head -n 1 ./base_category.csv</p> <pre><code>{\n  \"nome\": \"andy\",\n  \"dataNascita\": \"1973-05-08\",\n  \"altezza\": 176,\n  \"peso\": 86.5,\n  \"comuneNascita\": \"Roma\"\n}\n</code></pre> <p><code>--icsv</code> e <code>--ojson</code> sono i flag con cui impostare il formato di input e output.</p>"},{"location":"miller/flag/#csv","title":"CSV","text":"<p>Si tratta di flag applicabili al formato <code>CSV</code>.</p> <p>Lista:</p> <ul> <li><code>--allow-ragged-csv-input</code> o <code>--ragged</code>: Se una riga dati ha meno campi della riga di intestazione, viene riempire con i campi rimanenti, vuoti. Se una riga di dati ha pi\u00f9 campi della riga di intestazione, Miller utilizza dei nomi di campo numerici interi, a partire da 1;</li> <li><code>--implicit-csv-header</code>, associa ai campi un nome implicito, un numero intero che parte da 1 e viene incrementato di 1, dal primo campo a sinistra all'ultimo a destra;</li> <li><code>--headerless-csv-output</code>, rimuove dall'output la riga di intestazione;</li> <li><code>-N</code>: una scorciatoia per mettere insieme la coppia <code>--implicit-csv-header</code> <code>--headerless-csv-output</code> (utile in casi come questo);</li> </ul>"},{"location":"miller/flag/#json","title":"JSON","text":"<p>Si tratta di flag applicabili al formato <code>CSV</code>.</p> <p>Lista:</p> <ul> <li><code>--jlistwrap</code>, include l'output JSON (che di default \u00e8 un JSON lines), tra <code>[]</code>;</li> <li><code>--no-jvstack</code>, dispone gli oggetti/array di output su una riga;</li> </ul>"},{"location":"miller/flag/#formati-file","title":"Formati file","text":"<p>Questi sono i flag legati ai formati.</p> <p>Lista:</p> <ul> <li><code>--asv or --asvlite</code>: Use ASV format for input and output data.</li> <li><code>--csv or -c</code>: il formato CSV come input e output.</li> <li><code>--csvlite</code>: Use CSV-lite format for input and output data.</li> <li><code>--dkvp</code>: Use DKVP format for input and output data.</li> <li><code>--gen-field-name</code>: Specify field name for --igen. Defaults to \"i\".</li> <li><code>--gen-start</code>: Specify start value for --igen. Defaults to 1.</li> <li><code>--gen-step</code>: Specify step value for --igen. Defaults to 1.</li> <li><code>--gen-stop</code>: Specify stop value for --igen. Defaults to 100.</li> <li><code>--iasv or --iasvlite</code>: Use ASV format for input data.</li> <li><code>--icsv</code>: il formato CSV come input.</li> <li><code>--icsvlite</code>: Use CSV-lite format for input data.</li> <li><code>--idkvp</code>: Use DKVP format for input data.</li> <li><code>--igen</code>: Ignore input files and instead generate sequential numeric input using --gen-field-name, --gen-start, --gen-step, and --gen-stop values. See also the seqgen verb, which is more useful/intuitive.</li> <li><code>--ijson</code>: il formato JSON come input.</li> <li><code>--ijsonl</code>: Use JSON Lines format for input data.</li> <li><code>--inidx</code>: Use NIDX format for input data.</li> <li><code>--io {format name}</code>: Use format name for input and output data. For example: <code>--io csv</code> is the same as <code>--csv</code>.</li> <li><code>--ipprint</code>: Use PPRINT format for input data.</li> <li><code>--itsv</code>: il formato TSV come input.</li> <li><code>--itsvlite</code>: Use TSV-lite format for input data.</li> <li><code>--iusv or --iusvlite</code>: Use USV format for input data.</li> <li><code>--ixtab</code>: Use XTAB format for input data.</li> <li><code>--json or -j</code>: il formato JSON come input and output.</li> <li><code>--jsonl</code>: Use JSON Lines format for input and output data.</li> <li><code>--nidx</code>: Use NIDX format for input and output data.</li> <li><code>--oasv or --oasvlite</code>: Use ASV format for output data.</li> <li><code>--ocsv</code>: il formato CSV come output.</li> <li><code>--ocsvlite</code>: Use CSV-lite format for output data.</li> <li><code>--odkvp</code>: Use DKVP format for output data.</li> <li><code>--ojson</code>: il formato JSON come output.</li> <li><code>--ojsonl</code>: Use JSON Lines format for output data.</li> <li><code>--omd</code>: Use markdown-tabular format for output data.</li> <li><code>--onidx</code>: Use NIDX format for output data.</li> <li><code>--opprint</code>: Use PPRINT format for output data.</li> <li><code>--otsv</code>: il formato TSV come output.</li> <li><code>--otsvlite</code>: Use TSV-lite format for output data.</li> <li><code>--ousv or --ousvlite</code>: Use USV format for output data.</li> <li><code>--oxtab</code>: Use XTAB format for output data.</li> <li><code>--pprint</code>: Use PPRINT format for input and output data.</li> <li><code>--tsv</code>: il formato TSV come input e output.</li> <li><code>--tsvlite or -t</code>: Use TSV-lite format for input and output data.</li> <li><code>--usv or --usvlite</code>: Use USV format for input and output data.</li> <li><code>--xtab</code>: Use XTAB format for input and output data.</li> <li><code>-i {format name}</code>: Use format name for input data. For example: <code>-i csv</code> is the same as <code>--icsv</code>.</li> <li><code>-o {format name}</code>: Use format name for output data.  For example: <code>-o csv</code> is the same as <code>--ocsv</code>.</li> </ul>"},{"location":"miller/formati/","title":"Gestione formati","text":"<p>Il formato nativo di Miller \u00e8 il <code>DKVP</code> (\"Delimited Key-Value Pairs\"), ovvero delle coppie chiave-valore, separate da virgola (la <code>,</code> \u00e8 il separatore di default). A seguire un esempio.</p> <p>Esempio del formato nativo. Notare che i record non hanno lo stesso numero di campi</p> <pre><code>nome=andy,dataNascita=1973-05-08,altezza=176,peso=86.5,comuneNascita=Roma\nnome=chiara,dataNascita=1993-12-13,altezza=162,peso=58.3,comuneNascita=Milano\nnome=guido,altezza=196,peso=90.4,comuneNascita=Roma\nnome=sara,dataNascita=2000-02-22,altezza=166,peso=70.4,comuneNascita=Roma\nnome=giulia,dataNascita=1997-08-13,altezza=169,peso=68.3\n</code></pre> <p>Spesso si pensa ai dati come \"rettangolari\": se sono previsti 10 campi, ogni record sar\u00e0 composto da 10 valori. Ma non \u00e8 sempre cos\u00ec, come nel caso del formato <code>JSON</code> o appunto di quello nativo di Miller, in cui ogni record non ha necessariamente lo stesso numero di campi degli altri.</p> <p>Miller di default gestisce quindi l'eterogeneit\u00e0 dei record.</p>"},{"location":"miller/formati/#conversione-di-formato","title":"Conversione di formato","text":"<p>Miller legge e scrive diversi formati di testo strutturato. Per impostare quello di input e di output \u00e8 necessario utilizzare uno dei flag dedicati e una delle modalit\u00e0 per farlo.</p> <p>Ad esempio per convertire un file da <code>CSV</code> a <code>TSV</code>, si pu\u00f2 usare questo comando:</p> <pre><code>mlr --icsv --otsv cat input.csv&gt;output.csv\n</code></pre> <p>Nel dettaglio:</p> <ul> <li><code>--icsv</code> per impostare il formato di input;</li> <li><code>--ocsv</code> per impostare il formato di output;</li> <li><code>cat</code> \u00e8 uno dei verbi di Miller, quello di base, che passa i dati senza alcuna trasformazione dall'input all'output.</li> </ul> <p>Nota  bene</p> <p>In un comando Miller, \u00e8 sempre necessario inserire almeno uno dei suoi verbi. Qui \u00e8 <code>cat</code>.</p> <p>C'\u00e8 anche la versione \"breve\" dello stesso comando, in cui <code>--icsv --otsv</code>, diventa <code>--c2t</code> (ovvero da <code>CSV</code> a <code>TSV</code>, che in inglese \u00e8 \"<code>CSV</code> TO <code>TSV</code>\"):</p> <pre><code>mlr --c2t cat input.csv&gt;output.csv\n</code></pre> <p>Qui a seguire i flag di base, per passare da uno dei possibili formati di input a uno di quelli di output.</p> IN/OUT CSV TSV JSON DKVP NIDX XTAB PPRINT Markdown CSV <code>--c2t</code> <code>--c2j</code> <code>--c2d</code> <code>--c2n</code> <code>--c2x</code> <code>--c2p</code> <code>--c2m</code> TSV <code>--t2c</code> <code>--t2j</code> <code>--t2d</code> <code>--t2n</code> <code>--t2x</code> <code>--t2p</code> <code>--t2m</code> JSON <code>--j2c</code> <code>--j2t</code> <code>--j2d</code> <code>--j2n</code> <code>--j2x</code> <code>--j2p</code> <code>--j2m</code> DKVP <code>--d2c</code> <code>--d2t</code> <code>--d2j</code> <code>--d2n</code> <code>--d2x</code> <code>--d2p</code> <code>--d2m</code> NIDX <code>--n2c</code> <code>--n2t</code> <code>--n2j</code> <code>--n2d</code> <code>--n2x</code> <code>--n2p</code> <code>--n2m</code> XTAB <code>--x2c</code> <code>--x2t</code> <code>--x2j</code> <code>--x2d</code> <code>--x2n</code> <code>--x2p</code> <code>--x2m</code> PPRINT <code>--p2c</code> <code>--p2t</code> <code>--p2j</code> <code>--p2d</code> <code>--p2n</code> <code>--p2x</code> <code>--p2m</code>"},{"location":"miller/formati/#elenco-formati","title":"Elenco formati","text":"<p>Il file di riferimento di input, usato per produrre i vari formati di output \u00e8 <code>base_category.csv</code>. Per ognuno di questi, \u00e8 stato inserito il comando per generarlo a partire dal <code>CSV</code> di input.</p>"},{"location":"miller/formati/#csv","title":"CSV","text":"<p>mlr --csv cat base_category.csv</p> <pre><code>andy,1973-05-08,176,86.5,Roma\nchiara,1993-12-13,162,58.3,Milano\nguido,2001-01-22,196,90.4,Roma\nsara,2000-02-22,166,70.4,Roma\ngiulia,1997-08-13,169,68.3,Milano\n</code></pre>"},{"location":"miller/formati/#dkvp-il-formato-nativo","title":"DKVP (il formato nativo)","text":"<p>mlr --c2d cat base_category.csv</p> <pre><code>nome=andy,dataNascita=1973-05-08,altezza=176,peso=86.5,comuneNascita=Roma\nnome=chiara,dataNascita=1993-12-13,altezza=162,peso=58.3,comuneNascita=Milano\nnome=guido,dataNascita=2001-01-22,altezza=196,peso=90.4,comuneNascita=Roma\nnome=sara,dataNascita=2000-02-22,altezza=166,peso=70.4,comuneNascita=Roma\nnome=giulia,dataNascita=1997-08-13,altezza=169,peso=68.3,comuneNascita=Milano\n</code></pre> <p>A seguire, lo stesso input in altri dei formati supportati da Miller.</p>"},{"location":"miller/formati/#tsv","title":"TSV","text":"<p>mlr --c2t cat base_category.csv</p> <pre><code>nome    dataNascita     altezza peso    comuneNascita\nandy    1973-05-08      176     86.5    Roma\nchiara  1993-12-13      162     58.3    Milano\nguido   2001-01-22      196     90.4    Roma\nsara    2000-02-22      166     70.4    Roma\ngiulia  1997-08-13      169     68.3    Milano\n</code></pre>"},{"location":"miller/formati/#nidx-index-numbered","title":"NIDX: Index-numbered","text":"<p>mlr --c2n cat base_category.csv</p> <pre><code>andy 1973-05-08 176 86.5 Roma\nchiara 1993-12-13 162 58.3 Milano\nguido 2001-01-22 196 90.4 Roma\nsara 2000-02-22 166 70.4 Roma\ngiulia 1997-08-13 169 68.3 Milano\n</code></pre>"},{"location":"miller/formati/#json","title":"JSON","text":"<p>Warning</p> <p>Il JSON di output di default di Miller 5 non \u00e8 propriamente un JSON.</p> <p>In Miller 5 (versione attuale, che a breve sar\u00e0 superata dalla 6), l'output di default  \u00e8 il JSON Lines. La scelta deriva dal fatto che \u00e8 un formato molto pi\u00f9 comodo per l'elaborazione con strumenti di parsing di testo e di versionamento; perch\u00e9 in questo formato ogni linea \u00e8 un JSON valido, e l'elaborazione per linea \u00e8 molto pi\u00f9 comoda e tipica per i client.</p> <p>\u00c8 quindi come sotto:</p> <p>mlr --c2j cat base_category.csv</p> <pre><code>{\"nome\": \"andy\", \"dataNascita\": \"1973-05-08\", \"altezza\": 176, \"peso\": 86.5, \"comuneNascita\": \"Roma\"}\n{\"nome\": \"chiara\", \"dataNascita\": \"1993-12-13\", \"altezza\": 162, \"peso\": 58.3, \"comuneNascita\": \"Milano\"}\n{\"nome\": \"guido\", \"dataNascita\": \"2001-01-22\", \"altezza\": 196, \"peso\": 90.4, \"comuneNascita\": \"Roma\"}\n{\"nome\": \"sara\", \"dataNascita\": \"2000-02-22\", \"altezza\": 166, \"peso\": 70.4, \"comuneNascita\": \"Roma\"}\n{\"nome\": \"giulia\", \"dataNascita\": \"1997-08-13\", \"altezza\": 169, \"peso\": 68.3, \"comuneNascita\": \"Milano\"}\n</code></pre> <p>Se si vuole un \"vero\" JSON bisogna aggiungere il flag <code>--jlistwrap</code>:</p> <p>mlr --c2j --jlistwrap cat base_category.csv</p> <pre><code>[\n{ \"nome\": \"andy\", \"dataNascita\": \"1973-05-08\", \"altezza\": 176, \"peso\": 86.5, \"comuneNascita\": \"Roma\" }\n,{ \"nome\": \"chiara\", \"dataNascita\": \"1993-12-13\", \"altezza\": 162, \"peso\": 58.3, \"comuneNascita\": \"Milano\" }\n,{ \"nome\": \"guido\", \"dataNascita\": \"2001-01-22\", \"altezza\": 196, \"peso\": 90.4, \"comuneNascita\": \"Roma\" }\n,{ \"nome\": \"sara\", \"dataNascita\": \"2000-02-22\", \"altezza\": 166, \"peso\": 70.4, \"comuneNascita\": \"Roma\" }\n,{ \"nome\": \"giulia\", \"dataNascita\": \"1997-08-13\", \"altezza\": 169, \"peso\": 68.3, \"comuneNascita\": \"Milano\" }\n]\n</code></pre> <p>In Miller 6 (prossimo al rilascio) l'output di default \u00e8 invece un <code>JSON</code>:</p> <p>mlr --c2j cat base_category.csv</p> <pre><code>[\n  {\n    \"nome\": \"andy\",\n    \"dataNascita\": \"1973-05-08\",\n    \"altezza\": 176,\n    \"peso\": 86.5,\n    \"comuneNascita\": \"Roma\"\n  },\n  {\n    \"nome\": \"chiara\",\n    \"dataNascita\": \"1993-12-13\",\n    \"altezza\": 162,\n    \"peso\": 58.3,\n    \"comuneNascita\": \"Milano\"\n  },\n  {\n    \"nome\": \"guido\",\n    \"dataNascita\": \"2001-01-22\",\n    \"altezza\": 196,\n    \"peso\": 90.4,\n    \"comuneNascita\": \"Roma\"\n  },\n  {\n    \"nome\": \"sara\",\n    \"dataNascita\": \"2000-02-22\",\n    \"altezza\": 166,\n    \"peso\": 70.4,\n    \"comuneNascita\": \"Roma\"\n  },\n  {\n    \"nome\": \"giulia\",\n    \"dataNascita\": \"1997-08-13\",\n    \"altezza\": 169,\n    \"peso\": 68.3,\n    \"comuneNascita\": \"Milano\"\n  }\n]\n</code></pre> <p>Se in Miller 6 si vuole un JSON Lines (formato molto consigliato), bisogna scegliere il flag <code>--ojsonl</code>:</p> <p>mlr --icsv --ojsonl cat base_category.csv</p> <pre><code>{\"nome\": \"andy\", \"dataNascita\": \"1973-05-08\", \"altezza\": 176, \"peso\": 86.5, \"comuneNascita\": \"Roma\"}\n{\"nome\": \"chiara\", \"dataNascita\": \"1993-12-13\", \"altezza\": 162, \"peso\": 58.3, \"comuneNascita\": \"Milano\"}\n{\"nome\": \"guido\", \"dataNascita\": \"2001-01-22\", \"altezza\": 196, \"peso\": 90.4, \"comuneNascita\": \"Roma\"}\n{\"nome\": \"sara\", \"dataNascita\": \"2000-02-22\", \"altezza\": 166, \"peso\": 70.4, \"comuneNascita\": \"Roma\"}\n{\"nome\": \"giulia\", \"dataNascita\": \"1997-08-13\", \"altezza\": 169, \"peso\": 68.3, \"comuneNascita\": \"Milano\"}\n</code></pre>"},{"location":"miller/formati/#pprint-pretty-printed-tabular","title":"PPRINT: Pretty-printed tabular","text":"<p>mlr --c2p --barred cat base_category.csv</p> <pre><code>+--------+-------------+---------+------+---------------+\n| nome   | dataNascita | altezza | peso | comuneNascita |\n+--------+-------------+---------+------+---------------+\n| andy   | 1973-05-08  | 176     | 86.5 | Roma          |\n| chiara | 1993-12-13  | 162     | 58.3 | Milano        |\n| guido  | 2001-01-22  | 196     | 90.4 | Roma          |\n| sara   | 2000-02-22  | 166     | 70.4 | Roma          |\n| giulia | 1997-08-13  | 169     | 68.3 | Milano        |\n+--------+-------------+---------+------+---------------+\n</code></pre>"},{"location":"miller/formati/#xtab-vertical-tabular","title":"XTAB: Vertical tabular","text":"<p>mlr --c2x cat base_category.csv</p> <pre><code>nome          andy\ndataNascita   1973-05-08\naltezza       176\npeso          86.5\ncomuneNascita Roma\n\nnome          chiara\ndataNascita   1993-12-13\naltezza       162\npeso          58.3\ncomuneNascita Milano\n\nnome          guido\ndataNascita   2001-01-22\naltezza       196\npeso          90.4\ncomuneNascita Roma\n\nnome          sara\ndataNascita   2000-02-22\naltezza       166\npeso          70.4\ncomuneNascita Roma\n\nnome          giulia\ndataNascita   1997-08-13\naltezza       169\npeso          68.3\ncomuneNascita Milano\n</code></pre>"},{"location":"miller/formati/#markdown","title":"Markdown","text":"<p>mlr --c2m cat base_category.csv</p> <pre><code>| nome | dataNascita | altezza | peso | comuneNascita |\n| --- | --- | --- | --- | --- |\n| andy | 1973-05-08 | 176 | 86.5 | Roma |\n| chiara | 1993-12-13 | 162 | 58.3 | Milano |\n| guido | 2001-01-22 | 196 | 90.4 | Roma |\n| sara | 2000-02-22 | 166 | 70.4 | Roma |\n| giulia | 1997-08-13 | 169 | 68.3 | Milano |\n</code></pre>"},{"location":"miller/formati/#casi-speciali-e-consigli","title":"Casi speciali e consigli","text":""},{"location":"miller/formati/#file-csv-anche-tsv-senza-riga-di-intestazione","title":"File CSV (anche TSV) senza riga di intestazione","text":"<p>In questo file non \u00e8 presente la riga di intestazione.</p> input.csv<pre><code>1,861265,C,A,0.071\n1,861265,C,A,0.148\n1,861265,C,G,0.001\n1,861265,C,G,0.108\n1,861265,C,T,0\n1,861265,C,T,0.216\n2,193456,G,A,0.006\n2,193456,G,A,0.094\n2,193456,G,C,0.011\n2,193456,G,C,0.152\n2,193456,G,T,0.003\n2,193456,G,T,0.056\n</code></pre> <p>Si pu\u00f2 fare riferimento ai campi in modo da numerico, con un progressivo numerico di una unit\u00e0 a partire da 1, da sinistra verso destra. Il <code>flag</code> utile al caso \u00e8 <code>-N</code>, che d\u00e0 per implicito che non ci sia la riga di intestazione (e ne assegna una temporanea con campi numerici) e che non venga aggiunta in output. Quindi se da questo file vorr\u00f2 estrarre le prime tre righe della prima colonna il comando sar\u00e0:</p> <p>mlr --csv -N cut -f 1 then head -n 3 input.csv</p> <pre><code>1\n1\n1\n</code></pre>"},{"location":"miller/formati/#per-condividere-esempi-di-dati-da-leggere-a-schermo","title":"Per condividere esempi di dati da \"leggere\" a schermo","text":"<p>Molto spesso in mailing list, forum, su Stack Exchange, su GitHub \u00e8 utile condividere una porzione di file sui cui si sta lavorando, in modo da essere pi\u00f9 chiari nel descrivere qual \u00e8 la questione che si sta affrontando con i dati.</p> <p>Il formato consigliato \u00e8 il pretty printed, che produce un output ben leggibile ovunque.</p> <p>Ad esempio se lo si vuole applicare a questo <code>CSV</code>, il comando</p> <pre><code>mlr --c2p --barred cat ./base_category.csv\n</code></pre> <p>produce l'output sottostante, che si potr\u00e0 copiare e incollare dovunque serva, producendo qualcosa di leggibile ovunque, senza vincoli di formattazione o client usato.</p> <pre><code>+--------+-------------+---------+------+---------------+\n| nome   | dataNascita | altezza | peso | comuneNascita |\n+--------+-------------+---------+------+---------------+\n| andy   | 1973-05-08  | 176     | 86.5 | Roma          |\n| chiara | 1993-12-13  | 162     | 58.3 | Milano        |\n| guido  | 2001-01-22  | 196     | 90.4 | Roma          |\n| sara   | 2000-02-22  | 166     | 70.4 | Roma          |\n| giulia | 1997-08-13  | 169     | 68.3 | Milano        |\n+--------+-------------+---------+------+---------------+\n</code></pre> <p>In alternativa si pu\u00f2 produrre un output in <code>Markdown</code>, ma non \u00e8 compatibile con tutti gli spazi di pubblicazione e/o tutti i client.</p>"},{"location":"miller/ricette/","title":"Ricette Miller","text":""},{"location":"miller/ricette/#concatenare-in-verticale-piu-file","title":"Concatenare in \"verticale\" pi\u00f9 file","text":"<p>Il verbo \"tipico\" per concatenare due o pi\u00f9 file \u00e8 <code>cat</code>. Ad esempio se voglio unire in verticale questi due file <code>CSV</code></p> base.cvbase_merge.csv <pre><code>nome,dataNascita,altezza,peso\nandy,1973-05-08,176,86.5\nchiara,1993-12-13,162,58.3\nguido,2001-01-22,196,90.4\n</code></pre> <pre><code>nome,dataNascita,altezza,peso,coloreOcchi\nmarco,1983-12-08,183,,verdi\nlicia,1993-12-07,158,57.9,neri\n</code></pre> <p>il comando da lanciare sar\u00e0</p> <pre><code>mlr --csv cat base.csv base_merge.csv &gt;output.csv\n</code></pre> <p>che dar\u00e0 in output</p> <pre><code>+--------+-------------+---------+------+\n| nome   | dataNascita | altezza | peso |\n+--------+-------------+---------+------+\n| andy   | 1973-05-08  | 176     | 86.5 |\n| chiara | 1993-12-13  | 162     | 58.3 |\n| guido  | 2001-01-22  | 196     | 90.4 |\n| marco  | 1983-12-08  | 183     | -    |\n| licia  | 1993-12-07  | 158     | 57.9 |\n+--------+-------------+---------+------+\n</code></pre> <p>\u00c8 possibile fare il merge, l'unione in verticale, anche di due file con uno schema in parte diverso, perch\u00e9 Miller gestiste l'eterogeneit\u00e0 dei record. Se ad esempio si \u00e8 in presenza di un file che ha una colonna in pi\u00f9 (<code>coloreOcchi</code>) rispetto a <code>base.csv</code>, come questo (<code>base_altro.csv</code>)</p> <pre><code>nome,dataNascita,altezza,peso,coloreOcchi\nmarco,1983-12-08,183,,verdi\nlicia,1993-12-07,158,57.9,neri\n</code></pre> <p>il verbo da usare \u00e8 <code>unsparsify</code>. Il comando sar\u00e0:</p> <pre><code>mlr --csv unsparsify base.csv base_altro.csv &gt;output.csv\n</code></pre> <p>In output, verr\u00e0 aggiunta la colonna <code>coloreOcchi</code>, che non sar\u00e0 valorizzata per i record del file che in partenza non aveva questa colonna:</p> <pre><code>+--------+-------------+---------+------+-------------+\n| nome   | dataNascita | altezza | peso | coloreOcchi |\n+--------+-------------+---------+------+-------------+\n| andy   | 1973-05-08  | 176     | 86.5 | -           |\n| chiara | 1993-12-13  | 162     | 58.3 | -           |\n| guido  | 2001-01-22  | 196     | 90.4 | -           |\n| marco  | 1983-12-08  | 183     | -    | verdi       |\n| licia  | 1993-12-07  | 158     | 57.9 | neri        |\n+--------+-------------+---------+------+-------------+\n</code></pre>"},{"location":"miller/ricette/#suddividere-un-file-di-input-in-piu-file-di-output-ogni-xxx-record","title":"Suddividere un file di input in pi\u00f9 file di output, ogni xxx record","text":"<pre><code> mlr --csv put -q '\n  begin {\n    @batch_size = 1000;\n  }\n  index = int(floor((NR-1) / @batch_size));\n  label = fmtnum(index,\"%04d\");\n  filename = \"part-\".label.\".json\";\n  tee &gt; filename, $*\n' ./input.csv\n</code></pre> <p>Verr\u00e0 creato un file di output, con nome <code>part-000XXX</code>, ogni 1000 (si imposta tramite <code>@batch_size</code>) record.</p>"},{"location":"miller/ricette/#estrarre-le-righe-che-contengono-il-valore-massimo-di-un-campo","title":"Estrarre le righe che contengono il valore massimo di un campo","text":"<p>Alcune delle righe sottostanti, sono identiche, fatta eccezione per il V campo.</p> input.csv<pre><code>1,861265,C,A,0.071\n1,861265,C,A,0.148\n1,861265,C,G,0.001\n1,861265,C,G,0.108\n1,861265,C,T,0\n1,861265,C,T,0.216\n2,193456,G,A,0.006\n2,193456,G,A,0.094\n2,193456,G,C,0.011\n2,193456,G,C,0.152\n2,193456,G,T,0.003\n2,193456,G,T,0.056\n</code></pre> <p>Se si vogliono estrarre soltanto quelle con il valore massimo del V campo, raggruppate per i valori degli altri 4, il verbo da usare \u00e8 top</p> <p>mlr --csv -N top -f 5  -g 1,2,3,4 input.tsv</p> <pre><code>1,861265,C,A,1,0.148\n1,861265,C,G,1,0.108\n1,861265,C,T,1,0.216\n2,193456,G,A,1,0.094\n2,193456,G,C,1,0.152\n2,193456,G,T,1,0.056\n</code></pre> <p>Vedi https://stackoverflow.com/a/70664880/757714</p>"},{"location":"miller/ricette/#eseguire-un-comando-esterno-allinterno-di-una-funzione","title":"Eseguire un comando esterno all'interno di una funzione","text":"<p>All'interno di un comando Miller \u00e8 possibile lanciare una utility esterna, usando la funzione <code>system</code>.</p> <p>Immaginiamo ad esempio di avere un file come questo</p> input.txt<pre><code>a,b\n1,\"15,1,2/AX,22,1/C,1/A,1/BA,2,3\"\n</code></pre> <p>e di voler applicare il cosiddetto natural sorting alla stringa <code>15,1,2/AX,22,1/C,1/A,1/BA,2,3</code>, ottenendo questo ordinamento <code>1,1/A,1/BA,1/C,2,2/AX,3,15,22</code>.</p> <p>Utilizzando le utility standard della shell di Linux basterebbe fare cos\u00ec (in <code>paste</code> si usa <code>-</code> perch\u00e9 l'input \u00e8 l'<code>stdin</code>):</p> <pre><code>echo \"15,1,2/AX,22,1/C,1/A,1/BA,2,3\" | tr , \"\\n\" | sort -V | paste -sd, -\n</code></pre> <p>Per riportare questa sintassi<sup>1</sup> in un comando Miller, il comando sarebbe come questo di sotto, in cui viene creato il campo <code>toto</code>, che raccoglie valori derivanti dal lancio di utility esterne, grazie alla funzione <code>system</code>.</p> <pre><code>&lt;input.txt mlr --c2p --barred cat  then put -S '$toto=system(\"echo \".$b.\" | tr , \\\"\\n\\\" | sort -V | paste -sd, -\")'\n</code></pre> <p>E l'output:</p> <pre><code>+---+-------------------------------+-------------------------------+\n| a | b                             | toto                          |\n+---+-------------------------------+-------------------------------+\n| 1 | 15,1,2/AX,22,1/C,1/A,1/BA,2,3 | 1,1/A,1/BA,1/C,2,2/AX,3,15,22 |\n+---+-------------------------------+-------------------------------+\n</code></pre> <p>Nel comando bisogna avere cura di inserire eventuali <code>escape</code> a caratteri come <code>\"</code>.</p>"},{"location":"miller/ricette/#fare-un-trova-e-sostituisci-per-campo","title":"Fare un trova e sostituisci per campo","text":"<p>\u00c8 un'operazione classica che si realizza sfruttando il verbo <code>put</code> e funzioni come <code>sub</code> e <code>gsub</code>.</p> <p>Se ad esempio si vuole cercare la stringa <code>Denis</code> e sostituirla con <code>Dennis</code>, nel campo <code>nomeCampo</code>, la sintassi tipo \u00e8:</p> <pre><code>mlr --csv put '$nomeCampo=sub($nomeCampo,\"Denis\",\"Dennis\")' input.csv\n</code></pre> <p>La funzione <code>sub</code> ha tre argomenti, separati <code>,</code>:</p> <ul> <li>a cosa applicare la sostituzione;</li> <li>cosa cercare;</li> <li>con cosa sostituirlo.</li> </ul> <p>Alcune note:</p> <ul> <li>i nomi dei campi, nelle funzioni scritte in <code>put</code> hanno come prefisso il <code>$</code>. Se ci sono spazi usare le parentesi graffe (i.e. <code>${nome campo}</code>);</li> <li>in questo esempio si dice al campo <code>nomeCampo</code>, che sar\u00e0 uguale a se stesso, con le sostituzioni da fare.</li> </ul> <p>\u00c8 possibile usare le espressioni regolari, usando la sintassi del caso, ed \u00e8 possibile mettere in fila enne processi di sostituzione, separati da <code>;</code>:</p> <pre><code>$nomeCampo=sub($nomeCampo,\"Denis\",\"Dennis\");$altroCampo=sub($altroCampo,\"^Bau\",\"Miao\")\n</code></pre> <p>Nota bene</p> <p><code>sub</code> esegue il trova e sostituisci della prima occorrenza che trova in una cella, mentre <code>gsub</code> per tutte le occorrenze in quella cella.</p>"},{"location":"miller/ricette/#fare-un-trova-e-sostituisci-globale","title":"Fare un trova e sostituisci globale","text":"<p>\u00c8 comodo utilizzare <code>DSL</code>, il linguaggio di scripting di Miller e usare un ciclo for:</p> <pre><code>mlr --csv -S put 'for (k in $*) {$[k] = gsub($[k], \"e\", \"X\")}' foo.csv\n</code></pre> <p>Per tutti i campi - <code>k</code> - verr\u00e0 applicata la funzione <code>gsub</code> (trova e sostituisci globale con supporto a espressioni regolari), che (in questo esempio) cerca la stringa <code>e</code> e la sostituisce con <code>X</code>.</p> <p>L'opzione <code>-S</code> per forzare che tutti i campi siano interpretati come stringhe.</p> <p>Se usi Miller 5</p> <p>Il comando cambia in questo modo, inserendo l'opzione <code>-S</code> dopo il verbo <code>put</code>:</p> <pre><code>mlr --csv put -S 'for (k in $*) {$[k] = gsub($[k], \"e\", \"X\")}' foo.csv\n</code></pre>"},{"location":"miller/ricette/#fare-un-trova-e-sostituisci-per-campo-nuovi-verbi","title":"Fare un trova e sostituisci per campo - nuovi verbi","text":"<p>La release <code>6.9</code> di Miller ha introdotto i nuovi verbi <code>sub</code>, <code>gsub</code> e <code>ssub</code>, per applicare comandi di trova e sostituisci, in modo molto pi\u00f9 diretto e comodo della modalit\u00e0 descritta sopra. Con i verbi <code>sub</code> e <code>gsub</code> sono supportate le espressioni regolari.</p> <p>Se ad esempio ho questa tabella di input:</p> nome dataNascita altezza peso andy 1973-05-08 176 86.5 chiara 1993-12-13 162 58.3 guido 2001-01-22 196 90.4 <p>E voglio sostituire <code>1973</code> con <code>2021</code> nella colonna <code>dataNascita</code>, si pu\u00f2 lanciare questo comando:</p> <pre><code>mlr --csv sub -f dataNascita 1973 2021 input.csv\n</code></pre> <p>Ed ottenere:</p> nome dataNascita altezza peso andy 2021-05-08 176 86.5 chiara 1993-12-13 162 58.3 guido 2001-01-22 196 90.4"},{"location":"miller/ricette/#rimuovere-i-ritorni-a-capo-nelle-celle","title":"Rimuovere i ritorni a capo nelle celle","text":"<p>Prendendo spunto dalla ricetta sul trova e sostituisce globale, basta cercare il carattere di ritorno a capo.</p> <p>In input un CSV come quello di sotto (qui), in cui all'interno delle celle, ci sono dei ritorno a capo materializzati da dei line feed, ovvero mappati con i caratteri speciali <code>\\n</code>.</p> rimuovi-a-capo.txt<pre><code>Campo 1,Campo 2\n\"Cella con\nA capo\nFastidiosi\",Ipsum\nLorem,\"uno\nDue\nTre\nQuattro\nCinque\"\n</code></pre> <p>Si pu\u00f2 cercare appunto <code>\\n</code> e sostituirlo con spazio, e poi rimuovere eventuali doppi spazi usando il verbo <code>clean-whitespace</code>:</p> <pre><code>mlr --csv -S put 'for (k in $*) {$[k] = gsub($[k], \"\\n\", \" \")}' then clean-whitespace rimuovi-a-capo.txt\n</code></pre> <p>In output:</p> <pre><code>Campo 1,Campo 2\nCella con A capo Fastidiosi,Ipsum\nLorem,uno Due Tre Quattro Cinque\n</code></pre> <p>Nota bene</p> <p><code>\\n</code> non \u00e8 l'unico modo di materializzare un ritorno a capo, quindi \u00e8 possibile dover modificare l'esempio di sopra.</p>"},{"location":"miller/ricette/#suddividere-un-file-in-piu-parti-in-dipendenza-del-valore-di-un-campo","title":"Suddividere un file in pi\u00f9 parti, in dipendenza del valore di un campo","text":"<p>Il verbo da usare \u00e8 <code>tee</code>.</p> <p>A partire ad esempio da</p> <pre><code>DeviceID,AreaName,Longitude,Latitude\n12311,Dubai,55.55431,25.45631\n12311,Dubai,55.55432,25.45634\n12311,Dubai,55.55433,25.45637\n12311,Dubai,55.55431,25.45621\n12309,Dubai,55.55427,25.45627\n12309,Dubai,55.55436,25.45655\n12412,Dubai,55.55441,25.45657\n12412,Dubai,55.55442,25.45656\n</code></pre> <p>e lanciando</p> <pre><code>mlr --csv --from input.csv put -q 'tee &gt; $DeviceID.\".csv\", $*'\n</code></pre> <p>si avranno questi tre file</p> <pre><code>#12311.csv\nDeviceID,AreaName,Longitude,Latitude\n12311,Dubai,55.55431,25.45631\n12311,Dubai,55.55432,25.45634\n12311,Dubai,55.55433,25.45637\n12311,Dubai,55.55431,25.45621\n\n#12412.csv\nDeviceID,AreaName,Longitude,Latitude\n12412,Dubai,55.55441,25.45657\n12412,Dubai,55.55442,25.45656\n\n#12309.csv\nDeviceID,AreaName,Longitude,Latitude\n12309,Dubai,55.55427,25.45627\n12309,Dubai,55.55436,25.45655\n</code></pre>"},{"location":"miller/ricette/#aggiungere-una-colonna-con-il-conteggio-dei-valori-distinti-di-un-o-piu-campo","title":"Aggiungere una colonna con il conteggio dei valori distinti di un (o pi\u00f9) campo","text":"<p>A partire da</p> nome dataNascita altezza peso comuneNascita sesso andy 1973-05-08 176 86.5 Roma maschio chiara 1993-12-13 162 58.3 Milano femmina guido 2001-01-22 196 90.4 Roma maschio sara 2000-02-22 166 70.4 Roma femmina giulia 1997-08-13 169 68.3 Milano femmina <p>si vuole aggiungere un colonna che riporti il numero di valori distinti del campo <code>comuneNascita</code>.</p> <p>Si pu\u00f2 usare <code>count-similar</code>. Il comando \u00e8</p> <pre><code>mlr --csv count-similar -g comuneNascita input.csv\n</code></pre> <p>che in output restituisce</p> nome dataNascita altezza peso comuneNascita sesso count andy 1973-05-08 176 86.5 Roma maschio 3 guido 2001-01-22 196 90.4 Roma maschio 3 sara 2000-02-22 166 70.4 Roma femmina 3 chiara 1993-12-13 162 58.3 Milano femmina 2 giulia 1997-08-13 169 68.3 Milano femmina 2 <p>Due note:</p> <ul> <li>si pu\u00f2 cambiare nome alla colonna di output usando il parametro <code>-o</code>;</li> <li>si possono conteggiare i valori distinti per combinazioni di pi\u00f9 campi. Ad esempio <code>-g comuneNascita,sesso</code>.</li> </ul>"},{"location":"miller/ricette/#estrarre-una-colonna-in-modo-numerico","title":"Estrarre una colonna in modo numerico","text":"<p>In Miller si pu\u00f2 puntare a una colonna in modo numerico, soltanto dentro codice DSL (il codice di programmazione di Miller). Se si vuole ad esempio estrarre la colonna numero <code>3</code>, si pu\u00f2 fare cos\u00ec:</p> <pre><code>mlrgo --csv put -q '\n  @i = 0;\n  for (k, v in $*) {\n    @i += 1;\n    if (@i == 3) {\n      emit {k: v};\n      break;\n    }\n  }\n' input.csv\n</code></pre> <p>Nel codice di sopra la terza colonna, la numero <code>3</code>, si fissa con <code>@i == 3</code>.</p> <ol> <li> <p>qui commentata su explainshell \u21a9</p> </li> </ol>"},{"location":"miller/verbi/","title":"Verbi","text":"<p>I verbi sono i sub comandi di Miller. Queste le categorie:</p> <ul> <li>quelli analoghi allo Unix-toolkit: cat, cut, grep, head, join, sort, tac, tail, top, uniq.</li> <li>quelli con funziolit\u00e0 simili a quelli di <code>awk</code>: filter, put, sec2gmt, sec2gmtdate, step, tee.</li> <li>quelli statistici: bar, bootstrap, decimate, histogram, least-frequent, most-frequent, sample, shuffle, stats1, stats2.</li> <li>quelli orientati all'eterogeneit\u00e0 dei record, sebbene tutti i verbi di Miller sono in grado di gestire record eterogenei: group-by, group-like, having-fields.</li> <li>e altri ancora: check, count-distinct, label, merge-fields, nest, nothing, regularize, rename, reorder, reshape, seqgen.</li> </ul>"},{"location":"miller/verbi/#file-di-esempio","title":"File di esempio","text":"<p>File per sviluppare esempi</p> <p>Per la gran parte degli esempi sviluppati in questa pagina, verr\u00e0 usato il file <code>base_category.csv</code> (vedi sotto). \u00c8 stato scelto un file piccolo e semplice, per ragioni didattiche e di leggibilit\u00e0.</p> nome dataNascita altezza peso comuneNascita andy 1973-05-08 176 86.5 Roma chiara 1993-12-13 162 58.3 Milano guido 2001-01-22 196 90.4 Roma sara 2000-02-22 166 70.4 Roma giulia 1997-08-13 169 68.3 Milano base_category.csv<pre><code>nome,dataNascita,altezza,peso,comuneNascita\nandy,1973-05-08,176,86.5,Roma\nchiara,1993-12-13,162,58.3,Milano\nguido,2001-01-22,196,90.4,Roma\nsara,2000-02-22,166,70.4,Roma\ngiulia,1997-08-13,169,68.3,Milano\n</code></pre>"},{"location":"miller/verbi/#guida-in-linea-per-un-verbo","title":"Guida in linea per un verbo","text":"<p>Per aprire la guida in linea di un verbo, basta lanciare <code>mlr nomeVerbo --help</code>. Ad esempio per il verbo <code>cat</code>, il comando <code>mlr cat --help</code> restituir\u00e0:</p> <pre><code>Usage: mlr cat [options]\nPasses input records directly to output. Most useful for format conversion.\nOptions:\n-n        Prepend field \"n\" to each record with record-counter starting at 1\n-g {comma-separated field name(s)} When used with -n/-N, writes record-counters\n          keyed by specified field name(s).\n-v        Write a low-level record-structure dump to stderr.\n-N {name} Prepend field {name} to each record with record-counter starting at 1\n</code></pre> <p>A seguire, per ogni verbo, sar\u00e0 inserito l'help ufficiale di ogni comando.</p>"},{"location":"miller/verbi/#lista-dei-verbi","title":"Lista dei verbi","text":""},{"location":"miller/verbi/#altkv","title":"altkv","text":"<p>Mappa una lista di valori, come coppie alternate chiave/valore.</p> <p>mlr altkv --help</p> <pre><code>Usage: mlr altkv [options]\nGiven fields with values of the form a,b,c,d,e,f emits a=b,c=d,e=f pairs.\n</code></pre> <p>Ad esempio</p> <p>echo \"a,b,c,d\" | mlr --ocsv altkv</p> <pre><code>{\"a\":\"b\",\"c\":\"d\"}\n</code></pre>"},{"location":"miller/verbi/#bar","title":"bar","text":"<p>Per creare dei grafici a barre (bruttini \ud83d\ude43), rimpiazzando dei valori numeri con una serie di asterichi. Per allinearli meglio si possono usare le opzioni di output <code>--opprint</code> o <code>--oxtab</code>.</p> <p>mlr bar --help</p> <pre><code>Replaces a numeric field with a number of asterisks, allowing for cheesy\nbar plots. These align best with --opprint or --oxtab output format.\nOptions:\n-f   {a,b,c}      Field names to convert to bars.\n--lo {lo}         Lower-limit value for min-width bar: default '0.000000'.\n--hi {hi}         Upper-limit value for max-width bar: default '100.000000'.\n-w   {n}          Bar-field width: default '40'.\n--auto            Automatically computes limits, ignoring --lo and --hi.\n                Holds all records in memory before producing any output.\n-c   {character}  Fill character: default '*'.\n-x   {character}  Out-of-bounds character: default '#'.\n-b   {character}  Blank character: default '.'.\nNominally the fill, out-of-bounds, and blank characters will be strings of length 1.\nHowever you can make them all longer if you so desire.\n</code></pre> <p>A partire ad esempio da questo file di input:</p> bar.csv<pre><code>Area,percUtenti\nNord,25\nCentro,32\nSud e isole,43\n</code></pre> <p>mlr --c2p bar -f percUtenti ./bar.csv</p> <pre><code>Area        percUtenti\nNord        **********..............................\nCentro      ************............................\nSud e isole *****************.......................\n</code></pre>"},{"location":"miller/verbi/#bootstrap","title":"bootstrap","text":"<p>L'uso tipico di <code>bootstrap</code> \u00e8 quello di estrarre un campione random dall'input, con un numero di record pari al numero record di input; con doppioni possibili.</p> <p>mlr bootstrap --help</p> <pre><code>Emits an n-sample, with replacement, of the input records.\nSee also mlr sample and mlr shuffle.\nOptions:\n -n Number of samples to output. Defaults to number of input records.\n    Must be non-negative.\n</code></pre> <p>Sotto un esempio in cui a partire dal file di input, composto da 5 record distinti, sono estratti in modo randomico 5 record.</p> <p>mlr --csv bootstrap base_category.csv</p> <pre><code>nome,dataNascita,altezza,peso,comuneNascita\nguido,2001-01-22,196,90.4,Roma\nchiara,1993-12-13,162,58.3,Milano\nchiara,1993-12-13,162,58.3,Milano\nguido,2001-01-22,196,90.4,Roma\nchiara,1993-12-13,162,58.3,Milano\n</code></pre>"},{"location":"miller/verbi/#cat","title":"cat","text":"<p>Utile sopratutto per conversione di formato (vedi formati) e per concatenare in \"verticale\" file con lo stesso schema ( ricetta).</p> <p>mlr cat --help</p> <pre><code>Usage: mlr cat [options]\nPasses input records directly to output. Most useful for format conversion.\nOptions:\n-n        Prepend field \"n\" to each record with record-counter starting at 1\n-g {comma-separated field name(s)} When used with -n/-N, writes record-counters\n          keyed by specified field name(s).\n-v        Write a low-level record-structure dump to stderr.\n-N {name} Prepend field {name} to each record with record-counter starting at 1\n</code></pre> <p>Il comando di base stampa a schermo il contenuto di un file:</p> <p>mlr --csv cat base_category.csv</p> <pre><code>nome,dataNascita,altezza,peso,comuneNascita\nandy,1973-05-08,176,86.5,Roma\nchiara,1993-12-13,162,58.3,Milano\nguido,2001-01-22,196,90.4,Roma\nsara,2000-02-22,166,70.4,Roma\ngiulia,1997-08-13,169,68.3,Milano\n</code></pre> <p>Con l'opzione <code>-n</code> si aggiunge un campo con un progressivo numerico intero che parte da <code>1</code>:</p> <p>mlr --csv -n cat base.csv</p> <pre><code>n,nome,dataNascita,altezza,peso,comuneNascita\n1,andy,1973-05-08,176,86.5,Roma\n2,chiara,1993-12-13,162,58.3,Milano\n3,guido,2001-01-22,196,90.4,Roma\n4,sara,2000-02-22,166,70.4,Roma\n5,giulia,1997-08-13,169,68.3,Milano\n</code></pre> <p>Se all'opzione <code>-n</code> si aggiunge <code>-g</code> seguito da dal nome di uno o pi\u00f9 campi, il \"contatore\" sar\u00e0 applicato per gruppo e partir\u00e0 da <code>1</code> distintamente per ogni gruppo. Qui sotto ad esempio il raggruppamento \u00e8 fatto per <code>comuneNascita</code>:</p> <p>mlr --csv cat -n -g comuneNascita base_category.csv</p> <pre><code>n,nome,dataNascita,altezza,peso,comuneNascita\n1,andy,1973-05-08,176,86.5,Roma\n1,chiara,1993-12-13,162,58.3,Milano\n2,guido,2001-01-22,196,90.4,Roma\n3,sara,2000-02-22,166,70.4,Roma\n2,giulia,1997-08-13,169,68.3,Milano\n</code></pre>"},{"location":"miller/verbi/#check","title":"check","text":"<p>\u00c8 un verbo che non produce un output, salvo un report sulla correttezza di \"formattazione\" del file di input.</p> <p>mlr check --help</p> <pre><code>Consumes records without printing any output.\nUseful for doing a well-formatted check on input data.\nOptions:\n</code></pre> <p>Se ad esempio si ha un file <code>CSV</code>, con un numero di campi nell'intestazione (<code>4</code>) diverso da quello del corpo (<code>5</code>), come questo</p> check.csv<pre><code>nome,dataNascita,altezza,comuneNascita\nandy,1973-05-08,176,86.5,Roma\nchiara,1993-12-13,162,58.3,Milano\nguido,2001-01-22,196,90.4,Roma\n</code></pre> <p>si avr\u00e0 l'output sottostante</p> <p> mlr --csv check check.csv</p> <pre><code>mlr :  mlr: CSV header/data length mismatch 4 != 5 at filename ./check.csv row 2.\n</code></pre>"},{"location":"miller/verbi/#clean-whitespace","title":"clean-whitespace","text":"<p>\u00c8 un prezioso comando, per \"pulire\" i dati: rimuove gli spazi ridondanti.</p> <p>mlr clean-whitespace --help</p> <pre><code>For each record, for each field in the record, whitespace-cleans the keys and/or\nvalues. Whitespace-cleaning entails stripping leading and trailing whitespace,\nand replacing multiple whitespace with singles. For finer-grained control,\nplease see the DSL functions lstrip, rstrip, strip, collapse_whitespace,\nand clean_whitespace.\n\nOptions:\n-k|--keys-only    Do not touch values.\n-v|--values-only  Do not touch keys.\nIt is an error to specify -k as well as -v -- to clean keys and values,\nleave off -k as well as -v.\n</code></pre> <p>Riuove in particolare uno o pi\u00f9 spazi bianchi a inizio e fine cella e due o pi\u00f9 spazi all'interndo della cella. Ad esempio nel file sottostante ci sono due spazi tra <code>Busto</code> e <code>Arsizio</code>, uno spazio a fine cella dopo <code>andy</code> e uno a inizio cella i corrispondenza di <code>chiara</code>. Questi sono ridondanti, e nella grandissima parte dei casi sono da rimuovere.</p> clean-whitespace.json<pre><code>{\"nome\":\"andy \",\"dataNascita\":\"1973-05-08\",\"comuneNascita\":\"Roma\"}\n{\"nome\":\" chiara\",\"dataNascita\":\"1993-12-13\",\"comuneNascita\":\"Busto  Arsizio\"}\n</code></pre> <p>Per pulire il file:</p> <p>mlr --json clean-whitespace input.json</p> <pre><code>{\"nome\":\"andy\",\"dataNascita\":\"1973-05-08\",\"comuneNascita\":\"Roma\"}\n{\"nome\":\"chiara\",\"dataNascita\":\"1993-12-13\",\"comuneNascita\":\"Busto Arsizio\"}\n</code></pre> <p>Due comode opzioni:</p> <ul> <li><code>-k</code> fa la pulizia soltanto nei nomi dei campi, nelle chiavi;</li> <li><code>-v</code> fa la pulizia soltanto nei valori.</li> </ul>"},{"location":"miller/verbi/#count","title":"count","text":"<p>Restituisce il numero di record. Miller tiene conto del formato, quindi per un CSV composto da 6 righe, 1 di intestazione pi\u00f9 5 di dati, restituir\u00e0 5.</p> <p>mlr count --help</p> <pre><code>Prints number of records, optionally grouped by distinct values for specified field names.\nOptions:\n-g {a,b,c} Optional group-by-field names for counts, e.g. a,b,c\n-n {n} Show only the number of distinct values. Not interesting without -g.\n-o {name} Field name for output-count. Default \"count\".\n</code></pre> <p>File di esempio:</p> base_category.csv<pre><code>nome,dataNascita,altezza,peso,comuneNascita\nandy,1973-05-08,176,86.5,Roma\nchiara,1993-12-13,162,58.3,Milano\nguido,2001-01-22,196,90.4,Roma\nsara,2000-02-22,166,70.4,Roma\ngiulia,1997-08-13,169,68.3,Milano\n</code></pre> <p>Comando di esempio:</p> <p>mlr --csv count ./base_category.csv</p> <pre><code>count\n5\n</code></pre> <p>Se si desidera soltanto l'output numerico, senza riga di intestazione, si pu\u00f2 fare in tantissimi modi. Uno \u00e8 quello di cambiare il formato di output in NIDX:</p> <p>mlr --c2n count ./base_category.csv</p> <pre><code>5\n</code></pre>"},{"location":"miller/verbi/#count-distinct","title":"count-distinct","text":"<p>Restituisce il numero di record che hanno valori distinti, per uno o pi\u00f9 campi specificati.</p> <p>mlr count-distinct --help</p> <pre><code>Prints number of records having distinct values for specified field names.\nSame as uniq -c.\n\nOptions:\n-f {a,b,c}    Field names for distinct count.\n-n            Show only the number of distinct values. Not compatible with -u.\n-o {name}     Field name for output count. Default \"count\".\n              Ignored with -u.\n-u            Do unlashed counts for multiple field names. With -f a,b and\n              without -u, computes counts for distinct combinations of a\n              and b field values. With -f a,b and with -u, computes counts\n              for distinct a field values and counts for distinct b field\n              values separately.\n</code></pre> <p>File di esempio:</p> count-distinct.csv<pre><code>nome,dataNascita,altezza,peso,comuneNascita,sesso\nandy,1973-05-08,176,86.5,Roma,maschio\nchiara,1993-12-13,162,58.3,Milano,femmina\nguido,2001-01-22,196,90.4,Roma,maschio\nsara,2000-02-22,166,70.4,Roma,femmina\ngiulia,1997-08-13,169,68.3,Milano,femmina\n</code></pre> <p>Ad esempio il conteggio per combinazioni distinte di comune di nascita e sesso:</p> <p>mlr --csv count-distinct -f comuneNascita,sesso -o conteggio ./base_category.csv</p> <pre><code>comuneNascita,sesso,conteggio\nRoma,maschio,2\nMilano,femmina,2\nRoma,femmina,1\n</code></pre> <p>Aggiungendo il parametro <code>-u</code>, si ottengono i valori distinti non per combinazioni distinti dei campi indicati, ma per ogni singolo campo.</p> <p>mlr --csv count-distinct -f comuneNascita,sesso -o conteggio -u ./base_category.csv</p> <pre><code>field,value,count\ncomuneNascita,Roma,3\ncomuneNascita,Milano,2\nsesso,maschio,2\nsesso,femmina,3\n</code></pre>"},{"location":"miller/verbi/#count-similar","title":"count-similar","text":"<p>Aggiunge un campo, con il conteggio dei record che ha lo stesso valore per uno o pi\u00f9 campi.</p> <p>mlr count-similar --help</p> <pre><code>Ingests all records, then emits each record augmented by a count of\nthe number of other records having the same group-by field values.\nOptions:\n-g {a,b,c} Group-by-field names for counts, e.g. a,b,c\n-o {name} Field name for output-counts. Defaults to \"count\".\n</code></pre> <p>File di esempio:</p> base_category.csv<pre><code>nome,dataNascita,altezza,peso,comuneNascita\nandy,1973-05-08,176,86.5,Roma\nchiara,1993-12-13,162,58.3,Milano\nguido,2001-01-22,196,90.4,Roma\nsara,2000-02-22,166,70.4,Roma\ngiulia,1997-08-13,169,68.3,Milano\n</code></pre> <p>A seguire ad esempio viene aggiunto il campo <code>conteggio</code> al file di input, con il conteggio dei valori distinti per il campo <code>comuneNascita</code>.</p> <p>mlr --csv count-similar -g comuneNascita -o conteggio ./base_category.csv</p> <pre><code>nome,dataNascita,altezza,peso,comuneNascita,conteggio\nandy,1973-05-08,176,86.5,Roma,3\nguido,2001-01-22,196,90.4,Roma,3\nsara,2000-02-22,166,70.4,Roma,3\nchiara,1993-12-13,162,58.3,Milano,2\ngiulia,1997-08-13,169,68.3,Milano,2\n</code></pre>"},{"location":"miller/verbi/#cut","title":"cut","text":"<p>Estrae/rimuove dall'input uno o pi\u00f9 campi.</p> <p>mlr cut --help</p> <pre><code>Passes through input records with specified fields included/excluded.\nOptions:\n -f {a,b,c} Comma-separated field names for cut, e.g. a,b,c.\n -o Retain fields in the order specified here in the argument list.\n    Default is to retain them in the order found in the input data.\n -x|--complement  Exclude, rather than include, field names specified by -f.\n -r Treat field names as regular expressions. \"ab\", \"a.*b\" will\n   match any field name containing the substring \"ab\" or matching\n   \"a.*b\", respectively; anchors of the form \"^ab$\", \"^a.*b$\" may\n   be used. The -o flag is ignored when -r is present.\nExamples:\n  mlr cut -f hostname,status\n  mlr cut -x -f hostname,status\n  mlr cut -r -f '^status$,sda[0-9]'\n  mlr cut -r -f '^status$,\"sda[0-9]\"'\n  mlr cut -r -f '^status$,\"sda[0-9]\"i' (this is case-insensitive)\n</code></pre> <p>File di esempio:</p> base_category.csv<pre><code>nome,dataNascita,altezza,peso,comuneNascita\nandy,1973-05-08,176,86.5,Roma\nchiara,1993-12-13,162,58.3,Milano\nguido,2001-01-22,196,90.4,Roma\nsara,2000-02-22,166,70.4,Roma\ngiulia,1997-08-13,169,68.3,Milano\n</code></pre> <p>Per estrarre soltanto il campo <code>nome</code>:</p> <p>mlr --csv cut -f nome base_category.csv</p> <pre><code>nome\nandy\nchiara\nguido\nsara\ngiulia\n</code></pre> <p>Per rimuovere il campo <code>nome</code>, bisogner\u00e0 aggiungere l'opzione <code>-x</code>:</p> <p>mlr --csv cut -x -f nome base_category.csv</p> <pre><code>dataNascita,altezza,peso,comuneNascita\n1973-05-08,176,86.5,Roma\n1993-12-13,162,58.3,Milano\n2001-01-22,196,90.4,Roma\n2000-02-22,166,70.4,Roma\n1997-08-13,169,68.3,Milano\n</code></pre> <p>Per impostare il filtro tramite espressione regolare si usa l'opzione <code>-r</code>. Ad esempio estrarre soltanto i campi che iniziano per <code>a</code> e che terminano per <code>o</code>:</p> <p>mlr --csv cut -r -f \"^a\",\"o$\" base_category.csv</p> <pre><code>altezza,peso\n176,86.5\n162,58.3\n196,90.4\n166,70.4\n169,68.3\n</code></pre>"},{"location":"miller/verbi/#decimate","title":"decimate","text":"<p>Estrae un record ogni <code>n</code>, opzionalmente per categoria.</p> <p>mlr decimate --help</p> <pre><code>Passes through one of every n records, optionally by category.\nOptions:\n -b Decimate by printing first of every n.\n -e Decimate by printing last of every n (default).\n -g {a,b,c} Optional group-by-field names for decimate counts, e.g. a,b,c.\n -n {n} Decimation factor (default 10).\n</code></pre>"},{"location":"miller/verbi/#fill-down","title":"fill-down","text":"<p>Se un dato record ha un valore mancante per un dato campo, verr\u00e0 riempito dal valore corrispondente del record precedente, se presente.</p> <p>mlr fill-down --help</p> <pre><code>If a given record has a missing value for a given field, fill that from\nthe corresponding value from a previous record, if any.\nBy default, a 'missing' field either is absent, or has the empty-string value.\nWith -a, a field is 'missing' only if it is absent.\n\nOptions:\n --all Operate on all fields in the input.\n -a|--only-if-absent If a given record has a missing value for a given field,\n     fill that from the corresponding value from a previous record, if any.\n     By default, a 'missing' field either is absent, or has the empty-string value.\n     With -a, a field is 'missing' only if it is absent.\n -f  Field names for fill-down.\n</code></pre> <p>File di esempio:</p> fill-down.csv<pre><code>a,b,c\n1,,3\n4,5,\n7,,9\n</code></pre> <p>Con l'opzione <code>-all</code> il verbo viene applicato per tutti i record:</p> <p>mlr --csv  fill-down --all  fill-down.csv</p> <pre><code>a,b,c\n1,,3\n4,5,3\n7,5,9\n</code></pre> <p>Con <code>-f</code> \u00e8 possibile scegliere di applicaro a 1 o pi\u00f9 campi.</p>"},{"location":"miller/verbi/#fill-empty","title":"fill-empty","text":"<p>Il verbo <code>fill-empty</code> richiede Miller &gt;= 6.0</p> <p>Riempe le celle vuote, con un valore specifico</p> <p>mlr fill-empty --help</p> <pre><code>Fills empty-string fields with specified fill-value.\nOptions:\n-v {string} Fill-value: defaults to \"N/A\"\n-S          Don't infer type -- so '-v 0' would fill string 0 not int 0.\n</code></pre> <p>File di esempio:</p> fill-down.csv<pre><code>a,b,c\n1,,3\n4,5,\n7,,9\n</code></pre> <p>Come impopstazione predefinita, il valore assegnato sar\u00e0 <code>N/A</code>, ma \u00e8 possibile impostarlo con <code>-v</code>:</p> <p>mlr --csv  fill-empty fill-down.csv</p> <pre><code>a,b,c\n1,N/A,3\n4,5,N/A\n7,N/A,9\n</code></pre>"},{"location":"miller/verbi/#filter","title":"filter","text":"<p>mlr filter --help</p> <pre><code>Options:\n-f {file name} File containing a DSL expression (see examples below). If the filename\n   is a directory, all *.mlr files in that directory are loaded.\n\n-e {expression} You can use this after -f to add an expression. Example use\n   case: define functions/subroutines in a file you specify with -f, then call\n   them with an expression you specify with -e.\n\n(If you mix -e and -f then the expressions are evaluated in the order encountered.\nSince the expression pieces are simply concatenated, please be sure to use intervening\nsemicolons to separate expressions.)\n\n-s name=value: Predefines out-of-stream variable @name to have\n    Thus mlr put -s foo=97 '$column += @foo' is like\n    mlr put 'begin {@foo = 97} $column += @foo'.\n    The value part is subject to type-inferencing.\n    May be specified more than once, e.g. -s name1=value1 -s name2=value2.\n    Note: the value may be an environment variable, e.g. -s sequence=$SEQUENCE\n\n-x (default false) Prints records for which {expression} evaluates to false, not true,\n   i.e. invert the sense of the filter expression.\n\n-q Does not include the modified record in the output stream.\n   Useful for when all desired output is in begin and/or end blocks.\n\n-S and -F: There are no-ops in Miller 6 and above, since now type-inferencing is done\n   by the record-readers before filter/put is executed. Supported as no-op pass-through\n   flags for backward compatibility.\n\n\nParser-info options:\n\n-w Print warnings about things like uninitialized variables.\n\n-W Same as -w, but exit the process if there are any warnings.\n\n-p Prints the expressions's AST (abstract syntax tree), which gives full\n  transparency on the precedence and associativity rules of Miller's grammar,\n  to stdout.\n\n-d Like -p but uses a parenthesized-expression format for the AST.\n\n-D Like -d but with output all on one line.\n\n-E Echo DSL expression before printing parse-tree\n\n-v Same as -E -p.\n\n-X Exit after parsing but before stream-processing. Useful with -v/-d/-D, if you\n   only want to look at parser information.\n\nRecords will pass the filter depending on the last bare-boolean statement in\nthe DSL expression. That can be the result of &lt;, ==, &gt;, etc., the return value of a function call\nwhich returns boolean, etc.\n\nExamples:\n  mlr --csv --from example.csv filter '$color == \"red\"'\n  mlr --csv --from example.csv filter '$color == \"red\" &amp;&amp; flag == true'\nMore example filter expressions:\n  First record in each file:\n    'FNR == 1'\n  Subsampling:\n    'urand() &lt; 0.001'\n  Compound booleans:\n    '$color != \"blue\" &amp;&amp; $value &gt; 4.2'\n    '($x &lt; 0.5 &amp;&amp; $y &lt; 0.5) || ($x &gt; 0.5 &amp;&amp; $y &gt; 0.5)'\n  Regexes with case-insensitive flag\n    '($name =~ \"^sys.*east$\") || ($name =~ \"^dev.[0-9]+\"i)'\n  Assignments, then bare-boolean filter statement:\n    '$ab = $a+$b; $cd = $c+$d; $ab != $cd'\n  Bare-boolean filter statement within a conditional:\n    'if (NR &lt; 100) {\n      $x &gt; 0.3;\n    } else {\n      $x &gt; 0.002;\n    }\n    '\n  Using 'any' higher-order function to see if $index is 10, 20, or 30:\n    'any([10,20,30], func(e) {return $index == e})'\n\nSee also https://miller.readthedocs.io/reference-dsl for more context.\n</code></pre>"},{"location":"miller/verbi/#flatten","title":"flatten","text":"<p>mlr flatten --help</p> <pre><code>Flattens multi-level maps to single-level ones. Example: field with name 'a'\nand value '{\"b\": { \"c\": 4 }}' becomes name 'a.b.c' and value 4.\nOptions:\n-f Comma-separated list of field names to flatten (default all).\n-s Separator, defaulting to mlr --flatsep value.\n</code></pre>"},{"location":"miller/verbi/#format-values","title":"format-values","text":"<p>mlr format-values --help</p> <pre><code>Applies format strings to all field values, depending on autodetected type.\n* If a field value is detected to be integer, applies integer format.\n* Else, if a field value is detected to be float, applies float format.\n* Else, applies string format.\n\nNote: this is a low-keystroke way to apply formatting to many fields. To get\nfiner control, please see the fmtnum function within the mlr put DSL.\n\nNote: this verb lets you apply arbitrary format strings, which can produce\nundefined behavior and/or program crashes.  See your system's \"man printf\".\n\nOptions:\n-i {integer format} Defaults to \"%d\".\n                    Examples: \"%06lld\", \"%08llx\".\n                    Note that Miller integers are long long so you must use\n                    formats which apply to long long, e.g. with ll in them.\n                    Undefined behavior results otherwise.\n-f {float format}   Defaults to \"%f\".\n                    Examples: \"%8.3lf\", \"%.6le\".\n                    Note that Miller floats are double-precision so you must\n                    use formats which apply to double, e.g. with l[efg] in them.\n                    Undefined behavior results otherwise.\n-s {string format}  Defaults to \"%s\".\n                    Examples: \"_%s\", \"%08s\".\n                    Note that you must use formats which apply to string, e.g.\n                    with s in them. Undefined behavior results otherwise.\n-n                  Coerce field values autodetected as int to float, and then\n                    apply the float format.\n</code></pre>"},{"location":"miller/verbi/#fraction","title":"fraction","text":"<p>mlr fraction --help</p> <pre><code>For each record's value in specified fields, computes the ratio of that\nvalue to the sum of values in that field over all input records.\nE.g. with input records  x=1  x=2  x=3  and  x=4, emits output records\nx=1,x_fraction=0.1  x=2,x_fraction=0.2  x=3,x_fraction=0.3  and  x=4,x_fraction=0.4\n\nNote: this is internally a two-pass algorithm: on the first pass it retains\ninput records and accumulates sums; on the second pass it computes quotients\nand emits output records. This means it produces no output until all input is read.\n\nOptions:\n-f {a,b,c}    Field name(s) for fraction calculation\n-g {d,e,f}    Optional group-by-field name(s) for fraction counts\n-p            Produce percents [0..100], not fractions [0..1]. Output field names\n              end with \"_percent\" rather than \"_fraction\"\n-c            Produce cumulative distributions, i.e. running sums: each output\n              value folds in the sum of the previous for the specified group\n              E.g. with input records  x=1  x=2  x=3  and  x=4, emits output records\n              x=1,x_cumulative_fraction=0.1  x=2,x_cumulative_fraction=0.3\n              x=3,x_cumulative_fraction=0.6  and  x=4,x_cumulative_fraction=1.0\n</code></pre>"},{"location":"miller/verbi/#gap","title":"gap","text":"<p>mlr gap --help</p> <pre><code>Emits an empty record every n records, or when certain values change.\nOptions:\nEmits an empty record every n records, or when certain values change.\n-g {a,b,c} Print a gap whenever values of these fields (e.g. a,b,c) changes.\n-n {n} Print a gap every n records.\nOne of -f or -g is required.\n-n is ignored if -g is present.\n</code></pre>"},{"location":"miller/verbi/#grep","title":"grep","text":"<p>mlr grep --help</p> <pre><code>Passes through records which match the regular expression.\nOptions:\n-i  Use case-insensitive search.\n-v  Invert: pass through records which do not match the regex.\nNote that \"mlr filter\" is more powerful, but requires you to know field names.\nBy contrast, \"mlr grep\" allows you to regex-match the entire record. It does\nthis by formatting each record in memory as DKVP, using command-line-specified\nORS/OFS/OPS, and matching the resulting line against the regex specified\nhere. In particular, the regex is not applied to the input stream: if you\nhave CSV with header line \"x,y,z\" and data line \"1,2,3\" then the regex will\nbe matched, not against either of these lines, but against the DKVP line\n\"x=1,y=2,z=3\".  Furthermore, not all the options to system grep are supported,\nand this command is intended to be merely a keystroke-saver. To get all the\nfeatures of system grep, you can do\n  \"mlr --odkvp ... | grep ... | mlr --idkvp ...\"\n</code></pre>"},{"location":"miller/verbi/#group-by","title":"group-by","text":"<p>mlr group-by --help</p> <pre><code>Outputs records in batches having identical values at specified field names.Options:\n</code></pre>"},{"location":"miller/verbi/#group-like","title":"group-like","text":"<p>mlr group-like --help</p> <pre><code>Outputs records in batches having identical field names.\nOptions:\n</code></pre>"},{"location":"miller/verbi/#having-fields","title":"having-fields","text":"<p>mlr having-fields --help</p> <pre><code>Conditionally passes through records depending on each record's field names.\nOptions:\n  --at-least      {comma-separated names}\n  --which-are     {comma-separated names}\n  --at-most       {comma-separated names}\n  --all-matching  {regular expression}\n  --any-matching  {regular expression}\n  --none-matching {regular expression}\nExamples:\n  mlr having-fields --which-are amount,status,owner\n  mlr having-fields --any-matching 'sda[0-9]'\n  mlr having-fields --any-matching '\"sda[0-9]\"'\n  mlr having-fields --any-matching '\"sda[0-9]\"i' (this is case-insensitive)\n</code></pre>"},{"location":"miller/verbi/#head","title":"head","text":"<p>mlr head --help</p> <pre><code>Passes through the first n records, optionally by category.\nWithout -g, ceases consuming more input (i.e. is fast) when n records have been read.\nOptions:\n-g {a,b,c} Optional group-by-field names for head counts, e.g. a,b,c.\n-n {n} Head-count to print. Default 10.\n</code></pre>"},{"location":"miller/verbi/#histogram","title":"histogram","text":"<p>mlr histogram --help</p> <pre><code>Just a histogram. Input values &lt; lo or &gt; hi are not counted.\n-f {a,b,c}    Value-field names for histogram counts\n--lo {lo}     Histogram low value\n--hi {hi}     Histogram high value\n--nbins {n}   Number of histogram bins. Defaults to 20.\n--auto        Automatically computes limits, ignoring --lo and --hi.\n              Holds all values in memory before producing any output.\n-o {prefix}   Prefix for output field name. Default: no prefix.\n</code></pre>"},{"location":"miller/verbi/#json-parse","title":"json-parse","text":"<p>mlr json-parse --help</p> <pre><code>Tries to convert string field values to parsed JSON, e.g. \"[1,2,3]\" -&gt; [1,2,3].\nOptions:\n-f {...} Comma-separated list of field names to json-parse (default all).\n</code></pre>"},{"location":"miller/verbi/#json-stringify","title":"json-stringify","text":"<p>mlr json-stringify --help</p> <pre><code>Produces string field values from field-value data, e.g. [1,2,3] -&gt; \"[1,2,3]\".\nOptions:\n-f {...} Comma-separated list of field names to json-parse (default all).\n--jvstack Produce multi-line JSON output.\n--no-jvstack Produce single-line JSON output per record (default).\n</code></pre>"},{"location":"miller/verbi/#join","title":"join","text":"<p>mlr join --help</p> <pre><code>Joins records from specified left file name with records from all file names\nat the end of the Miller argument list.\nFunctionality is essentially the same as the system \"join\" command, but for\nrecord streams.\nOptions:\n  -f {left file name}\n  -j {a,b,c}   Comma-separated join-field names for output\n  -l {a,b,c}   Comma-separated join-field names for left input file;\n               defaults to -j values if omitted.\n  -r {a,b,c}   Comma-separated join-field names for right input file(s);\n               defaults to -j values if omitted.\n  --lp {text}  Additional prefix for non-join output field names from\n               the left file\n  --rp {text}  Additional prefix for non-join output field names from\n               the right file(s)\n  --np         Do not emit paired records\n  --ul         Emit unpaired records from the left file\n  --ur         Emit unpaired records from the right file(s)\n  -s|--sorted-input  Require sorted input: records must be sorted\n               lexically by their join-field names, else not all records will\n               be paired. The only likely use case for this is with a left\n               file which is too big to fit into system memory otherwise.\n  -u           Enable unsorted input. (This is the default even without -u.)\n               In this case, the entire left file will be loaded into memory.\n               If you wish to use a prepipe command for the main input as well\n               as here, it must be specified there as well as here.\n  --prepipex {command} Likewise.\nFile-format options default to those for the right file names on the Miller\nargument list, but may be overridden for the left file as follows. Please see\n  -i {one of csv,dkvp,nidx,pprint,xtab}\n  --irs {record-separator character}\n  --ifs {field-separator character}\n  --ips {pair-separator character}\n  --repifs\n  --implicit-csv-header\n  --no-implicit-csv-header\nFor example, if you have 'mlr --csv ... join -l foo ... ' then the left-file format will\nbe specified CSV as well unless you override with 'mlr --csv ... join --ijson -l foo' etc.\nLikewise, if you have 'mlr --csv --implicit-csv-header ...' then the join-in file will be\nexpected to be headerless as well unless you put '--no-implicit-csv-header' after 'join'.\nPlease use \"mlr --usage-separator-options\" for information on specifying separators.\nPlease see https://miller.readthedocs.io/en/latest/reference-verbs.html#join for more information\nincluding examples.\n</code></pre>"},{"location":"miller/verbi/#label","title":"label","text":"<p>mlr label --help</p> <pre><code>Given n comma-separated names, renames the first n fields of each record to\nhave the respective name. (Fields past the nth are left with their original\nnames.) Particularly useful with --inidx or --implicit-csv-header, to give\nuseful names to otherwise integer-indexed fields.\n\nOptions:\n</code></pre>"},{"location":"miller/verbi/#least-frequent","title":"least-frequent","text":"<p>mlr least-frequent --help</p> <pre><code>Shows the least frequently occurring distinct values for specified field names.\nThe first entry is the statistical anti-mode; the remaining are runners-up.\nOptions:\n-f {one or more comma-separated field names}. Required flag.\n-n {count}. Optional flag defaulting to 10.\n-b          Suppress counts; show only field values.\n-o {name}   Field name for output count. Default \"count\".\nSee also \"mlr most-frequent\".\n</code></pre>"},{"location":"miller/verbi/#merge-fields","title":"merge-fields","text":"<p>mlr merge-fields --help</p> <pre><code>Computes univariate statistics for each input record, accumulated across\nspecified fields.\nOptions:\n-a {sum,count,...}  Names of accumulators. One or more of:\n  count    Count instances of fields\n  mode     Find most-frequently-occurring values for fields; first-found wins tie\n  antimode Find least-frequently-occurring values for fields; first-found wins tie\n  sum      Compute sums of specified fields\n  mean     Compute averages (sample means) of specified fields\n  var      Compute sample variance of specified fields\n  stddev   Compute sample standard deviation of specified fields\n  meaneb   Estimate error bars for averages (assuming no sample autocorrelation)\n  skewness Compute sample skewness of specified fields\n  kurtosis Compute sample kurtosis of specified fields\n  min      Compute minimum values of specified fields\n  max      Compute maximum values of specified fields\n-f {a,b,c}  Value-field names on which to compute statistics. Requires -o.\n-r {a,b,c}  Regular expressions for value-field names on which to compute\n            statistics. Requires -o.\n-c {a,b,c}  Substrings for collapse mode. All fields which have the same names\n            after removing substrings will be accumulated together. Please see\n            examples below.\n-i          Use interpolated percentiles, like R's type=7; default like type=1.\n            Not sensical for string-valued fields.\n-o {name}   Output field basename for -f/-r.\n-k          Keep the input fields which contributed to the output statistics;\n            the default is to omit them.\n\nString-valued data make sense unless arithmetic on them is required,\ne.g. for sum, mean, interpolated percentiles, etc. In case of mixed data,\nnumbers are less than strings.\n\nExample input data: \"a_in_x=1,a_out_x=2,b_in_y=4,b_out_x=8\".\nExample: mlr merge-fields -a sum,count -f a_in_x,a_out_x -o foo\n  produces \"b_in_y=4,b_out_x=8,foo_sum=3,foo_count=2\" since \"a_in_x,a_out_x\" are\n  summed over.\nExample: mlr merge-fields -a sum,count -r in_,out_ -o bar\n  produces \"bar_sum=15,bar_count=4\" since all four fields are summed over.\nExample: mlr merge-fields -a sum,count -c in_,out_\n  produces \"a_x_sum=3,a_x_count=2,b_y_sum=4,b_y_count=1,b_x_sum=8,b_x_count=1\"\n  since \"a_in_x\" and \"a_out_x\" both collapse to \"a_x\", \"b_in_y\" collapses to\n  \"b_y\", and \"b_out_x\" collapses to \"b_x\".\n</code></pre>"},{"location":"miller/verbi/#most-frequent","title":"most-frequent","text":"<p>mlr most-frequent --help</p> <pre><code>Shows the most frequently occurring distinct values for specified field names.\nThe first entry is the statistical mode; the remaining are runners-up.\nOptions:\n-f {one or more comma-separated field names}. Required flag.\n-n {count}. Optional flag defaulting to 10.\n-b          Suppress counts; show only field values.\n-o {name}   Field name for output count. Default \"count\".\nSee also \"mlr least-frequent\".\n</code></pre>"},{"location":"miller/verbi/#nest","title":"nest","text":"<p>mlr nest --help</p> <pre><code>Explodes specified field values into separate fields/records, or reverses this.\nOptions:\n  --explode,--implode   One is required.\n  --values,--pairs      One is required.\n  --across-records,--across-fields One is required.\n  -f {field name}       Required.\n  --nested-fs {string}  Defaults to \";\". Field separator for nested values.\n  --nested-ps {string}  Defaults to \":\". Pair separator for nested key-value pairs.\n  --evar {string}       Shorthand for --explode --values ---across-records --nested-fs {string}\n  --ivar {string}       Shorthand for --implode --values ---across-records --nested-fs {string}\nPlease use \"mlr --usage-separator-options\" for information on specifying separators.\n\nExamples:\n\n  mlr nest --explode --values --across-records -f x\n  with input record \"x=a;b;c,y=d\" produces output records\n    \"x=a,y=d\"\n    \"x=b,y=d\"\n    \"x=c,y=d\"\n  Use --implode to do the reverse.\n\n  mlr nest --explode --values --across-fields -f x\n  with input record \"x=a;b;c,y=d\" produces output records\n    \"x_1=a,x_2=b,x_3=c,y=d\"\n  Use --implode to do the reverse.\n\n  mlr nest --explode --pairs --across-records -f x\n  with input record \"x=a:1;b:2;c:3,y=d\" produces output records\n    \"a=1,y=d\"\n    \"b=2,y=d\"\n    \"c=3,y=d\"\n\n  mlr nest --explode --pairs --across-fields -f x\n  with input record \"x=a:1;b:2;c:3,y=d\" produces output records\n    \"a=1,b=2,c=3,y=d\"\n\nNotes:\n* With --pairs, --implode doesn't make sense since the original field name has\n  been lost.\n* The combination \"--implode --values --across-records\" is non-streaming:\n  no output records are produced until all input records have been read. In\n  particular, this means it won't work in tail -f contexts. But all other flag\n  combinations result in streaming (tail -f friendly) data processing.\n* It's up to you to ensure that the nested-fs is distinct from your data's IFS:\n  e.g. by default the former is semicolon and the latter is comma.\nSee also mlr reshape.\n</code></pre>"},{"location":"miller/verbi/#nothing","title":"nothing","text":"<p>mlr nothing --help</p> <pre><code>Drops all input records. Useful for testing, or after tee/print/etc. have\nproduced other output.\nOptions:\n</code></pre>"},{"location":"miller/verbi/#put","title":"put","text":"<p>mlr put --help</p> <pre><code>Options:\n-f {file name} File containing a DSL expression (see examples below). If the filename\n   is a directory, all *.mlr files in that directory are loaded.\n\n-e {expression} You can use this after -f to add an expression. Example use\n   case: define functions/subroutines in a file you specify with -f, then call\n   them with an expression you specify with -e.\n\n(If you mix -e and -f then the expressions are evaluated in the order encountered.\nSince the expression pieces are simply concatenated, please be sure to use intervening\nsemicolons to separate expressions.)\n\n-s name=value: Predefines out-of-stream variable @name to have\n    Thus mlr put -s foo=97 '$column += @foo' is like\n    mlr put 'begin {@foo = 97} $column += @foo'.\n    The value part is subject to type-inferencing.\n    May be specified more than once, e.g. -s name1=value1 -s name2=value2.\n    Note: the value may be an environment variable, e.g. -s sequence=$SEQUENCE\n\n-x (default false) Prints records for which {expression} evaluates to false, not true,\n   i.e. invert the sense of the filter expression.\n\n-q Does not include the modified record in the output stream.\n   Useful for when all desired output is in begin and/or end blocks.\n\n-S and -F: There are no-ops in Miller 6 and above, since now type-inferencing is done\n   by the record-readers before filter/put is executed. Supported as no-op pass-through\n   flags for backward compatibility.\n\n\nParser-info options:\n\n-w Print warnings about things like uninitialized variables.\n\n-W Same as -w, but exit the process if there are any warnings.\n\n-p Prints the expressions's AST (abstract syntax tree), which gives full\n  transparency on the precedence and associativity rules of Miller's grammar,\n  to stdout.\n\n-d Like -p but uses a parenthesized-expression format for the AST.\n\n-D Like -d but with output all on one line.\n\n-E Echo DSL expression before printing parse-tree\n\n-v Same as -E -p.\n\n-X Exit after parsing but before stream-processing. Useful with -v/-d/-D, if you\n   only want to look at parser information.\n\nExamples:\n  mlr --from example.csv put '$qr = $quantity * $rate'\nMore example put expressions:\n  If-statements:\n    'if ($flag == true) { $quantity *= 10}'\n    'if ($x &gt; 0.0 { $y=log10($x); $z=sqrt($y) } else {$y = 0.0; $z = 0.0}'\n  Newly created fields can be read after being written:\n    '$new_field = $index**2; $qn = $quantity * $new_field'\n  Regex-replacement:\n    '$name = sub($name, \"http.*com\"i, \"\")'\n  Regex-capture:\n    'if ($a =~ \"([a-z]+)_([0-9]+)) { $b = \"left_\\1\"; $c = \"right_\\2\" }'\n  Built-in variables:\n    '$filename = FILENAME'\n  Aggregations (use mlr put -q):\n    '@sum += $x; end {emit @sum}'\n    '@sum[$shape] += $quantity; end {emit @sum, \"shape\"}'\n    '@sum[$shape][$color] += $x; end {emit @sum, \"shape\", \"color\"}'\n    '\n      @min = min(@min,$x);\n      @max=max(@max,$x);\n      end{emitf @min, @max}\n    '\n\nSee also https://miller.readthedocs.io/reference-dsl for more context.\n</code></pre>"},{"location":"miller/verbi/#regularize","title":"regularize","text":"<p>Il verbo <code>regularize</code> riordina le righe nello stesso ordine della prima (qualunque sia l'ordine).</p> <p>mlr regularize --help</p> <pre><code>Outputs records sorted lexically ascending by keys.\nOptions:\n</code></pre>"},{"location":"miller/verbi/#remove-empty-columns","title":"remove-empty-columns","text":"<p>mlr remove-empty-columns --help</p> <pre><code>Omits fields which are empty on every input row. Non-streaming.\nOptions:\n</code></pre>"},{"location":"miller/verbi/#rename","title":"rename","text":"<p>mlr rename --help</p> <pre><code>Renames specified fields.\nOptions:\n-r         Treat old field  names as regular expressions. \"ab\", \"a.*b\"\n           will match any field name containing the substring \"ab\" or\n           matching \"a.*b\", respectively; anchors of the form \"^ab$\",\n           \"^a.*b$\" may be used. New field names may be plain strings,\n           or may contain capture groups of the form \"\\1\" through\n           \"\\9\". Wrapping the regex in double quotes is optional, but\n           is required if you wish to follow it with 'i' to indicate\n           case-insensitivity.\n-g         Do global replacement within each field name rather than\n           first-match replacement.\nExamples:\nmlr rename old_name,new_name'\nmlr rename old_name_1,new_name_1,old_name_2,new_name_2'\nmlr rename -r 'Date_[0-9]+,Date,'  Rename all such fields to be \"Date\"\nmlr rename -r '\"Date_[0-9]+\",Date' Same\nmlr rename -r 'Date_([0-9]+).*,\\1' Rename all such fields to be of the form 20151015\nmlr rename -r '\"name\"i,Name'       Rename \"name\", \"Name\", \"NAME\", etc. to \"Name\"\n</code></pre>"},{"location":"miller/verbi/#reorder","title":"reorder","text":"<p>mlr reorder --help</p> <pre><code>Moves specified names to start of record, or end of record.\nOptions:\n-e Put specified field names at record end: default is to put them at record start.\n-f {a,b,c} Field names to reorder.\n-b {x}     Put field names specified with -f before field name specified by {x},\n           if any. If {x} isn't present in a given record, the specified fields\n           will not be moved.\n-a {x}     Put field names specified with -f after field name specified by {x},\n           if any. If {x} isn't present in a given record, the specified fields\n           will not be moved.\n\nExamples:\nmlr reorder    -f a,b sends input record \"d=4,b=2,a=1,c=3\" to \"a=1,b=2,d=4,c=3\".\nmlr reorder -e -f a,b sends input record \"d=4,b=2,a=1,c=3\" to \"d=4,c=3,a=1,b=2\".\n</code></pre>"},{"location":"miller/verbi/#repeat","title":"repeat","text":"<p>mlr repeat --help</p> <pre><code>Copies input records to output records multiple times.\nOptions must be exactly one of the following:\n-n {repeat count}  Repeat each input record this many times.\n-f {field name}    Same, but take the repeat count from the specified\n                   field name of each input record.\nExample:\n  echo x=0 | mlr repeat -n 4 then put '$x=urand()'\nproduces:\n x=0.488189\n x=0.484973\n x=0.704983\n x=0.147311\nExample:\n  echo a=1,b=2,c=3 | mlr repeat -f b\nproduces:\n  a=1,b=2,c=3\n  a=1,b=2,c=3\nExample:\n  echo a=1,b=2,c=3 | mlr repeat -f c\nproduces:\n  a=1,b=2,c=3\n  a=1,b=2,c=3\n  a=1,b=2,c=3\n</code></pre>"},{"location":"miller/verbi/#reshape","title":"reshape","text":"<p>Trasforma lo schema da <code>wide</code> a <code>long</code> e viceversa. Vedi approfondimento.</p> <p>mlr reshape --help</p> <pre><code>Wide-to-long options:\n-i {input field names}   -o {key-field name,value-field name}\n-r {input field regexes} -o {key-field name,value-field name}\nThese pivot/reshape the input data such that the input fields are removed\nand separate records are emitted for each key/value pair.\nNote: this works with tail -f and produces output records for each input\nrecord seen.\nLong-to-wide options:\n-s {key-field name,value-field name}\nThese pivot/reshape the input data to undo the wide-to-long operation.\nNote: this does not work with tail -f; it produces output records only after\nall input records have been read.\n</code></pre> <p>Ad esempio da <code>wide</code></p> Studente Scuola Matematica Italiano Andy Liceo Cannizzaro 7 6 Lisa Liceo Garibaldi 6 7 Giovanna Liceo Garibaldi 7 7 <p>a <code>long</code></p> Studente Scuola materia voto Andy Liceo Cannizzaro Matematica 7 Andy Liceo Cannizzaro Italiano 6 Lisa Liceo Garibaldi Matematica 6 Lisa Liceo Garibaldi Italiano 7 Giovanna Liceo Garibaldi Matematica 7 Giovanna Liceo Garibaldi Italiano 7"},{"location":"miller/verbi/#sample","title":"sample","text":"<p>mlr sample --help</p> <pre><code>Reservoir sampling (subsampling without replacement), optionally by category.\nSee also mlr bootstrap and mlr shuffle.\nOptions:\n-g {a,b,c} Optional: group-by-field names for samples, e.g. a,b,c.\n-k {k} Required: number of records to output in total, or by group if using -g.\n</code></pre>"},{"location":"miller/verbi/#sec2gmtdate","title":"sec2gmtdate","text":"<p>mlr sec2gmtdate --help</p> <pre><code>Replaces a numeric field representing seconds since the epoch with the\ncorresponding GMT year-month-day timestamp; leaves non-numbers as-is.\nThis is nothing more than a keystroke-saver for the sec2gmtdate function:\n  ../c/mlr sec2gmtdate time1,time2\nis the same as\n  ../c/mlr put '$time1=sec2gmtdate($time1);$time2=sec2gmtdate($time2)'\n</code></pre>"},{"location":"miller/verbi/#sec2gmt","title":"sec2gmt","text":"<p>mlr sec2gmt --help</p> <pre><code>Replaces a numeric field representing seconds since the epoch with the\ncorresponding GMT timestamp; leaves non-numbers as-is. This is nothing\nmore than a keystroke-saver for the sec2gmt function:\n  mlr sec2gmt time1,time2\nis the same as\n  mlr put '$time1 = sec2gmt($time1); $time2 = sec2gmt($time2)'\nOptions:\n-1 through -9: format the seconds using 1..9 decimal places, respectively.\n--millis Input numbers are treated as milliseconds since the epoch.\n--micros Input numbers are treated as microseconds since the epoch.\n--nanos  Input numbers are treated as nanoseconds since the epoch.\n</code></pre>"},{"location":"miller/verbi/#seqgen","title":"seqgen","text":"<p>mlr seqgen --help</p> <pre><code>Passes input records directly to output. Most useful for format conversion.\nProduces a sequence of counters.  Discards the input record stream. Produces\noutput as specified by the options\n\nOptions:\n-f {name} (default \"i\") Field name for counters.\n--start {value} (default 1) Inclusive start value.\n--step {value} (default 1) Step value.\n--stop {value} (default 100) Inclusive stop value.\nStart, stop, and/or step may be floating-point. Output is integer if start,\nstop, and step are all integers. Step may be negative. It may not be zero\nunless start == stop.\n</code></pre>"},{"location":"miller/verbi/#shuffle","title":"shuffle","text":"<p>mlr shuffle --help</p> <pre><code>Outputs records randomly permuted. No output records are produced until\nall input records are read. See also mlr bootstrap and mlr sample.\nOptions:\n</code></pre>"},{"location":"miller/verbi/#skip-trivial-records","title":"skip-trivial-records","text":"<p>mlr skip-trivial-records --help</p> <pre><code>Passes through all records except those with zero fields,\nor those for which all fields have empty value.\nOptions:\n</code></pre>"},{"location":"miller/verbi/#sort","title":"sort","text":"<p>mlr sort --help</p> <pre><code>Sorts records primarily by the first specified field, secondarily by the second\nfield, and so on.  (Any records not having all specified sort keys will appear\nat the end of the output, in the order they were encountered, regardless of the\nspecified sort order.) The sort is stable: records that compare equal will sort\nin the order they were encountered in the input record stream.\n\nOptions:\n-f  {comma-separated field names}  Lexical ascending\n-r  {comma-separated field names}  Lexical descending\n-c  {comma-separated field names}  Case-folded lexical ascending\n-cr {comma-separated field names}  Case-folded lexical descending\n-n  {comma-separated field names}  Numerical ascending; nulls sort last\n-nf {comma-separated field names}  Same as -n\n-nr {comma-separated field names}  Numerical descending; nulls sort first\n-t  {comma-separated field names}  Natural ascending\n-tr {comma-separated field names}  Natural descending\n-h|--help Show this message.\n\nExample:\n  mlr sort -f a,b -nr x,y,z\nwhich is the same as:\n  mlr sort -f a -f b -nr x -nr y -nr z\n</code></pre>"},{"location":"miller/verbi/#sort-within-records","title":"sort-within-records","text":"<p>Riordina i campi in ordine lessicamente crescente per nome campo.</p> <p>mlr sort-within-records --help</p> <pre><code>Outputs records sorted lexically ascending by keys.\nOptions:\n-r        Recursively sort subobjects/submaps, e.g. for JSON input.\n</code></pre> <p>Ad esempio a questo file, in cui i campi hanno ordine diverso</p> eterogeneita_irregular.json<pre><code>{ \"a\": 1, \"b\": 2, \"c\": 3 }\n{ \"c\": 6, \"a\": 4, \"b\": 5 }\n{ \"b\": 8, \"c\": 9, \"a\": 7 }\n</code></pre> <p>si potr\u00e0 applicare questo comando per ordinare i campi secondo l'ordine alfabetico dei loro nomi</p> <p>mlr --json sort-within-records ./eterogeneita_irregular.json</p> <pre><code>{ \"a\": 1, \"b\": 2, \"c\": 3 }\n{ \"a\": 4, \"b\": 5, \"c\": 6 }\n{ \"a\": 7, \"b\": 8, \"c\": 9 }\n</code></pre>"},{"location":"miller/verbi/#stats1","title":"stats1","text":"<p>mlr stats1 --help</p> <pre><code>Computes univariate statistics for one or more given fields, accumulated across\nthe input record stream.\nOptions:\n-a {sum,count,...} Names of accumulators: one or more of:\n  median   This is the same as p50\n  p10 p25.2 p50 p98 p100 etc.\n  count    Count instances of fields\n  mode     Find most-frequently-occurring values for fields; first-found wins tie\n  antimode Find least-frequently-occurring values for fields; first-found wins tie\n  sum      Compute sums of specified fields\n  mean     Compute averages (sample means) of specified fields\n  var      Compute sample variance of specified fields\n  stddev   Compute sample standard deviation of specified fields\n  meaneb   Estimate error bars for averages (assuming no sample autocorrelation)\n  skewness Compute sample skewness of specified fields\n  kurtosis Compute sample kurtosis of specified fields\n  min      Compute minimum values of specified fields\n  max      Compute maximum values of specified fields\n\n-f {a,b,c}     Value-field names on which to compute statistics\n--fr {regex}   Regex for value-field names on which to compute statistics\n               (compute statistics on values in all field names matching regex\n--fx {regex}   Inverted regex for value-field names on which to compute statistics\n               (compute statistics on values in all field names not matching regex)\n\n-g {d,e,f}     Optional group-by-field names\n--gr {regex}   Regex for optional group-by-field names\n               (group by values in field names matching regex)\n--gx {regex}   Inverted regex for optional group-by-field names\n               (group by values in field names not matching regex)\n\n--grfx {regex} Shorthand for --gr {regex} --fx {that same regex}\n\n-i             Use interpolated percentiles, like R's type=7; default like type=1.\n               Not sensical for string-valued fields.\\n\");\n-s             Print iterative stats. Useful in tail -f contexts (in which\n               case please avoid pprint-format output since end of input\n               stream will never be seen).\nExample: mlr stats1 -a min,p10,p50,p90,max -f value -g size,shape\nExample: mlr stats1 -a count,mode -f size\nExample: mlr stats1 -a count,mode -f size -g shape\nExample: mlr stats1 -a count,mode --fr '^[a-h].*$' -gr '^k.*$'\n        This computes count and mode statistics on all field names beginning\n         with a through h, grouped by all field names starting with k.\n\nNotes:\n* p50 and median are synonymous.\n* min and max output the same results as p0 and p100, respectively, but use\n  less memory.\n* String-valued data make sense unless arithmetic on them is required,\n  e.g. for sum, mean, interpolated percentiles, etc. In case of mixed data,\n  numbers are less than strings.\n* count and mode allow text input; the rest require numeric input.\n  In particular, 1 and 1.0 are distinct text for count and mode.\n* When there are mode ties, the first-encountered datum wins.\n</code></pre>"},{"location":"miller/verbi/#stats2","title":"stats2","text":"<p>mlr stats2 --help</p> <pre><code>Computes bivariate statistics for one or more given field-name pairs,\naccumulated across the input record stream.\n-a {linreg-ols,corr,...}  Names of accumulators: one or more of:\n  linreg-ols Linear regression using ordinary least squares\n  linreg-pca Linear regression using principal component analysis\n  r2       Quality metric for linreg-ols (linreg-pca emits its own)\n  logireg  Logistic regression\n  corr     Sample correlation\n  cov      Sample covariance\n  covx     Sample-covariance matrix\n-f {a,b,c,d}   Value-field name-pairs on which to compute statistics.\n               There must be an even number of names.\n-g {e,f,g}     Optional group-by-field names.\n-v             Print additional output for linreg-pca.\n-s             Print iterative stats. Useful in tail -f contexts (in which\n               case please avoid pprint-format output since end of input\n               stream will never be seen).\n--fit          Rather than printing regression parameters, applies them to\n               the input data to compute new fit fields. All input records are\n               held in memory until end of input stream. Has effect only for\n               linreg-ols, linreg-pca, and logireg.\nOnly one of -s or --fit may be used.\nExample: mlr stats2 -a linreg-pca -f x,y\nExample: mlr stats2 -a linreg-ols,r2 -f x,y -g size,shape\nExample: mlr stats2 -a corr -f x,y\n</code></pre>"},{"location":"miller/verbi/#step","title":"step","text":"<p>mlr step --help</p> <pre><code>Computes values dependent on the previous record, optionally grouped by category.\nOptions:\n-a {delta,rsum,...}   Names of steppers: comma-separated, one or more of:\n  delta    Compute differences in field(s) between successive records\n  shift    Include value(s) in field(s) from previous record, if any\n  from-first Compute differences in field(s) from first record\n  ratio    Compute ratios in field(s) between successive records\n  rsum     Compute running sums of field(s) between successive records\n  counter  Count instances of field(s) between successive records\n  ewma     Exponentially weighted moving average over successive records\n\n-f {a,b,c} Value-field names on which to compute statistics\n-g {d,e,f} Optional group-by-field names\n-F         Computes integerable things (e.g. counter) in floating point.\n           As of Miller 6 this happens automatically, but the flag is accepted\n           as a no-op for backward compatibility with Miller 5 and below.\n-d {x,y,z} Weights for ewma. 1 means current sample gets all weight (no\n           smoothing), near under under 1 is light smoothing, near over 0 is\n           heavy smoothing. Multiple weights may be specified, e.g.\n           \"mlr step -a ewma -f sys_load -d 0.01,0.1,0.9\". Default if omitted\n           is \"-d 0.5\".\n-o {a,b,c} Custom suffixes for EWMA output fields. If omitted, these default to\n           the -d values. If supplied, the number of -o values must be the same\n           as the number of -d values.\n\nExamples:\n  mlr step -a rsum -f request_size\n  mlr step -a delta -f request_size -g hostname\n  mlr step -a ewma -d 0.1,0.9 -f x,y\n  mlr step -a ewma -d 0.1,0.9 -o smooth,rough -f x,y\n  mlr step -a ewma -d 0.1,0.9 -o smooth,rough -f x,y -g group_name\n\nPlease see https://miller.readthedocs.io/en/latest/reference-verbs.html#filter or\nhttps://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average\nfor more information on EWMA.\n</code></pre>"},{"location":"miller/verbi/#tac","title":"tac","text":"<p>mlr tac --help</p> <pre><code>Prints records in reverse order from the order in which they were encountered.\nOptions:\n</code></pre>"},{"location":"miller/verbi/#tail","title":"tail","text":"<p>mlr tail --help</p> <pre><code>Passes through the last n records, optionally by category.\nOptions:\n-g {a,b,c} Optional group-by-field names for head counts, e.g. a,b,c.\n-n {n} Head-count to print. Default 10.\n</code></pre>"},{"location":"miller/verbi/#tee","title":"tee","text":"<p>mlr tee --help</p> <pre><code>Options:\n-a    Append to existing file, if any, rather than overwriting.\n-p    Treat filename as a pipe-to command.\nAny of the output-format command-line flags (see mlr -h). Example: using\n  mlr --icsv --opprint put '...' then tee --ojson ./mytap.dat then stats1 ...\nthe input is CSV, the output is pretty-print tabular, but the tee-file output\nis written in JSON format.\n</code></pre>"},{"location":"miller/verbi/#template","title":"template","text":"<p>mlr template --help</p> <pre><code>Places input-record fields in the order specified by list of column names.\nIf the input record is missing a specified field, it will be filled with the fill-with.\nIf the input record possesses an unspecified field, it will be discarded.\nOptions:\n -f {a,b,c} Comma-separated field names for template, e.g. a,b,c.\n -t {filename} CSV file whose header line will be used for template.\n--fill-with {filler string}  What to fill absent fields with. Defaults to the empty string.\nExample:\n* Specified fields are a,b,c.\n* Input record is c=3,a=1,f=6.\n* Output record is a=1,b=,c=3.\n</code></pre>"},{"location":"miller/verbi/#top","title":"top","text":"<p>Restituisce i record con i valori pi\u00f9 grandi (o pi\u00f9 piccoli), per uno o pi\u00f9 campi, anche raggruppando per campi.</p> <p>mlr top --help</p> <pre><code>-f {a,b,c}    Value-field names for top counts.\n-g {d,e,f}    Optional group-by-field names for top counts.\n-n {count}    How many records to print per category; default 1.\n-a            Print all fields for top-value records; default is\n              to print only value and group-by fields. Requires a single\n              value-field name only.\n--min         Print top smallest values; default is top largest values.\n-F            Keep top values as floats even if they look like integers.\n-o {name}     Field name for output indices. Default \"top_idx\".\nPrints the n records with smallest/largest values at specified fields,\noptionally by category.\n</code></pre> <p>Qui un file (senza intestazione, nel comando di sotto si usa infatti il flag <code>-N</code>), in cui i primi 4 campi sono a volte duplicati in pi\u00f9 record.</p> input.csv<pre><code>1,861265,C,A,0.071\n1,861265,C,A,0.148\n1,861265,C,G,0.001\n1,861265,C,G,0.108\n1,861265,C,T,0\n1,861265,C,T,0.216\n2,193456,G,A,0.006\n2,193456,G,A,0.094\n2,193456,G,C,0.011\n2,193456,G,C,0.152\n2,193456,G,T,0.003\n2,193456,G,T,0.056\n</code></pre> <p>Se se si vogliono estrarre le righe con il valore massimo del campo <code>5</code>, a parit\u00e0 di valori dei campi <code>1</code>,<code>2</code>,<code>3</code>,<code>4</code>, il comando sar\u00e0:</p> <p>mlr --csv -N top -f 5  -g 1,2,3,4 input.tsv</p> <pre><code>1,861265,C,A,1,0.148\n1,861265,C,G,1,0.108\n1,861265,C,T,1,0.216\n2,193456,G,A,1,0.094\n2,193456,G,C,1,0.152\n2,193456,G,T,1,0.056\n</code></pre> <p>In output soltanto i campi definiti nel comando. Se si vogliono tutti, bisogna aggiungere <code>-a</code></p>"},{"location":"miller/verbi/#unflatten","title":"unflatten","text":"<p>mlr unflatten --help</p> <pre><code>Reverses flatten. Example: field with name 'a.b.c' and value 4\nbecomes name 'a' and value '{\"b\": { \"c\": 4 }}'.\nOptions:\n-f {a,b,c} Comma-separated list of field names to unflatten (default all).\n-s {string} Separator, defaulting to mlr --flatsep value.\n</code></pre>"},{"location":"miller/verbi/#uniq","title":"uniq","text":"<p>mlr uniq --help</p> <pre><code>Prints distinct values for specified field names. With -c, same as\ncount-distinct. For uniq, -f is a synonym for -g.\n\nOptions:\n-g {d,e,f}    Group-by-field names for uniq counts.\n-c            Show repeat counts in addition to unique values.\n-n            Show only the number of distinct values.\n-o {name}     Field name for output count. Default \"count\".\n-a            Output each unique record only once. Incompatible with -g.\n              With -c, produces unique records, with repeat counts for each.\n              With -n, produces only one record which is the unique-record count.\n              With neither -c nor -n, produces unique records.\n</code></pre>"},{"location":"miller/verbi/#unsparsify","title":"unsparsify","text":"<p>mlr unsparsify --help</p> <pre><code>Usage: mlr unsparsify [options]\nPrints records with the union of field names over all input records.\nFor field names absent in a given record but present in others, fills in\na value. This verb retains all input before producing any output.\nOptions:\n--fill-with {filler string}  What to fill absent fields with. Defaults to\n                            the empty string.\n-f {a,b,c} Specify field names to be operated on. Any other fields won't be\n          modified, and operation will be streaming.\n-h|--help  Show this message.\nExample: if the input is two records, one being 'a=1,b=2' and the other\nbeing 'b=3,c=4', then the output is the two records 'a=1,b=2,c=' and\n'a=,b=3,c=4'.\n</code></pre> <p>Alcuni formati, come il <code>JSON</code>, non devono avere lo stesso numero di campi per record. Un esempio il file a seguire:</p> input.json<pre><code>[\n  {\n    \"nome\": \"sara\",\n    \"dataNascita\": \"2000-02-22\",\n    \"altezza\": 166,\n    \"peso\": 70.4,\n    \"comuneNascita\": \"Roma\"\n  },\n  {\n    \"nome\": \"giulia\",\n    \"comuneNascita\": \"Milano\"\n  }\n]\n</code></pre> <p>Con <code>unsparsify</code>, viene di default prodotto un output in cui tutti i record hanno gli stessi campi, e l'input viene fatto diventare rettangolare (vedi eterogeneit\u00e0 dei record); laddove erano assenti viene assegnato un valore nullo.</p> <p>mlr --json unsparsify input.json</p> <pre><code>[\n  {\n    \"nome\": \"sara\",\n    \"dataNascita\": \"2000-02-22\",\n    \"altezza\": 166,\n    \"peso\": 70.4,\n    \"comuneNascita\": \"Roma\"\n  },\n  {\n    \"nome\": \"giulia\",\n    \"dataNascita\": \"\",\n    \"altezza\": \"\",\n    \"peso\": \"\",\n    \"comuneNascita\": \"Milano\"\n  }\n]\n</code></pre>"},{"location":"nushell/","title":"Nushell","text":"<p>Attenzione</p> <p>La sezione per Nushell non \u00e8 ancora pronta. \u00c8 possibile fare riferimento al sito ufficiale.</p> <p>Nushell \u00e8 una shell che vede un po' tutto in forma di dataset, tabelle, liste, ecc..</p> <p>Se si lancia ad esempio <code>ls</code>, il tipico comando per avere la lista dei contenuti di una directory, si ha indietro questa rappresentazione tabellare:</p> name type size modified index.md file 184 B Mon, 24 Oct 2022 18:31:43 +0200 (2 weeks ago) ricette.md file 3.0 KiB Sun, 30 Oct 2022 17:41:53 +0100 (5 days ago) <p>E si possono applicare comandi tipici del mondo dei dati. Ad esempio per avere in lista soltanto i file con dimensioni maggiori di 2kB, il comando sar\u00e0:</p> <pre><code>ls | where size &gt; 2kB\n</code></pre> <p>Alcune ricette.</p>"},{"location":"nushell/ricette/","title":"Ricette Nushell","text":""},{"location":"nushell/ricette/#generali","title":"Generali","text":""},{"location":"nushell/ricette/#lanciare-comandi-nushell-da-unaltra-shell","title":"Lanciare comandi Nushell da un'altra shell","text":"<p>Si usa l'eseguibile <code>nu</code> e il flag <code>-c</code> seguito dal comando Nushell che si vuole lanciare. Ad esempio:</p> <pre><code>nu -c 'ls | where size &gt; 16kB | to json'\n</code></pre>"},{"location":"nushell/ricette/#lanciare-uno-script-nushell-da-unaltra-shell","title":"Lanciare uno script Nushell da un'altra shell","text":"<pre><code>$ cat myscript.nu\nls | pivot\n\n$ nu myscript.nu\n</code></pre>"},{"location":"nushell/ricette/#dataframe","title":"dataframe","text":""},{"location":"nushell/ricette/#creare-un-dataframe-a-partire-da-un-csv","title":"Creare un dataframe a partire da un CSV","text":"<pre><code>let df = open-df input.csv\n</code></pre>"},{"location":"nushell/ricette/#descrivere-un-dataframe","title":"Descrivere un dataframe","text":"<pre><code>$df | describe\n</code></pre>"},{"location":"nushell/ricette/#filtrare-un-dataframe-per-il-valore-di-una-colonna","title":"Filtrare un dataframe per il valore di una colonna","text":"<pre><code>$df | filter ((col ANNO) &gt; 2001)\n</code></pre>"},{"location":"nushell/ricette/#avere-numero-di-righe-e-colonne","title":"Avere numero di righe e colonne","text":"<pre><code>$df | shape\n</code></pre>"},{"location":"nushell/ricette/#conteggiare-il-numero-di-record-per-valori-distinti-di-un-campo","title":"Conteggiare il numero di record per valori distinti di un campo","text":"<p>Qui sotto ad esempio, il conteggio di record, per valori distinti della colonna <code>REGMCR</code>. Il conteggio \u00e8 fatto sulla colonna <code>ANNO</code>, ma poteva essere fatto su qualunque altra.</p> <pre><code>$df | group-by REGMCR  | agg [(col ANNO | count  ) ]\n</code></pre> <p>In output:</p> index REGMCR ANNO 0 4 25238 1 20 15322 2 14 6728 3 17 14627 4 15 39949 5 18 14305 6 11 14443 7 3 60338 8 6 17455 9 7 15086 10 16 24787 11 10 12339 12 1 52535 13 19 47234 14 5 24254 15 13 11706 16 8 35748 17 9 29619 18 12 36093"},{"location":"nushell/ricette/#combinare-conteggi-aggregati-per-la-stessa-colonna","title":"Combinare conteggi aggregati per la stessa colonna","text":"<p>Qui sotto ad esempio, per valori distinti di <code>REGMCR</code>, si conteggiano per la colonna <code>ANNO</code> il numero totale di record e il numero di valori distinti presenti.</p> <pre><code>$df | group-by REGMCR  | agg [(col ANNO | count | as ANNO_count  ) (col ANNO | n-unique | as ANNO_unique)] | into nu\n</code></pre> <p>Nota</p> <p>In questo caso \u00e8 necessario rinominare il campo <code>ANNO</code> usando il comando <code>as</code>, perch\u00e9 i due conteggi si fanno sulla stessa colonna e si avrebbe due volte il nome campo <code>ANNO</code>; e la cosa non \u00e8 consentita.</p> <p>In output</p> index REGMCR ANNO_count ANNO_unique 0 8 35748 1 1 16 24787 1 2 7 15086 1 3 15 39949 1 4 14 6728 1 5 6 17455 1 6 13 11706 1 7 5 24254 1 8 12 36093 1 9 4 25238 1 10 20 15322 1 11 11 14443 1 12 3 60338 1 13 19 47234 1 14 18 14305 1 15 10 12339 1 16 17 14627 1 17 1 52535 1 18 9 29619 1"},{"location":"nushell/ricette/#avere-louput-di-un-comando-in-stdout","title":"Avere l'ouput di un comando in stdout","text":"<p>Bisogna prima trasformare il dataframe in una tabella <code>nu</code>, con il comando <code>into nu</code>.</p> <p>Ad esempio</p> <pre><code>$df | describe | into nu | to csv\n</code></pre>"},{"location":"nushell/ricette/#utilizzare-comandi-nushell-non-disponibili-per-i-dataframe","title":"Utilizzare comandi Nushell non disponibili per i dataframe","text":"<p>Non tutti i comandi Nushell sono applicabili al tipo di dato <code>dataframe</code>. Uno di questi \u00e8 ad esempio il comando <code>transpose</code>. Per applicarlo ad esempio ad una pipeline di comandi che deriva da un <code>dataframe</code>, bisogna convertire il tutto in una tabella <code>nu</code>:</p> <pre><code>$df | describe | into nu | transpose | to csv | save foo.sv\n</code></pre> <p>Utile leggere questa discussione.</p>"},{"location":"nushell/ricette/#applicare-una-query-sql-a-un-dataframe","title":"Applicare una query SQL a un dataframe","text":"<pre><code>[[a, b, c];[1,2,3] [2,3,4] ]| into df | query df 'select * from df where a &gt; 0 OR c &gt; 2'\n</code></pre> <p><code>df</code> \u00e8 il nome di default assegnato al dataframe.</p>"},{"location":"nushell/ricette/#applicare-un-filtro-regex-tramite-una-maschera","title":"Applicare un filtro regex tramite una maschera","text":"<p>Nell'esempio di sotto viene impostata una maschera tramite regex: quando il campo <code>CODICE_LOCALE_PROGETTO</code> contiene la stringa <code>PNRR</code>. Fissatala maschera poi si applica il comando <code>filter-with</code>.</p> <pre><code>let df = open-df  -d \";\" --infer-schema 10000000 OpenCup_Progetti.csv\nlet mask = ($df.CODICE_LOCALE_PROGETTO =~ \"PNRR\")\n$df | filter-with $mask\n</code></pre> <p>Nota</p> <p>Nell'apertura del dataframe viene usato il comando <code>--infer-schema 10000000</code>, perch\u00e9 si tratta di un file enorme, in cui nelle ultime righe di un campo che sembrava intero ci sono delle stringhe, quindi l'inferencing deve essere pi\u00f9 profondo.</p>"},{"location":"nushell/ricette/#scripting","title":"Scripting","text":""},{"location":"nushell/ricette/#aprire-tutti-i-file-con-una-certa-estensione-in-una-cartella-estrarre-valori-e-salvarli","title":"Aprire tutti i file con una certa estensione in una cartella, estrarre valori e salvarli","text":"<pre><code>ls *.csv | each { |it| open $it.name --raw | from csv --separator \";\" | get Period } | flatten\n</code></pre> <ul> <li>si filtrano tutti i file con estensione <code>csv</code></li> <li>si apre ognuno <code>|it| open $it.name --raw | from csv --separator \";\"</code></li> <li>di ognuno si estra la colonna <code>Period</code></li> <li>si \"flattenizza\" il risultato, perch\u00e9 l'output grezzo \u00e8 una lista.</li> </ul> <p>Nota</p> <p>Si apre il file con opzione <code>--raw</code> (grezza), in modo che non vengano fatte intepretazioni a partire dall'estensione del file, quindi ad esempio dare per scontato che il separatore di campo \u00e8 la <code>,</code>.</p>"},{"location":"qsv/","title":"qsv","text":"<p>qsv \u00e8 un toolkit ultra-veloce per la manipolazione di dati strutturati, con oltre 50 comandi disponibili. \u00c8 costruito in Rust e utilizza Polars per offrire prestazioni eccezionali nell'elaborazione di file CSV e altri formati di dati.</p> <p>Ha una ottima documentazione ufficiale in inglese, consultabile qui.</p>"},{"location":"qsv/#caratteristiche-principali","title":"Caratteristiche principali","text":"<ul> <li>Velocit\u00e0: grazie a Rust e Polars, qsv \u00e8 estremamente veloce anche con dataset di grandi dimensioni</li> <li>Parallelizzazione: sfrutta al meglio le CPU multi-core</li> <li>Streaming: la maggior parte dei comandi usa memoria costante e pu\u00f2 processare file CSV di dimensioni arbitrarie</li> <li>Formati supportati: CSV, TSV, Excel, Open Document Spreadsheet, JSON/JSONL, DataPackage, Apache Arrow/Parquet, SQLite, PostgreSQL</li> </ul>"},{"location":"qsv/#installazione","title":"Installazione","text":"<p>qsv pu\u00f2 essere installato in diversi modi:</p> <ul> <li>utilizzando i file binari precompilati, presenti nella pagina delle release</li> <li>compilandolo da sorgente (guida dettagliata sotto)</li> <li>utilizzando un gestore di pacchetti (vedi documentazione ufficiale)</li> </ul>"},{"location":"qsv/#risorse","title":"Risorse","text":"<ul> <li>Sito ufficiale: qsv.dathere.com</li> <li>Repository GitHub: github.com/dathere/qsv</li> <li>Tutorial con 100 esercizi: 100.dathere.com</li> <li>qsv pro: versione desktop con interfaccia grafica, disponibile su qsvpro.dathere.com</li> </ul>"},{"location":"qsv/#compilazione-da-sorgente","title":"Compilazione da sorgente","text":"<p>Questa guida spiega passo passo come compilare qsv da sorgente usando una configurazione che riduce il consumo di memoria ed evita gli errori OOM durante la compilazione di Polars.</p>"},{"location":"qsv/#1-clonare-il-repository-qsv","title":"1. Clonare il repository qsv","text":"<p>Scegli dove posizionare la cartella del progetto, ad esempio nella tua home:</p> <pre><code>cd ~\ngit clone https://github.com/dathere/qsv.git\ncd qsv\n</code></pre> <p>Questo scarica il codice sorgente e ti porta dentro la cartella del progetto.</p>"},{"location":"qsv/#2-assicurarsi-di-usare-il-tag-corretto-opzionale-ma-consigliato","title":"2. Assicurarsi di usare il tag corretto (opzionale ma consigliato)","text":"<p>Per essere certo di compilare una versione stabile:</p> <pre><code>git checkout 11.0.2\n</code></pre>"},{"location":"qsv/#3-compilare-qsv-con-impostazioni-a-bassa-memoria","title":"3. Compilare qsv con impostazioni a bassa memoria","text":"<p>La compilazione di qsv richiede molta RAM a causa delle dipendenze Polars. Per evitare che il kernel uccida <code>rustc</code>, si usa:</p> <ul> <li>CARGO_BUILD_JOBS=1 \u2192 compila un solo crate alla volta</li> <li>-C target-cpu=native \u2192 ottimizza il binario per la tua CPU</li> <li>--locked \u2192 usa esattamente le versioni del lockfile</li> <li>-F all_features \u2192 abilita tutte le funzionalit\u00e0</li> </ul> <p>Comando completo:</p> <pre><code>CARGO_BUILD_RUSTFLAGS='-C target-cpu=native' CARGO_BUILD_JOBS=1 \\\ncargo build --release --locked -F all_features\n</code></pre> <p>La compilazione pu\u00f2 richiedere un po' di tempo, ma con questa configurazione funziona anche su macchine con RAM limitata.</p>"},{"location":"qsv/#4-trovare-il-binario-compilato","title":"4. Trovare il binario compilato","text":"<p>A fine compilazione, il binario si trova qui:</p> <pre><code>./target/release/qsv\n</code></pre> <p>Puoi provarlo con:</p> <pre><code>./target/release/qsv --version\n</code></pre> <p>Se vuoi renderlo disponibile nel tuo PATH:</p> <pre><code>mkdir -p ~/.local/bin\ncp target/release/qsv ~/.local/bin/\n</code></pre>"},{"location":"qsv/#5-pulizia-opzionale-dei-file-generati-da-cargo","title":"5. Pulizia opzionale dei file generati da cargo","text":"<p>Se vuoi liberare spazio dopo la compilazione:</p> <pre><code>rm -rf ~/.cargo/registry ~/.cargo/git /tmp/cargo-*\n</code></pre> <p>e per pulire la build del progetto:</p> <pre><code>rm -rf target\n</code></pre> <p>Questi file verranno ricreati automaticamente da cargo quando serviranno.</p>"},{"location":"qsv/ricette/","title":"Ricette","text":""},{"location":"qsv/ricette/#fare-il-reverse-geocoding","title":"Fare il reverse geocoding","text":"<p>File di input</p> <pre><code>citta\nPalermo\nRoma\nCagliari\nBari\n</code></pre> <p>Comando</p> <pre><code>qsv fetch --report d --jql '[0].\"lat\",[0].\"lon\"' input.csv -c output --url-template \"https://nominatim.openstreetmap.org/search?q={citta}&amp;country=Italy&amp;format=json\"\n</code></pre> <ul> <li><code>--report d</code>, per creare un file di report dell'operazione di fetch;</li> <li><code>--jql '[0].\"lat\",[0].\"lon\"'</code>, per estrarre i campi del JSON dall'output dell'API;</li> <li><code>input.csv</code>, il file di input;</li> <li><code>-c output</code>, la nuova colonna dove inserire i valori estratti da <code>jql</code>;</li> <li><code>--url-template \"https://nominatim.openstreetmap.org/search?q={citta}&amp;country=Italy&amp;format=json\"</code>, l'URL delle API, con i nomi delle colonne come variabili (qui \u00e8 la sola <code>citt\u00e0</code>).</li> </ul>"},{"location":"ricette/","title":"Ricette","text":"<p>In questa sezione, una raccolta di ricette che andr\u00e0 crescendo nel tempo.</p>"},{"location":"ricette/paste-unire-due-file-per-colonne/","title":"Come unire colonne di testo da file separati","text":"","tags":["paste","qsv","unire","colonne"]},{"location":"ricette/paste-unire-due-file-per-colonne/#usando-paste","title":"Usando paste","text":"<p>Immaginiamo di avere questi due file e di volerli unire in orizzontale, per avere un unico file composto da tre colonne (in questo caso i due file di input sono dei <code>CSV</code>).</p> input_01<pre><code>a,1\nd,7\nc,2\n</code></pre> input_01<pre><code>2022\n2024\n1985\n</code></pre> <p>Esiste un'utility standard di Linux, <code>paste</code>, che si utilizza proprio per casi di questo tipo.</p> <p>Il comando da lanciare \u00e8</p> <pre><code>paste -d \",\" input_01 input_02 &gt;output\n</code></pre> <p>che restituir\u00e0 in output</p> output<pre><code>a,1,2022\nd,7,2024\nc,2,1985\n</code></pre> <p>Con <code>-d \",\"</code> si restituisce a <code>paste</code> il fatto che il separatore di colonne del file di input \u00e8 la <code>,</code>.</p> <p>Warning</p> <p><code>paste</code> non \u00e8 <code>format aware</code></p>","tags":["paste","qsv","unire","colonne"]},{"location":"ricette/paste-unire-due-file-per-colonne/#usando-qsv","title":"Usando qsv","text":"<p>A partire dagli stessi file di sopra, il comando per <code>qsv</code> da usare \u00e8 <code>cat</code>:</p> <pre><code>qsv cat columns input_01 input_02 &gt;output\n</code></pre> <p>Note</p> <p><code>qsv</code> \u00e8 <code>format aware</code></p>","tags":["paste","qsv","unire","colonne"]},{"location":"ricette/salvare-paginaweb-completa-unicofile/","title":"Salvare una pagina web in unico file autoconsitente","text":"","tags":["html","scraping"]},{"location":"ricette/salvare-paginaweb-completa-unicofile/#monolith","title":"Monolith","text":"<p>Uno strumento ottimo per farlo \u00e8 monolith.</p> <p>Un comando tipo \u00e8 questo:</p> <pre><code>monolith https://lyrics.github.io/db/P/Portishead/Dummy/Roads/ -o portishead-roads-lyrics.html\n</code></pre> <p>In output un solo file, che contiene al suo interno i CSS, i javascript, le immagini, i font, ecc., autoconsistente.</p>","tags":["html","scraping"]},{"location":"ricette/salvare-paginaweb-completa-unicofile/#se-la-pagina-ha-contenuti-js-driven-ajax","title":"Se la pagina ha contenuti JS-driven (AJAX)","text":"<p>In questi casi, la pagina sorgente non ha i contenuti al suo interno. Si pu\u00f2 usare la versione headless di un browser per scaricare la pagina con tutti i contenuti e poi passare l'output a monolith:</p> <pre><code>chromium --headless --disable-gpu --dump-dom \"https://www.agenas.gov.it/covid19/web/index.php?r=site%2Ftab2\" | monolith - -b \"https://www.agenas.gov.it/covid19/web/index.php?r=site%2Ftab2\" -o output.html\n</code></pre> <p>In ambiente debian l'eseguibile di <code>chromium</code> si chiama <code>google-chrome-stable</code>.</p>","tags":["html","scraping"]},{"location":"ricette/split-suddividere-file-parti/","title":"Suddividere un file in parti","text":"<p>Con file di testo, specialmente quelli in cui ogni linea contiene un'informazione auto consistente (come il JSON Lines), pu\u00f2 essere utile suddividerli in pi\u00f9 file.</p> <p>Ad esempio, a partire da un file <code>500.000</code> righe, suddividerlo in blocchi di file da <code>50.000</code>.</p> <p>Un'utility per farlo \u00e8 <code>split</code>. La sintassi per l'esempio di sopra \u00e8:</p> <pre><code>split -l 50000 -d input.csv input_ --additional-suffix=.csv\n</code></pre> <p>Informazioni</p> <ul> <li><code>-l</code> per impostare il valore nel numero di righe;</li> <li><code>-d</code> per avere nel nome dei file di output un suffisso numerico progressivo;</li> <li><code>input.csv</code> \u00e8 il nome del file di input;</li> <li><code>input_</code> per impostare il prefisso del nome del file di output;</li> <li><code>--additional-suffix=.csv</code> per aggiungere al nome dei file di output <code>.csv</code>.</li> </ul> <p>In output si avranno i file <code>input_00.csv</code>, <code>input_01.csv</code>, ecc..</p> <p>\u00c8 possibile dividere il file anche in un determinato numero di parti e per tagli dimensionali in byte.</p> <p>Warning</p> <p><code>split</code> non \u00e8 <code>format aware</code></p>","tags":["split"]},{"location":"ricette/tor-forzare-chiamata-italia/","title":"Come forzare la chiamata tor da un determinato paese","text":"<p>In alcuni casi \u00e8 impossibile raggiungere alcuni siti da alcuni paesi. Facciamo l'ipotesi che un sito sia soltanto raggiungibile da IP italiani.</p> <p><code>tor</code> si installa con <code>sudo apt install tor</code>.</p> <p>Una volta installato bisogna modificare il file di configurazione <code>/etc/tor/torrc</code> aggiungere dei parametri di configurazione, e riavviare il servizio.</p> <pre><code>sudo sh -c \"echo 'SOCKSPort 9050 # Default' &gt;&gt; /etc/tor/torrc\"\nsudo sh -c \"echo 'ExitNodes {it} StrictNodes 1' &gt;&gt; /etc/tor/torrc\"\nsudo systemctl start tor\nsudo service tor stop\nsudo service tor start\n</code></pre> <p>A cose fatte si potr\u00e0 lanciare la chiamata, e l'IP assegnato sar\u00e0 \"italiano\".</p> <pre><code>curl --socks5-hostname localhost:9050  ifconfig.me\n</code></pre> <p>Nota</p> <p>In questa modalit\u00e0 si rischia di perdere l'anonimizzazione. Quindi usarla soltanto se non quest'ultima non \u00e8 un requisito.</p>","tags":["tor","hacking","curl","scraping"]},{"location":"ricette/tree-lista-file-cartelle/","title":"Esportare la lista di file e cartelle in formato JSON","text":"<p>Si pu\u00f2 usare l'utility <code>tree</code>:</p> <pre><code>tree -Jhfpug /myPC/myFolder\n</code></pre> <p>In output (\u00e8 un esempio):</p> <pre><code>[\n  {\"type\":\"directory\",\"name\":\".\",\"contents\":[\n    {\"type\":\"file\",\"name\":\"./converti.md\",\"mode\":\"0744\",\"prot\":\"-rwxr--r--\",\"user\":\"username\",\"group\":\"groupname\",\"size\":\"11\"},\n    {\"type\":\"file\",\"name\":\"./descrivi.md\",\"mode\":\"0744\",\"prot\":\"-rwxr--r--\",\"user\":\"username\",\"group\":\"groupname\",\"size\":\"0\"},\n    {\"type\":\"file\",\"name\":\"./esplora.md\",\"mode\":\"0744\",\"prot\":\"-rwxr--r--\",\"user\":\"username\",\"group\":\"groupname\",\"size\":\"16K\"},\n    {\"type\":\"file\",\"name\":\"./index.md\",\"mode\":\"0744\",\"prot\":\"-rwxr--r--\",\"user\":\"username\",\"group\":\"groupname\",\"size\":\"1\"},\n    {\"type\":\"file\",\"name\":\"./tmp.csv\",\"mode\":\"0644\",\"prot\":\"-rw-r--r--\",\"user\":\"username\",\"group\":\"groupname\",\"size\":\"568K\"},\n    {\"type\":\"file\",\"name\":\"./trasforma.md\",\"mode\":\"0744\",\"prot\":\"-rwxr--r--\",\"user\":\"username\",\"group\":\"groupname\",\"size\":\"671\"}\n  ]},\n  {\"type\":\"report\",\"directories\":0,\"files\":6}\n]\n</code></pre> <p>Nota</p> <p>Se non inserisci alcun path, il comando verr\u00e0 eseguito per file e cartelle contenuti nella cartella corrente</p> <p>Fonte: https://twitter.com/climagic/status/1268900527561113600</p>","tags":["tree","json","list","file"]},{"location":"ricette/xmlstarlet-rendere-leggibile-file-xml/","title":"Come rendere pi\u00f9 leggibile un file XML","text":"<p>Alle volte pu\u00f2 essere utile applicare a un file <code>XML</code> un'indentazione che ne renda pi\u00f9 leggibile la struttura.</p> <p>Si pu\u00f2 usare l'utility <code>xmlstarlet</code> e applicare ad esempio a questo input (\u00e8 su una sola riga, scorrerlo tutto verso destra per farsi un'idea)</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;legge190:pubblicazione xsi:schemaLocation=\"legge190_1_0 http://dati.anticorruzione.it/schema/datasetAppaltiL190.xsd\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:legge190=\"legge190_1_0\"&gt;&lt;metadata&gt;&lt;titolo&gt;Comune di Novara&lt;/titolo&gt;&lt;abstract/&gt;&lt;dataPubblicazioneDataset&gt;2020-01-29+01:00&lt;/dataPubblicazioneDataset&gt;&lt;entePubblicatore&gt;Comune di Novara&lt;/entePubblicatore&gt;&lt;dataUltimoAggiornamentoDataset&gt;2020-03-10+01:00&lt;/dataUltimoAggiornamentoDataset&gt;&lt;annoRiferimento&gt;2019&lt;/annoRiferimento&gt;&lt;urlFile&gt;http://llpp.comune.novara.it/PortaleAppalti/resources/appaltiavcp/2019/Z00199663A.xml&lt;/urlFile&gt;&lt;licenza xsi:type=\"xs:string\" xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"&gt;IODL&lt;/licenza&gt;&lt;/metadata&gt;&lt;data&gt;&lt;lotto&gt;&lt;cig&gt;Z00199663A&lt;/cig&gt;&lt;strutturaProponente&gt;&lt;codiceFiscaleProp&gt;00125680033&lt;/codiceFiscaleProp&gt;&lt;denominazione&gt;Comune di Novara&lt;/denominazione&gt;&lt;/strutturaProponente&gt;&lt;oggetto&gt;Assistenza tecnica fotocopiatori Ricoh periodo maggio-giugno 2016&lt;/oggetto&gt;&lt;sceltaContraente&gt;23-AFFIDAMENTO DIRETTO&lt;/sceltaContraente&gt;&lt;partecipanti&gt;&lt;partecipante&gt;&lt;codiceFiscale&gt;00748490158&lt;/codiceFiscale&gt;&lt;ragioneSociale&gt;Ricoh Italia Srl&lt;/ragioneSociale&gt;&lt;/partecipante&gt;&lt;/partecipanti&gt;&lt;aggiudicatari&gt;&lt;aggiudicatario&gt;&lt;codiceFiscale&gt;00748490158&lt;/codiceFiscale&gt;&lt;ragioneSociale&gt;Ricoh Italia Srl&lt;/ragioneSociale&gt;&lt;/aggiudicatario&gt;&lt;/aggiudicatari&gt;&lt;importoAggiudicazione&gt;1500.0&lt;/importoAggiudicazione&gt;&lt;tempiCompletamento&gt;&lt;dataInizio&gt;2016-04-27+02:00&lt;/dataInizio&gt;&lt;/tempiCompletamento&gt;&lt;importoSommeLiquidate&gt;1024.22&lt;/importoSommeLiquidate&gt;&lt;/lotto&gt;&lt;/data&gt;&lt;/legge190:pubblicazione&gt;\n</code></pre> <p>il comando</p> <pre><code>xmlstarlet format --indent-tab input.xml\n</code></pre> <p>per avere questa versione molto pi\u00f9 leggibile dall'uomo:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;legge190:pubblicazione xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:legge190=\"legge190_1_0\" xsi:schemaLocation=\"legge190_1_0 http://dati.anticorruzione.it/schema/datasetAppaltiL190.xsd\"&gt;\n        &lt;metadata&gt;\n                &lt;titolo&gt;Comune di Novara&lt;/titolo&gt;\n                &lt;abstract/&gt;\n                &lt;dataPubblicazioneDataset&gt;2020-01-29+01:00&lt;/dataPubblicazioneDataset&gt;\n                &lt;entePubblicatore&gt;Comune di Novara&lt;/entePubblicatore&gt;\n                &lt;dataUltimoAggiornamentoDataset&gt;2020-03-10+01:00&lt;/dataUltimoAggiornamentoDataset&gt;\n                &lt;annoRiferimento&gt;2019&lt;/annoRiferimento&gt;\n                &lt;urlFile&gt;http://llpp.comune.novara.it/PortaleAppalti/resources/appaltiavcp/2019/Z00199663A.xml&lt;/urlFile&gt;\n                &lt;licenza xmlns:xs=\"http://www.w3.org/2001/XMLSchema\" xsi:type=\"xs:string\"&gt;IODL&lt;/licenza&gt;\n        &lt;/metadata&gt;\n        &lt;data&gt;\n                &lt;lotto&gt;\n                        &lt;cig&gt;Z00199663A&lt;/cig&gt;\n                        &lt;strutturaProponente&gt;\n                                &lt;codiceFiscaleProp&gt;00125680033&lt;/codiceFiscaleProp&gt;\n                                &lt;denominazione&gt;Comune di Novara&lt;/denominazione&gt;\n                        &lt;/strutturaProponente&gt;\n                        &lt;oggetto&gt;Assistenza tecnica fotocopiatori Ricoh periodo maggio-giugno 2016&lt;/oggetto&gt;\n                        &lt;sceltaContraente&gt;23-AFFIDAMENTO DIRETTO&lt;/sceltaContraente&gt;\n                        &lt;partecipanti&gt;\n                                &lt;partecipante&gt;\n                                        &lt;codiceFiscale&gt;00748490158&lt;/codiceFiscale&gt;\n                                        &lt;ragioneSociale&gt;Ricoh Italia Srl&lt;/ragioneSociale&gt;\n                                &lt;/partecipante&gt;\n                        &lt;/partecipanti&gt;\n                        &lt;aggiudicatari&gt;\n                                &lt;aggiudicatario&gt;\n                                        &lt;codiceFiscale&gt;00748490158&lt;/codiceFiscale&gt;\n                                        &lt;ragioneSociale&gt;Ricoh Italia Srl&lt;/ragioneSociale&gt;\n                                &lt;/aggiudicatario&gt;\n                        &lt;/aggiudicatari&gt;\n                        &lt;importoAggiudicazione&gt;1500.0&lt;/importoAggiudicazione&gt;\n                        &lt;tempiCompletamento&gt;\n                                &lt;dataInizio&gt;2016-04-27+02:00&lt;/dataInizio&gt;\n                        &lt;/tempiCompletamento&gt;\n                        &lt;importoSommeLiquidate&gt;1024.22&lt;/importoSommeLiquidate&gt;\n                &lt;/lotto&gt;\n        &lt;/data&gt;\n&lt;/legge190:pubblicazione&gt;\n</code></pre>","tags":["xmlstarlet","xml","indentazione","leggibilit\u00e0"]},{"location":"sqlite/","title":"sqlite","text":"<p>\u00c8 uno straordinario database relazionale open source, che pu\u00f2 funzionare anche a riga di comando.</p>"},{"location":"sqlite/#ricette","title":"Ricette","text":""},{"location":"sqlite/#eseguire-una-query","title":"Eseguire una query","text":"<pre><code>sqlite3 nomeDb 'select * from nomeTabella;\"'\n</code></pre> <p>Con questo file di esempio:</p> <pre><code>sqlite3 sqlite.db 'SELECT \"color\", \"shape\"\nFROM \"colored-shapes\"\nlimit 5;'\n</code></pre> <p>L'output standard \u00e8 un testo senza intestazione, separato da <code>|</code>:</p> <pre><code>yellow|triangle\nred|square\nred|circle\nred|square\npurple|triangle\n</code></pre>"},{"location":"sqlite/#avere-loutput-di-una-query-in-csv","title":"Avere l'output di una query in CSV","text":"<p>Si aggiungono le opzioni <code>-header -csv</code>, per impostare il formato di output e avere anche la riga di intestazione.</p> <pre><code>sqlite3  -header -csv sqlite.db 'SELECT \"color\", \"shape\"\nFROM \"colored-shapes\"\nlimit 5;'\n</code></pre> <p>In output</p> <pre><code>color,shape\nyellow,triangle\nred,square\nred,circle\nred,square\npurple,triangle\n</code></pre>"},{"location":"utilities/","title":"Utility","text":"<p>In questo elenco, le principali utility citate nella guida e/o in generale consigliate.</p>"},{"location":"utilities/#cat","title":"cat","text":"<p>Concatena file e li stampa nello <code>standard output</code>.</p> <p> https://man7.org/linux/man-pages/man1/cat.1.html</p>"},{"location":"utilities/#chardet","title":"chardet","text":"<p>Rileva l'encoding dei caratteri.</p> <p> https://github.com/chardet/chardet</p>"},{"location":"utilities/#ckanapi","title":"ckanapi","text":"<p>Per utilizzare le API di CKAN, a riga di comando.</p> <p> pagina dedicata https://github.com/ckan/ckanapi</p>"},{"location":"utilities/#csvkit","title":"csvkit","text":"<p>Una bella suite di utility a riga di comando per convertire ed elaborare file CSV. Ha fatto la storia.</p> <p> https://csvkit.readthedocs.io/</p>"},{"location":"utilities/#daff","title":"daff","text":"<p>Per confrontare file di testo in formato tabellare (csv, tsv, ssv, psv, json, sqlite).</p> <p> https://github.com/paulfitz/daff</p>"},{"location":"utilities/#datamash","title":"datamash","text":"<p>Esegue analisi statistiche su file di testo strutturati.</p> <p> https://www.gnu.org/software/datamash/manual/.html</p>"},{"location":"utilities/#diff","title":"diff","text":"<p>Per confrontare file, linea per linea.</p> <p> https://man7.org/linux/man-pages/man1/diff.1.html</p>"},{"location":"utilities/#file","title":"file","text":"<p>Restituisce informazioni sui file.</p> <p> https://www.darwinsys.com/file/</p>"},{"location":"utilities/#frictionless","title":"frictionless","text":"<p>L'utility a riga di comando del framework Frictionless</p> <p> pagina dedicata https://framework.frictionlessdata.io/docs/tutorials/working-with-cli</p>"},{"location":"utilities/#head","title":"head","text":"<p>Stampa le prime 10 righe nello <code>standard output</code>.</p> <p> https://man7.org/linux/man-pages/man1/tail.1.html</p>"},{"location":"utilities/#iconv","title":"iconv","text":"<p>Trasforma una codifica di caratteri in un'altra.</p> <p> https://pubs.opengroup.org/onlinepubs/009695399/functions/iconv.html</p>"},{"location":"utilities/#jq","title":"jq","text":"<p>Fa il parsing e trasforma file in formato <code>JSON</code>.</p> <p> https://stedolan.github.io/jq/</p>"},{"location":"utilities/#less","title":"less","text":"<p>Mostra il contenuto di un file, una pagina per volta, permettendo di eseguire ricerche di testo.</p> <p> https://man7.org/linux/man-pages/man1/less.1.html</p>"},{"location":"utilities/#miller","title":"Miller","text":"<p>Uno strumento per eseguire query, modellare e ristrutturare file di testo strutturati in vari formati, tra cui CSV, TSV, JSON e JSON Lines.</p> <p> pagina dedicata https://miller.readthedocs.io/en/latest/</p>"},{"location":"utilities/#paste","title":"paste","text":"<p>Unisce pi\u00f9 file in uno solo.</p> <p> https://manpages.ubuntu.com/manpages/kinetic/en/man1/paste.1.html</p>"},{"location":"utilities/#qsv","title":"qsv","text":"<p>Una ricchissima e potente utility per analizzare file in formato <code>CSV</code>.</p> <p> https://github.com/jqnatividad/qsv</p>"},{"location":"utilities/#scrape-cli","title":"scrape-cli","text":"<p>Per estrarre testo da pagine HTML, tramite <code>XPATH</code> e <code>CSS selector.</code></p> <p> https://github.com/aborruso/scrape-cli</p>"},{"location":"utilities/#split","title":"split","text":"<p>Suddivide un file di testo in pi\u00f9 parti.</p> <p> https://pubs.opengroup.org/onlinepubs/9699919799/utilities/split.html</p>"},{"location":"utilities/#sqlite","title":"sqlite","text":"<p>Uno straordinario database relazionale.</p> <p> https://www.sqlite.org/index.html</p>"},{"location":"utilities/#stat","title":"stat","text":"<p>Restituisce informazioi su file e sul filesystem.</p> <p> https://man7.org/linux/man-pages/man1/stat.1.html</p>"},{"location":"utilities/#tail","title":"tail","text":"<p>Stampa le ultime 10 righe nello <code>standard output</code>.</p> <p> https://man7.org/linux/man-pages/man1/tail.1.html</p>"},{"location":"utilities/#tldr","title":"tldr","text":"<p>Una mini guida con i comandi essenziali per utilizzare le utility pi\u00f9 diffuse.</p> <p> https://github.com/tldr-pages/tldr</p>"},{"location":"utilities/#tree","title":"tree","text":"<p>Restituisce l'albero di cartelle e file di percorso.</p> <p> http://mama.indstate.edu/users/ice/tree/</p>"},{"location":"utilities/#wc","title":"wc","text":"<p>Restituisce numero di linee, caratteri, parole e di byte di un file.</p> <p> https://man7.org/linux/man-pages/man1/wc.1.html</p>"},{"location":"utilities/#xmlstarlet","title":"xmlstarlet","text":"<p>Per modifcare, interrogare e trasformare file XML.</p> <p> http://xmlstar.sourceforge.net/</p>"},{"location":"utilities/#xan","title":"xan","text":"<p>Toolkit a riga di comando per esplorare, trasformare e analizzare file <code>CSV</code>.</p> <p> pagina dedicata https://github.com/medialab/xan</p>"},{"location":"utilities/#xsv","title":"xsv","text":"<p>Uno straordinario toolkit per leggere, filtrare, analizzare un CSV.</p> <p> https://github.com/BurntSushi/xsv</p>"},{"location":"utilities/#yq","title":"yq","text":"<p>Per fare il parsing e trasformare file in formato <code>YAML</code> (anche <code>TOML</code> e <code>XML</code>). Lo fa trasformando l'input da <code>YAML</code> a <code>JSON</code>, via <code>jq</code>.</p> <p> https://github.com/kislyuk/yq</p>"},{"location":"uv/","title":"uv - Gestore di pacchetti Python ultra-veloce","text":"<p><code>uv</code> \u00e8 un gestore di pacchetti e ambienti virtuali Python estremamente veloce, scritto in Rust. \u00c8 un'alternativa moderna a <code>pip</code>, <code>pipenv</code> e <code>poetry</code> che promette di essere 10-100 volte pi\u00f9 veloce.</p>"},{"location":"uv/#caratteristiche-principali","title":"Caratteristiche principali","text":"<ul> <li>Velocit\u00e0 estrema: 10-100x pi\u00f9 veloce di pip grazie all'implementazione in Rust</li> <li>Gestione unificata: Pacchetti, ambienti virtuali e progetti in un unico strumento</li> <li>Compatibilit\u00e0: Funziona con l'ecosistema Python esistente</li> <li>Risoluzione avanzata: Algoritmi sofisticati per la risoluzione delle dipendenze</li> <li>Cross-platform: Supporta Windows, macOS e Linux</li> </ul>"},{"location":"uv/#installazione","title":"Installazione","text":""},{"location":"uv/#su-linux-e-macos","title":"Su Linux e macOS","text":"<p>Il modo pi\u00f9 semplice \u00e8 utilizzare il script di installazione ufficiale:</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre>"},{"location":"uv/#con-pip","title":"Con pip","text":"<p>Se preferisci usare pip:</p> <pre><code>pip install uv\n</code></pre>"},{"location":"uv/#con-homebrew-macos","title":"Con Homebrew (macOS)","text":"<pre><code>brew install uv\n</code></pre>"},{"location":"uv/#con-cargo-rust","title":"Con Cargo (Rust)","text":"<p>Se hai Rust installato:</p> <pre><code>cargo install uv\n</code></pre>"},{"location":"uv/#ricette","title":"Ricette","text":""},{"location":"uv/#creare-un-nuovo-progetto","title":"Creare un nuovo progetto","text":"<p>Per creare un nuovo progetto Python con uv:</p> <pre><code>uv init mio-progetto\ncd mio-progetto\n</code></pre> <p>Questo crea una struttura di progetto con:</p> <ul> <li><code>pyproject.toml</code> - file di configurazione</li> <li><code>README.md</code> - documentazione base</li> <li><code>src/mio_progetto/</code> - codice sorgente</li> <li><code>.python-version</code> - versione Python specifica</li> </ul>"},{"location":"uv/#aggiungere-dipendenze","title":"Aggiungere dipendenze","text":"<p>Per aggiungere una nuova dipendenza al progetto:</p> <pre><code># Aggiungere una dipendenza di runtime\nuv add requests\n\n# Aggiungere una dipendenza di sviluppo\nuv add --dev pytest\n\n# Aggiungere una dipendenza specifica per versione\nuv add \"django&gt;=4.0,&lt;5.0\"\n</code></pre>"},{"location":"uv/#installare-le-dipendenze","title":"Installare le dipendenze","text":"<p>Per installare tutte le dipendenze di un progetto esistente:</p> <pre><code>uv sync\n</code></pre>"},{"location":"uv/#pulire-la-cache","title":"Pulire la cache","text":"<pre><code>uv cache prune\n</code></pre>"},{"location":"uv/#installare-un-tool-come-cli-indipendente","title":"Installare un tool come cli indipendente","text":"<p>Per installare un tool come comando CLI indipendente, puoi usare il comando <code>uv tool install</code>:</p> <pre><code>uv tool install black\n</code></pre>"},{"location":"uv/#eseguire-comandi-nellambiente-virtuale","title":"Eseguire comandi nell'ambiente virtuale","text":"<p>Per eseguire comandi nell'ambiente virtuale del progetto:</p> <pre><code># Eseguire un file Python\nuv run python main.py\n\n# Eseguire un modulo\nuv run -m pytest\n\n# Avviare una shell interattiva\nuv run python\n</code></pre>"},{"location":"uv/#gestire-versioni-python","title":"Gestire versioni Python","text":"<pre><code># Installare una versione specifica di Python\nuv python install 3.12\n\n# Vedere tutte le versioni disponibili\nuv python list\n\n# Impostare la versione per il progetto corrente\nuv python pin 3.12\n</code></pre>"},{"location":"uv/#creare-script-eseguibili","title":"Creare script eseguibili","text":"<p>Puoi creare script che specificano le loro dipendenze:</p> <pre><code>#!/usr/bin/env uv run\n# /// script\n# dependencies = [\n#   \"requests\",\n#   \"beautifulsoup4\",\n# ]\n# ///\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Il tuo codice qui...\n</code></pre> <p>Salvalo come <code>script.py</code> e rendilo eseguibile:</p> <pre><code>chmod +x script.py\n./script.py\n</code></pre>"},{"location":"uv/#lavorare-con-ambienti-virtuali","title":"Lavorare con ambienti virtuali","text":"<pre><code># Creare un ambiente virtuale\nuv venv\n\n# Creare con una versione Python specifica\nuv venv --python 3.11\n\n# Attivare l'ambiente (Linux/macOS)\nsource .venv/bin/activate\n\n# Installare pacchetti nell'ambiente attivo\nuv pip install pandas numpy\n</code></pre>"},{"location":"uv/#migrare-da-requirementstxt","title":"Migrare da requirements.txt","text":"<p>Se hai un progetto esistente con <code>requirements.txt</code>:</p> <pre><code># Creare pyproject.toml da requirements.txt\nuv init --python 3.11\nuv add -r requirements.txt\n</code></pre>"},{"location":"uv/#lavorare-con-piu-progetti","title":"Lavorare con pi\u00f9 progetti","text":"<pre><code># Creare workspace con pi\u00f9 progetti\nmkdir workspace\ncd workspace\n\n# Progetto principale\nuv init app\ncd app\nuv add fastapi uvicorn\n\n# Libreria condivisa\ncd ..\nuv init --lib shared\ncd shared\nuv add pydantic\n\n# Nel progetto principale, aggiungere la libreria locale\ncd ../app\nuv add ../shared\n</code></pre>"},{"location":"uv/#esportare-dipendenze","title":"Esportare dipendenze","text":"<p>Per esportare le dipendenze in formati compatibili:</p> <pre><code># Esportare come requirements.txt\nuv export --format requirements-txt &gt; requirements.txt\n\n# Esportare solo dipendenze di produzione\nuv export --no-dev --format requirements-txt &gt; requirements-prod.txt\n</code></pre>"},{"location":"uv/#tool-di-sviluppo-globali","title":"Tool di sviluppo globali","text":"<p>Installare strumenti di sviluppo globalmente:</p> <pre><code># Installare black globalmente\nuv tool install black\n\n# Eseguire uno strumento senza installarlo\nuv tool run ruff check .\n\n# Aggiornare tutti gli strumenti\nuv tool upgrade --all\n</code></pre>"},{"location":"uv/#vantaggi-rispetto-ad-altri-strumenti","title":"Vantaggi rispetto ad altri strumenti","text":"Funzionalit\u00e0 uv pip poetry pipenv Velocit\u00e0 \u26a1\u26a1\u26a1 \u26a1 \u26a1\u26a1 \u26a1 Gestione dipendenze \u2705 \u274c \u2705 \u2705 Lock file \u2705 \u274c \u2705 \u2705 Ambienti virtuali \u2705 \u274c \u2705 \u2705 Gestione Python \u2705 \u274c \u274c \u274c Workspace \u2705 \u274c \u2705 \u274c"},{"location":"uv/#risorse-utili","title":"Risorse utili","text":"<ul> <li>Documentazione ufficiale</li> <li>Repository GitHub</li> <li>Guida migrazione da Poetry</li> <li>Blog Astral - annunci e guide</li> </ul>"},{"location":"xan/","title":"xan","text":"<p>xan \u00e8 uno strumento a riga di comando per lavorare con file CSV in modo rapido, pensato per esplorare, filtrare, trasformare e analizzare dati strutturati.</p> <p>\u00c8 scritto in Rust ed \u00e8 nato come evoluzione dell'ecosistema <code>xsv</code>, con funzionalit\u00e0 aggiuntive orientate all'analisi dati.</p>"},{"location":"xan/#cosa-lo-distingue","title":"Cosa lo distingue","text":"<ul> <li>usa Moonblade, un linguaggio di espressioni integrato per trasformazioni e filtri avanzati;</li> <li>include comandi pratici per flussi di analisi su CSV anche grandi;</li> <li>offre utilit\u00e0 aggiuntive per text/data analysis oltre alle operazioni classiche sui CSV.</li> </ul>"},{"location":"xan/#risorse-ufficiali","title":"Risorse ufficiali","text":"<ul> <li>Repository: github.com/medialab/xan</li> </ul> <p>Questa \u00e8 una pagina introduttiva della monografia: nelle prossime pagine verranno aggiunti installazione, comandi base e ricette operative.</p>"},{"location":"xan/bisect/","title":"Ricerca rapida su CSV ordinati con <code>xan bisect</code>","text":"<p>Quando cerchi un valore in un CSV grande, <code>xan filter</code> scansiona tutto il file dall'inizio alla fine. Se il file \u00e8 ordinato, puoi fare molto meglio: <code>xan bisect</code> usa la ricerca binaria e individua la posizione giusta nel file senza leggere ogni riga.</p> <p>Il guadagno \u00e8 concreto: su un milione di righe <code>bisect</code> \u00e8 circa 10 volte pi\u00f9 veloce di <code>filter</code>.</p> <p>Il vincolo \u00e8 uno solo: il file deve essere gi\u00e0 ordinato sulla colonna che cerchi, e deve esistere su disco (non funziona da stdin).</p>"},{"location":"xan/bisect/#dataset-di-esempio","title":"Dataset di esempio","text":"<p>Usa i file di esempio nella cartella <code>docs/xan/risorse/bisect/</code>:</p> <ul> <li>Ordinato per <code>comune</code> (alfabetico): <code>capoluoghi.csv</code></li> <li>Ordinato per <code>popolazione</code> (numerico crescente): <code>capoluoghi_per_pop.csv</code></li> </ul> <p>Estratto di <code>capoluoghi.csv</code> (ordinato per <code>comune</code>):</p> <pre><code>codice,comune,regione,popolazione\nLOM002,Brescia,Lombardia,196745\nSIC002,Catania,Sicilia,301248\nCAL001,Catanzaro,Calabria,87024\nCAL002,Cosenza,Calabria,60775\nTOS001,Firenze,Toscana,358079\n...\n</code></pre>"},{"location":"xan/bisect/#ricerca-esatta-s","title":"Ricerca esatta (<code>-S</code>)","text":"<p>Con <code>-S</code> (o <code>--search</code>) <code>bisect</code> restituisce solo le righe che corrispondono esattamente al valore cercato:</p> <pre><code>xan bisect -S comune Napoli capoluoghi.csv\n</code></pre> <p>Output:</p> <pre><code>codice,comune,regione,popolazione\nCAM001,Napoli,Campania,909048\n</code></pre> <p>Senza <code>-S</code>, <code>bisect</code> si posiziona nel punto dove il valore si inserirebbe nel file e restituisce tutto da l\u00ec in poi \u2014 comportamento utile per le query di prefisso e di range descritte sotto.</p>"},{"location":"xan/bisect/#ricerca-per-prefisso","title":"Ricerca per prefisso","text":"<p><code>bisect</code> senza <code>-S</code> fa una \"lower bound\": si posiziona alla prima riga che \u00e8 maggiore o uguale al valore dato, e scarica tutto ci\u00f2 che segue. Combinando con <code>xan filter</code> ottieni i soli valori con un certo prefisso.</p> <p>Tutti i capoluoghi che iniziano con <code>P</code>:</p> <pre><code>xan bisect comune P capoluoghi.csv | xan filter 'comune.startswith(\"P\")'\n</code></pre> <p>Output:</p> <pre><code>codice,comune,regione,popolazione\nSIC001,Palermo,Sicilia,640598\nABR002,Pescara,Abruzzo,116286\nTOS002,Pisa,Toscana,90617\nBAS002,Potenza,Basilicata,66076\n</code></pre> <p><code>bisect</code> individua subito la prima riga con <code>comune &gt;= \"P\"</code> e scarica solo la coda del file; <code>filter</code> rimuove le righe che non iniziano con <code>P</code>.</p>"},{"location":"xan/bisect/#query-di-range-lettere-cf","title":"Query di range (lettere C\u2013F)","text":"<p>Senza <code>-S</code>, <code>bisect</code> restituisce tutto ci\u00f2 che viene dopo il punto di inserimento del valore cercato nel file \u2014 non solo le righe che corrispondono esattamente. Questo permette di costruire query di range combinando <code>bisect</code> con <code>xan filter</code>.</p> <p>Per ottenere tutti i capoluoghi con <code>comune</code> compreso tra C e F:</p> <ul> <li><code>bisect comune C</code> salta le righe precedenti alla lettera C e scarica tutto da l\u00ec in poi;</li> <li><code>xan filter 'lt(comune, \"G\")'</code> tiene solo le righe il cui <code>comune</code> viene prima della G (<code>lt</code> sta per less than).</li> </ul> <pre><code>xan bisect comune C capoluoghi.csv | xan filter 'lt(comune, \"G\")'\n</code></pre> <p>Output:</p> <pre><code>codice,comune,regione,popolazione\nSIC002,Catania,Sicilia,301248\nCAL001,Catanzaro,Calabria,87024\nCAL002,Cosenza,Calabria,60775\nTOS001,Firenze,Toscana,358079\nLAZ001,Frosinone,Lazio,46243\n</code></pre>"},{"location":"xan/bisect/#ordinamento-numerico-n","title":"Ordinamento numerico (<code>-N</code>)","text":"<p>Se il file \u00e8 ordinato per un campo numerico devi indicarlo con <code>-N</code> (o <code>--numeric</code>), altrimenti <code>bisect</code> usa l'ordine lessicografico e i risultati sarebbero errati.</p> <p>Il file <code>capoluoghi_per_pop.csv</code> \u00e8 ordinato per <code>popolazione</code> in senso crescente. Per trovare tutti i capoluoghi con pi\u00f9 di 300.000 abitanti:</p> <pre><code>xan bisect -N popolazione 300000 capoluoghi_per_pop.csv\n</code></pre> <p>Output:</p> <pre><code>codice,comune,regione,popolazione\nSIC002,Catania,Sicilia,301248\nTOS001,Firenze,Toscana,358079\nSIC001,Palermo,Sicilia,640598\nCAM001,Napoli,Campania,909048\nLOM001,Milano,Lombardia,1409252\nLAZ002,Roma,Lazio,2783809\n</code></pre> <p>Per trovare i capoluoghi con pi\u00f9 di 500.000 abitanti:</p> <pre><code>xan bisect -N popolazione 500000 capoluoghi_per_pop.csv\n</code></pre> <pre><code>codice,comune,regione,popolazione\nSIC001,Palermo,Sicilia,640598\nCAM001,Napoli,Campania,909048\nLOM001,Milano,Lombardia,1409252\nLAZ002,Roma,Lazio,2783809\n</code></pre>"},{"location":"xan/bisect/#ordinamento-decrescente-r","title":"Ordinamento decrescente (<code>-R</code>)","text":"<p>Se il file \u00e8 ordinato in senso decrescente (dal valore pi\u00f9 alto al pi\u00f9 basso), aggiungi <code>-R</code> (o <code>--reverse</code>):</p> <pre><code>xan bisect -N -R popolazione 500000 capoluoghi_per_pop.csv\n</code></pre>"},{"location":"xan/bisect/#cosa-succede-se-il-file-non-e-ordinato","title":"Cosa succede se il file non \u00e8 ordinato","text":"<p>Se provi a usare <code>bisect</code> su un file non ordinato, il comando se ne accorge e ti avverte:</p> <pre><code>xan bisect: input is not sorted in specified order!\nSee first and last values: 196745 and 52929\n</code></pre> <p>Il controllo avviene sul primo e ultimo valore: se risultano fuori ordine, il comando si ferma prima di restituire risultati sbagliati.</p>"},{"location":"xan/bisect/#riepilogo-opzioni","title":"Riepilogo opzioni","text":"Opzione Cosa fa <code>-S</code>, <code>--search</code> Restituisce solo le righe che corrispondono esattamente al valore <code>-N</code>, <code>--numeric</code> Tratta i valori come numeri (file ordinato numericamente) <code>-R</code>, <code>--reverse</code> File ordinato in senso decrescente <code>-E</code>, <code>--exclude</code> Esclude le righe che corrispondono esattamente (upper bound)"},{"location":"xan/dalle-frasi-agli-insight/","title":"Dalle frasi agli insight: <code>tokenize</code> + <code>vocab</code>","text":"<p>Se parti da testo libero, questa coppia \u00e8 il motivo per cui <code>xan</code> sorprende:</p> <ul> <li><code>tokenize</code> spezza il testo in token utili (parole, punteggiatura, ecc.);</li> <li><code>vocab</code> trasforma quei token in metriche lessicali pronte da leggere.</li> </ul> <p>Con pochi comandi passi da frasi grezze a segnali quantitativi come frequenze, importanza dei termini (TF-IDF) e relazione documento-termine.</p>"},{"location":"xan/dalle-frasi-agli-insight/#dataset-di-partenza","title":"Dataset di partenza","text":"<p>Usa il file di esempio: <code>docs/xan/risorse/testi-mini.csv</code></p> <ul> <li>CSV nel repository: docs/xan/risorse/testi-mini.csv</li> <li>Download diretto CSV: raw.githubusercontent.com/.../testi-mini.csv</li> </ul> <p>Estratto input:</p> <pre><code>id,titolo,testo\n1,Bici urbana,\"La ciclabile collega il centro alla stazione. Pi\u00f9 bici, meno auto.\"\n2,Verde pubblico,\"Nuovi alberi nel quartiere: aria pi\u00f9 pulita e strade pi\u00f9 vivibili.\"\n3,Mobilit\u00e0 casa-scuola,\"Pedibus e zone 30 riducono traffico e rumore davanti alle scuole.\"\n</code></pre>"},{"location":"xan/dalle-frasi-agli-insight/#tokenizzazione-tokenize","title":"Tokenizzazione (<code>tokenize</code>)","text":"<p>Nel comando seguente, <code>words testo</code> significa:</p> <ul> <li><code>words</code>: modalit\u00e0 di tokenizzazione per parole;</li> <li><code>testo</code>: nome della colonna da tokenizzare (solo quel campo viene analizzato).</li> <li><code>-T tipo</code>: aggiunge la colonna <code>tipo</code> (es. <code>word</code>, <code>punct</code>) e produce una riga per token.</li> </ul> <pre><code>xan tokenize words testo -T tipo --lower --drop punct docs/xan/risorse/testi-mini.csv &gt; /tmp/token.csv\n</code></pre> <p>Porzione output (<code>/tmp/token.csv</code>):</p> <pre><code>id,titolo,token,tipo\n1,Bici urbana,la,word\n1,Bici urbana,ciclabile,word\n1,Bici urbana,collega,word\n1,Bici urbana,pi\u00f9,word\n2,Verde pubblico,nuovi,word\n2,Verde pubblico,alberi,word\n</code></pre> <p>Cosa sta succedendo:</p> <ul> <li>ogni parola diventa una riga;</li> <li><code>id</code> e <code>titolo</code> restano attaccati al token, quindi non perdi il contesto;</li> <li>con <code>--lower --drop punct</code> normalizzi il testo ed elimini la punteggiatura.</li> </ul>"},{"location":"xan/dalle-frasi-agli-insight/#fotografia-del-corpus-vocab-corpus","title":"Fotografia del corpus (<code>vocab corpus</code>)","text":"<p>Input: il file tokenizzato <code>/tmp/token.csv</code>.</p> <p>Parametri chiave:</p> <ul> <li><code>--doc id</code>: usa la colonna <code>id</code> come identificatore del documento;</li> <li><code>--token token</code>: usa la colonna <code>token</code> come colonna dei token;</li> <li><code>--implode</code>: indica che l'input ha gi\u00e0 un token per riga (come l'output di <code>tokenize words</code> con <code>-T</code>).</li> </ul> <pre><code>xan vocab corpus --doc id --token token --implode /tmp/token.csv\n</code></pre> <p>Porzione output:</p> <pre><code>doc_count,token_count,distinct_token_count,average_doc_len\n5,51,44,10.2\n</code></pre> <p>Cosa leggere:</p> <ul> <li><code>doc_count</code>: quanti documenti analizzi;</li> <li><code>doc_count</code>: numero di documenti unici nel corpus (identificati da <code>--doc</code>, qui <code>id</code>);</li> <li><code>token_count</code>: volume totale del lessico;</li> <li><code>distinct_token_count</code>: variet\u00e0 lessicale;</li> <li><code>average_doc_len</code>: lunghezza media (in token) dei documenti.</li> </ul>"},{"location":"xan/dalle-frasi-agli-insight/#parole-chiave-del-corpus-vocab-token","title":"Parole chiave del corpus (<code>vocab token</code>)","text":"<p>Input: sempre <code>/tmp/token.csv</code>.</p> <p>Parametri chiave:</p> <ul> <li><code>--doc id</code>: raggruppa i token per documento;</li> <li><code>--token token</code>: legge i token dalla colonna <code>token</code>;</li> <li><code>--implode</code>: necessario perch\u00e9 il file \u00e8 gi\u00e0 in formato \u201cun token per riga\u201d.</li> </ul> <pre><code>xan vocab token --doc id --token token --implode /tmp/token.csv\n</code></pre> <p>Porzione output:</p> <pre><code>token,gf,df,df_ratio,idf,gfidf,pigeon\npi\u00f9,4,3,0.6,0.5108256237659907,6.666666666666667,0.9839999999999999\ne,5,4,0.8,0.22314355131420976,6.25,0.8403999999999999\n</code></pre> <p>Cosa leggere:</p> <ul> <li><code>gf</code>: quante volte una parola compare nel corpus;</li> <li><code>df</code>: in quanti documenti compare;</li> <li><code>idf</code>: quanto la parola \u00e8 discriminante (pi\u00f9 alto = pi\u00f9 distintiva).</li> </ul>"},{"location":"xan/dalle-frasi-agli-insight/#relazione-documento-parola-vocab-doc-token","title":"Relazione documento-parola (<code>vocab doc-token</code>)","text":"<p>Input: sempre <code>/tmp/token.csv</code>.</p> <p>Parametri chiave:</p> <ul> <li><code>--doc id</code>: calcola le metriche per coppia documento-termine;</li> <li><code>--token token</code>: definisce il termine da analizzare;</li> <li><code>--implode</code>: usa correttamente l'input gi\u00e0 esploso in token.</li> </ul> <pre><code>xan vocab doc-token --doc id --token token --implode /tmp/token.csv\n</code></pre> <p>Porzione output:</p> <pre><code>id,token,tf,expected_tf,tfidf,bm25,chi2\n2,pi\u00f9,2,0.8627450980392157,1.0216512475319814,0.6872258391671962,2.0740328820116054\n3,e,2,1.0784313725490196,0.44628710262841953,0.30020031723566354,1.1132312252964427\n1,collega,1,0.21568627450980393,1.6094379124341003,1.5594035731874445,3.7090909090909085\n</code></pre> <p>Significato colonne:</p> <ul> <li><code>id</code>: identificatore del documento (nel nostro caso il valore della colonna <code>id</code>).</li> <li><code>token</code>: termine analizzato.</li> <li><code>tf</code>: frequenza del termine nel documento.</li> <li><code>expected_tf</code>: frequenza attesa del termine nel documento, data la distribuzione nel corpus e la lunghezza del documento.</li> <li><code>tfidf</code>: peso del termine nel documento rispetto all'intero corpus (pi\u00f9 alto = pi\u00f9 caratteristico).</li> <li><code>bm25</code>: punteggio di rilevanza del termine con normalizzazione sulla lunghezza del documento.</li> <li><code>chi2</code>: significativit\u00e0 statistica dell'associazione termine-documento.</li> </ul> <p>Cosa leggere:</p> <ul> <li>qui non vedi solo parole globali: vedi parola dentro documento;</li> <li><code>tfidf</code> e <code>bm25</code> aiutano a capire quali termini caratterizzano un documento specifico;</li> <li>\u00e8 il passaggio che trasforma la tokenizzazione in insight azionabili (ranking, confronto documenti, ricerca semantica di base).</li> </ul>"},{"location":"xan/dalle-frasi-agli-insight/#perche-ti-potrebbe-essere-utile","title":"Perch\u00e9 ti potrebbe essere utile","text":"<p>I casi sotto sono volutamente descritti ad alto livello: servono a dare il senso di analisi pi\u00f9 complesse da costruire. In questi percorsi <code>xan</code> \u00e8 un ottimo strumento di accompagnamento, perch\u00e9 aiuta a passare rapidamente dall'intuizione ai primi segnali misurabili.</p> <ol> <li>Inchiesta su delibere, bandi o atti pubblici.    In pochi minuti individui parole e temi che distinguono un ente, un periodo o un assessore, e scopri dove cambiano priorit\u00e0 e linguaggio.</li> <li>Analisi di campagne elettorali e discorsi politici.    Metti a confronto programmi e interventi: <code>tfidf</code> e <code>bm25</code> evidenziano i termini davvero identitari di ogni candidato, non solo quelli pi\u00f9 frequenti.</li> <li>Monitoraggio media e comunicati stampa.    Su centinaia di testi trovi subito i frame narrativi dominanti e i termini emergenti, utile per titolare, contestualizzare e costruire grafici comparativi.</li> </ol>"},{"location":"xan/flatten/","title":"Ispezionare i record in verticale con <code>xan flatten</code>","text":"<p>Quando un CSV ha molte colonne, la vista tabellare standard diventa difficile da leggere: i valori si spostano fuori schermo e perdi il contesto dei campi.</p> <p><code>xan flatten</code> ribalta la prospettiva: mostra ogni record in verticale, con il nome del campo a sinistra e il valore a destra. \u00c8 pensato per esplorare i dati uno alla volta, senza bisogno di scrollare orizzontalmente.</p>"},{"location":"xan/flatten/#dataset-di-esempio","title":"Dataset di esempio","text":"<p>Usa il file di esempio: <code>docs/xan/risorse/flatten/biblioteche.csv</code></p> <ul> <li>CSV nel repository: docs/xan/risorse/flatten/biblioteche.csv</li> <li>Download diretto CSV: raw.githubusercontent.com/.../biblioteche.csv</li> </ul> <p>Il file contiene 5 biblioteche toscane con 16 colonne: dati anagrafici, contatti, orari, servizi. Molte celle sono vuote perch\u00e9 non tutte le biblioteche hanno tutti i servizi.</p> <p>Estratto input:</p> <pre><code>id,nome,comune,provincia,indirizzo,telefono,email,sito_web,orario_lun_ven,orario_sab,orario_dom,...\n1,Biblioteca Nazionale Centrale,Firenze,FI,Piazza dei Cavalleggeri 1,055 249191,bncf@...,https://...,08:15-19:15,08:15-13:30,,400,...\n2,Biblioteca Medicea Laurenziana,Firenze,FI,Piazza San Lorenzo 9,055 210760,bml@...,https://...,08:00-13:30,,,30,...\n</code></pre>"},{"location":"xan/flatten/#uso-base","title":"Uso base","text":"<pre><code>xan flatten biblioteche.csv\n</code></pre> <p>Output (prima riga):</p> <pre><code>Row n\u00b00\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nid               1\nnome             Biblioteca Nazionale Centrale\ncomune           Firenze\nprovincia        FI\nindirizzo        Piazza dei Cavalleggeri 1\ntelefono         055 249191\nemail            bncf@beniculturali.it\nsito_web         https://www.bncf.firenze.sbn.it\norario_lun_ven   08:15-19:15\norario_sab       08:15-13:30\norario_dom       &lt;empty&gt;\nposti_lettura    400\nsale_studio      6\nwifi             s\u00ec\naccesso_disabili s\u00ec\nnote             Deposito legale\n</code></pre> <p>Ogni campo compare su una riga separata. Le celle vuote appaiono come <code>&lt;empty&gt;</code> (qui mostrata soltanto la prima riga, per dare un'idea di come funziona).</p> <p>Puoi limitare il numero di righe da leggere con <code>-l</code>:</p> <pre><code>xan flatten -l 3 biblioteche.csv\n</code></pre>"},{"location":"xan/flatten/#solo-i-campi-compilati-n","title":"Solo i campi compilati (<code>-N</code>)","text":"<p>Con dati sparsi \u2014 dove molte celle sono vuote \u2014 la vista standard riempie lo schermo di <code>&lt;empty&gt;</code> e diventa difficile da leggere.</p> <p>L'opzione <code>-N</code> (o <code>--non-empty</code>) mostra solo i campi che hanno un valore:</p> <pre><code>xan flatten -N biblioteche.csv\n</code></pre> <p>Output della seconda riga (biblioteca con molti campi vuoti):</p> <pre><code>Row n\u00b01\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nid               2\nnome             Biblioteca Medicea Laurenziana\ncomune           Firenze\nprovincia        FI\nindirizzo        Piazza San Lorenzo 9\ntelefono         055 210760\nemail            bml@beniculturali.it\nsito_web         https://bml.firenze.sbn.it\norario_lun_ven   08:00-13:30\nposti_lettura    30\nwifi             s\u00ec\naccesso_disabili s\u00ec\nnote             Solo consultazione\n</code></pre> <p>Rispetto alla vista normale, spariscono <code>orario_sab</code>, <code>orario_dom</code> e <code>sale_studio</code> \u2014 che per questa biblioteca non sono compilati. Vedi subito cosa c'\u00e8, senza rumore.</p>"},{"location":"xan/flatten/#selezionare-le-colonne-s","title":"Selezionare le colonne (<code>-s</code>)","text":"<p>Se vuoi visualizzare solo alcune colonne puoi usare <code>-s</code>:</p> <pre><code>xan flatten -s \"nome,comune,telefono,email,orario_lun_ven\" -l 3 biblioteche.csv\n</code></pre> <p>Output:</p> <pre><code>Row n\u00b00\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nnome           Biblioteca Nazionale Centrale\ncomune         Firenze\ntelefono       055 249191\nemail          bncf@beniculturali.it\norario_lun_ven 08:15-19:15\n\nRow n\u00b01\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nnome           Biblioteca Medicea Laurenziana\ncomune         Firenze\ntelefono       055 210760\nemail          bml@beniculturali.it\norario_lun_ven 08:00-13:30\n\nRow n\u00b02\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nnome           Biblioteca Comunale\ncomune         Prato\ntelefono       0574 1837801\nemail          biblioteca@comune.prato.it\norario_lun_ven 09:00-19:00\n</code></pre> <p>Utile quando vuoi fare una verifica rapida su un sottoinsieme di campi senza caricare tutto l'output.</p>"},{"location":"xan/flatten/#navigare-file-grandi-con-il-pager","title":"Navigare file grandi con il pager","text":"<p>Per file con molti record, puoi scorrere l'output con <code>less</code>. Per mantenere i colori usa <code>--color always</code> e il flag <code>-r</code> di <code>less</code>:</p> <pre><code>xan flatten --color always biblioteche.csv | less -Sr\n</code></pre> <p><code>-S</code> in <code>less</code> disabilita il word wrap orizzontale, <code>-r</code> mantiene i colori ANSI.</p>"},{"location":"xan/flatten/#vista-compatta-c","title":"Vista compatta (<code>-c</code>)","text":"<p>Con <code>--condense</code> (o <code>-c</code>) i valori lunghi vengono troncati con <code>\u2026</code> anzich\u00e9 andare a capo. Utile per avere una panoramica veloce senza che i valori occupino pi\u00f9 righe:</p> <pre><code>xan flatten -c biblioteche.csv\n</code></pre> <pre><code>Row n\u00b00\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nid               1\nnome             Biblioteca Nazionale Centrale\ncomune           Firenze\nprovincia        FI\nindirizzo        Piazza dei Cavalleggeri 1\ntelefono         055 249191\nemail            bncf@beniculturali.it\nsito_web         https://www.bncf.firenze.sbn.it\norario_lun_ven   08:15-19:15\norario_sab       08:15-13:30\norario_dom       &lt;empty&gt;\nposti_lettura    400\nsale_studio      6\nwifi             s\u00ec\naccesso_disabili s\u00ec\nnote             Deposito legale\n</code></pre>"},{"location":"xan/flatten/#vista-per-testo-lungo-f","title":"Vista per testo lungo (<code>-F</code>)","text":"<p>L'opzione <code>-F</code> (<code>--flatter</code>) mette il nome del campo e il valore su righe separate, con una riga vuota tra ogni campo. Torna utile quando le celle contengono testi lunghi (descrizioni, note, paragrafi) che altrimenti si sovrappongono visivamente al nome del campo:</p> <pre><code>xan flatten -F -N -l 1 biblioteche.csv\n</code></pre> <pre><code>Row n\u00b00\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nid\n1\n\nnome\nBiblioteca Nazionale Centrale\n\ncomune\nFirenze\n\nprovincia\nFI\n\nindirizzo\nPiazza dei Cavalleggeri 1\n\n...\n</code></pre>"},{"location":"xan/flatten/#separatore-personalizzato-tra-record-row-separator","title":"Separatore personalizzato tra record (<code>--row-separator</code>)","text":"<p>Di default ogni record \u00e8 preceduto da un'intestazione con il numero di riga. Con <code>--row-separator</code> puoi sostituirla con qualsiasi stringa:</p> <pre><code>xan flatten --row-separator \"---\" -N -l 3 biblioteche.csv\n</code></pre> <pre><code>id               1\nnome             Biblioteca Nazionale Centrale\ncomune           Firenze\n...\n---\nid               2\nnome             Biblioteca Medicea Laurenziana\n...\n---\nid               3\nnome             Biblioteca Comunale\n...\n</code></pre> <p>Usando <code>--row-separator ''</code> (stringa vuota) i record si susseguono senza alcuna separazione.</p>"},{"location":"xan/flatten/#esportare-come-csv-csv","title":"Esportare come CSV (<code>--csv</code>)","text":"<p>Con <code>--csv</code> l'output non \u00e8 testo formattato ma un CSV con tre colonne: <code>row</code> (indice del record), <code>field</code> (nome del campo), <code>value</code> (valore). \u00c8 un'operazione di unpivot \u2014 utile per elaborare ulteriormente i dati in verticale:</p> <pre><code>xan flatten --csv -l 2 biblioteche.csv\n</code></pre> <pre><code>row,field,value\n0,id,1\n0,nome,Biblioteca Nazionale Centrale\n0,comune,Firenze\n0,provincia,FI\n0,indirizzo,Piazza dei Cavalleggeri 1\n0,telefono,055 249191\n0,email,bncf@beniculturali.it\n0,sito_web,https://www.bncf.firenze.sbn.it\n0,orario_lun_ven,08:15-19:15\n0,orario_sab,08:15-13:30\n0,orario_dom,\n0,posti_lettura,400\n0,sale_studio,6\n0,wifi,s\u00ec\n0,accesso_disabili,s\u00ec\n0,note,Deposito legale\n1,id,2\n1,nome,Biblioteca Medicea Laurenziana\n...\n</code></pre>"},{"location":"xan/flatten/#evidenziare-valori-h","title":"Evidenziare valori (<code>-H</code>)","text":"<p>Con <code>-H</code> puoi mettere in evidenza le celle che corrispondono a un pattern regex. Torna utile quando cerchi un valore specifico tra molti campi:</p> <pre><code>xan flatten -H \"s\u00ec\" -l 1 biblioteche.csv\n</code></pre> <p>Le celle che contengono <code>s\u00ec</code> vengono evidenziate in rosso nel terminale.</p> <p>Puoi abbinare <code>-i</code> per rendere il pattern case-insensitive:</p> <pre><code>xan flatten -H \"firenze\" -i biblioteche.csv\n</code></pre>"},{"location":"xan/flatten/#riepilogo-opzioni-principali","title":"Riepilogo opzioni principali","text":"Opzione Cosa fa <code>-l &lt;n&gt;</code> Legge solo le prime <code>n</code> righe <code>-N</code>, <code>--non-empty</code> Mostra solo i campi compilati <code>-s &lt;colonne&gt;</code> Seleziona un sottoinsieme di colonne <code>-c</code>, <code>--condense</code> Tronca valori lunghi con <code>\u2026</code> <code>-F</code>, <code>--flatter</code> Valore su riga separata (utile per testi lunghi) <code>--csv</code> Output come CSV con colonne <code>row,field,value</code> <code>-H &lt;pattern&gt;</code> Evidenzia celle che corrispondono al pattern <code>-i</code> Pattern case-insensitive (da usare con <code>-H</code>) <code>--row-separator</code> Stringa separatrice tra record <code>--color always</code> Forza i colori ANSI (utile con <code>less -Sr</code>)"},{"location":"xan/from/","title":"Importare dati in xan con <code>xan from</code>","text":"<p>xan lavora nativamente con CSV. Se i tuoi dati sono in un altro formato \u2014 Excel, JSON, testo semplice, tabelle Markdown \u2014 <code>xan from</code> li converte in CSV cos\u00ec da poterli usare immediatamente nel resto del pipeline.</p> <p>Il formato viene rilevato automaticamente dall'estensione del file. Quando si legge da stdin va specificato esplicitamente con <code>-f</code>.</p>"},{"location":"xan/from/#file-di-esempio-download-raw","title":"File di esempio (download raw)","text":"<ul> <li><code>dipendenti.json</code> \u2014 JSON array</li> <li><code>eventi.ndjson</code> \u2014 NDJSON</li> <li><code>comuni.md</code> \u2014 Markdown con tabella</li> <li><code>dati_pa.xlsx</code> \u2014 Excel con due schede</li> </ul>"},{"location":"xan/from/#excel-e-ods","title":"Excel e ODS","text":"<p>Excel \u00e8 il formato pi\u00f9 diffuso nella pubblica amministrazione italiana. <code>xan from</code> converte qualsiasi scheda di un file <code>.xlsx</code>, <code>.xls</code>, <code>.xlsb</code> o <code>.ods</code> in CSV.</p> <p>Per vedere le schede disponibili in un file:</p> <pre><code>xan from --list-sheets dati_pa.xlsx\n</code></pre> <pre><code>Dipendenti\nComuni\n</code></pre> <p>Per convertire la prima scheda (quella di default):</p> <pre><code>xan from dati_pa.xlsx\n</code></pre> <pre><code>id,nome,reparto\n1,Alice Rossi,Anagrafe\n2,Bruno Ferrari,Urbanistica\n</code></pre> <p>Per convertire una scheda specifica per nome:</p> <pre><code>xan from --sheet-name Comuni dati_pa.xlsx\n</code></pre> <pre><code>comune,provincia\nFirenze,FI\nPrato,PO\n</code></pre> <p>Oppure per indice (0-based):</p> <pre><code>xan from --sheet-index 1 dati_pa.xlsx\n</code></pre>"},{"location":"xan/from/#json","title":"JSON","text":"<p><code>xan from</code> converte un array JSON in CSV, usando le chiavi del primo record come intestazioni. Ogni oggetto nell'array diventa una riga.</p> <p>Input (<code>dipendenti.json</code>):</p> <pre><code>[\n  {\"id\": 1, \"nome\": \"Alice Rossi\", \"reparto\": \"Anagrafe\", \"comune\": \"Firenze\", \"anno_assunzione\": 2018},\n  {\"id\": 2, \"nome\": \"Bruno Ferrari\", \"reparto\": \"Urbanistica\", \"comune\": \"Prato\", \"anno_assunzione\": 2015},\n  ...\n]\n</code></pre> <pre><code>xan from dipendenti.json\n</code></pre> <pre><code>id,nome,reparto,comune,anno_assunzione\n1,Alice Rossi,Anagrafe,Firenze,2018\n2,Bruno Ferrari,Urbanistica,Prato,2015\n3,Carla Bianchi,Anagrafe,Siena,2020\n4,Diego Conti,Tributi,Firenze,2012\n5,Elena De Luca,Urbanistica,Grosseto,2022\n</code></pre>"},{"location":"xan/from/#chiavi-non-consistenti-tra-record-sample-size","title":"Chiavi non consistenti tra record (<code>--sample-size</code>)","text":"<p>In un JSON reale non tutti gli oggetti hanno necessariamente le stesse chiavi. <code>xan from</code> risolve questo campionando i primi 64 record per stabilire l'elenco completo delle colonne, e riempiend con celle vuote le chiavi assenti in certi record.</p> <p>Se il file \u00e8 grande e le chiavi extra compaiono oltre i primi 64 record, potresti perdere colonne. In quel caso usa <code>--sample-size -1</code> per campionare l'intero file prima di emettere le intestazioni (costa pi\u00f9 memoria, ma garantisce completezza):</p> <pre><code>xan from --sample-size -1 avvisi.json\n</code></pre>"},{"location":"xan/from/#ordinare-le-colonne-sort-keys","title":"Ordinare le colonne (<code>--sort-keys</code>)","text":"<p>Di default le colonne seguono l'ordine in cui appaiono nei record JSON. Con <code>--sort-keys</code> vengono ordinate alfabeticamente, utile per armonizzare sorgenti diverse che usano lo stesso schema ma con chiavi in ordine diverso:</p> <pre><code>xan from --sort-keys avvisi.json\n</code></pre> <pre><code>data,id,luogo,ora,titolo\n2024-03-01,1,,,Chiusura uffici\n...\n</code></pre>"},{"location":"xan/from/#oggetto-singolo-single-object","title":"Oggetto singolo (<code>--single-object</code>)","text":"<p>Se il JSON \u00e8 un singolo oggetto (non un array), di default <code>xan from</code> lo converte in due colonne <code>key</code> e <code>value</code>. Con <code>--single-object</code> lo tratta invece come un'unica riga, con le chiavi come intestazioni:</p> <pre><code>xan from --single-object config.json\n</code></pre> <p>Input:</p> <pre><code>{\"ente\": \"Comune di Firenze\", \"codice\": \"FI001\", \"regione\": \"Toscana\", \"sito\": \"https://comune.fi.it\"}\n</code></pre> <p>Output:</p> <pre><code>ente,codice,regione,sito\nComune di Firenze,FI001,Toscana,https://comune.fi.it\n</code></pre>"},{"location":"xan/from/#ndjson-e-jsonl","title":"NDJSON e JSONL","text":"<p>Il formato NDJSON (newline-delimited JSON) ha un oggetto JSON per riga. \u00c8 comune negli export di API, nei log applicativi e nei dataset di grandi dimensioni perch\u00e9 pu\u00f2 essere letto in streaming senza caricare tutto in memoria.</p> <p>Input (<code>eventi.ndjson</code>):</p> <pre><code>{\"id\": 1, \"timestamp\": \"2024-01-10T08:15:00\", \"tipo\": \"accesso\", \"utente\": \"alice\", \"ip\": \"192.168.1.10\"}\n{\"id\": 2, \"timestamp\": \"2024-01-10T08:22:00\", \"tipo\": \"download\", \"utente\": \"bruno\", \"ip\": \"192.168.1.25\", \"file\": \"report_2023.pdf\"}\n{\"id\": 3, \"timestamp\": \"2024-01-10T09:01:00\", \"tipo\": \"accesso\", \"utente\": \"carla\", \"ip\": \"10.0.0.5\"}\n{\"id\": 4, \"timestamp\": \"2024-01-10T09:45:00\", \"tipo\": \"errore\", \"utente\": \"alice\", \"ip\": \"192.168.1.10\", \"codice\": 403}\n{\"id\": 5, \"timestamp\": \"2024-01-10T10:30:00\", \"tipo\": \"download\", \"utente\": \"alice\", \"ip\": \"192.168.1.10\", \"file\": \"bilancio_2024.xlsx\"}\n</code></pre> <pre><code>xan from eventi.ndjson\n</code></pre> <pre><code>id,timestamp,tipo,utente,ip,file,codice\n1,2024-01-10T08:15:00,accesso,alice,192.168.1.10,,\n2,2024-01-10T08:22:00,download,bruno,192.168.1.25,report_2023.pdf,\n3,2024-01-10T09:01:00,accesso,carla,10.0.0.5,,\n4,2024-01-10T09:45:00,errore,alice,192.168.1.10,,403\n5,2024-01-10T10:30:00,download,alice,192.168.1.10,bilancio_2024.xlsx,\n</code></pre> <p>I campi assenti in certi record (<code>file</code>, <code>codice</code>) diventano celle vuote. Il formato <code>.jsonl</code> si comporta in modo identico.</p>"},{"location":"xan/from/#testo-semplice-txt","title":"Testo semplice (TXT)","text":"<p>Un file di testo con un valore per riga viene convertito in un CSV a colonna singola, di default chiamata <code>value</code>:</p> <pre><code>cat nominativi.txt | xan from -f txt\n</code></pre> <pre><code>value\nAlice Rossi\nBruno Ferrari\nCarla Bianchi\n</code></pre> <p>Per rinominare la colonna usa <code>-c</code>:</p> <pre><code>cat nominativi.txt | xan from -f txt -c nominativo\n</code></pre> <pre><code>nominativo\nAlice Rossi\nBruno Ferrari\nCarla Bianchi\n</code></pre>"},{"location":"xan/from/#tabelle-markdown","title":"Tabelle Markdown","text":"<p><code>xan from</code> estrae la prima tabella da un file Markdown. Utile per portare nel pipeline tabelle gi\u00e0 formattate in documenti di testo o README.</p> <p>Input (<code>comuni.md</code>):</p> <pre><code>| comune | provincia | popolazione | superficie_kmq |\n|--------|-----------|-------------|----------------|\n| Firenze | FI | 358079 | 102.3 |\n| Prato | PO | 194875 | 97.5 |\n</code></pre> <pre><code>xan from comuni.md\n</code></pre> <pre><code>comune,provincia,popolazione,superficie_kmq\nFirenze,FI,358079,102.3\nPrato,PO,194875,97.5\nSiena,SI,53901,118.5\nGrosseto,GR,80611,473.3\nArezzo,AR,98822,388.0\n</code></pre> <p>Se il documento contiene pi\u00f9 tabelle puoi selezionarne una specifica con <code>--nth-table</code> (indice 0-based; valori negativi contano dalla fine):</p> <pre><code>xan from --nth-table 1 documento.md   # seconda tabella\nxan from --nth-table -1 documento.md  # ultima tabella\n</code></pre>"},{"location":"xan/from/#lettura-da-stdin","title":"Lettura da stdin","text":"<p>Quando i dati arrivano da stdin non c'\u00e8 estensione del file da cui inferire il formato: va specificato esplicitamente con <code>-f</code>.</p> <pre><code>curl -s https://example.com/api/data.json | xan from -f json\n</code></pre>"},{"location":"xan/from/#integrazione-nel-pipeline","title":"Integrazione nel pipeline","text":"<p><code>xan from</code> produce CSV su stdout, quindi si collega direttamente al resto dei comandi xan:</p> <pre><code>xan from dipendenti.json | xan filter 'eq(reparto, \"Anagrafe\")'\n</code></pre> <pre><code>id,nome,reparto,comune,anno_assunzione\n1,Alice Rossi,Anagrafe,Firenze,2018\n3,Carla Bianchi,Anagrafe,Siena,2020\n</code></pre> <pre><code>xan from eventi.ndjson | xan frequency -s tipo\n</code></pre> <pre><code>value,count\naccesso,2\ndownload,2\nerrore,1\n</code></pre>"},{"location":"xan/funzioni-fonetiche/","title":"Trovare nomi simili per suono con le funzioni fonetiche di Moonblade","text":"<p>Nei dati reali lo stesso cognome compare spesso scritto in modi diversi: Rossi, Rosi, Russi. Il confronto esatto non trova nulla; il confronto fonetico s\u00ec.</p> <p>Le funzioni <code>soundex</code>, <code>refined_soundex</code> e <code>phonogram</code> in Moonblade trasformano una stringa nel suo codice fonico \u2014 una rappresentazione di come suona, indipendentemente dalla grafia. Due parole con lo stesso codice si pronunciano in modo simile.</p> <p>Le funzioni si usano nelle espressioni Moonblade all'interno di comandi come <code>xan map</code> e <code>xan filter</code>.</p>"},{"location":"xan/funzioni-fonetiche/#dataset-di-esempio","title":"Dataset di esempio","text":"<p>Usa il file di esempio: <code>docs/xan/risorse/fonetiche/anagrafe.csv</code></p> <ul> <li>CSV nel repository: docs/xan/risorse/fonetiche/anagrafe.csv</li> <li>Download diretto CSV: raw.githubusercontent.com/.../anagrafe.csv</li> </ul> <p>Il file contiene 18 record con cognomi scritti in varianti ortografiche diverse:</p> <pre><code>id,cognome,nome,comune\n1,Rossi,Mario,Roma\n2,Rosi,Anna,Milano\n3,Russi,Giovanni,Napoli\n4,Ferrari,Lucia,Torino\n5,Ferrary,Marco,Bologna\n6,Ferraro,Sara,Palermo\n7,Bianchi,Luca,Firenze\n8,Bianki,Elena,Genova\n...\n</code></pre>"},{"location":"xan/funzioni-fonetiche/#come-funzionano-le-tre-funzioni","title":"Come funzionano le tre funzioni","text":"<p>Le tre funzioni seguono lo stesso principio: ignorano le differenze di grafia e codificano solo il suono. Lo fanno in modo diverso, con granularit\u00e0 crescente.</p> <p><code>soundex</code> \u00e8 l'algoritmo classico, sviluppato negli anni '40 per gestire varianti ortografiche nei registri anagrafici americani. Produce un codice di quattro caratteri: la prima lettera della parola seguita da tre cifre che rappresentano i gruppi consonantici principali. Consonanti foneticamente simili (come <code>c</code>, <code>g</code>, <code>k</code>) finiscono nello stesso gruppo e producono la stessa cifra. Vocali e lettere doppie vengono ignorate. Risultato: Rossi, Rosi e Russi producono tutti <code>R200</code>.</p> <p><code>refined_soundex</code> \u00e8 una versione pi\u00f9 precisa dello stesso algoritmo: usa una tabella di codifica pi\u00f9 dettagliata e produce codici pi\u00f9 lunghi. Distingue varianti che soundex classico raggrupperebbe: Russo e Ruzzo danno rispettivamente <code>R030</code> e <code>R050</code>.</p> <p><code>phonogram</code> usa un approccio diverso: si concentra sulle consonanti foneticamente rilevanti e produce codici brevi e compatti (es. <code>rs</code> per Rossi, <code>frr</code> per Ferrari). \u00c8 pi\u00f9 tollerante alle variazioni vocaliche e adatto a stringhe brevi.</p> <p>Il comando seguente aggiunge al file tre nuove colonne, una per ciascuna funzione:</p> <pre><code>xan map 'soundex(cognome) as sdx, refined_soundex(cognome) as rsdx, phonogram(cognome) as phon' \\\n  anagrafe.csv\n</code></pre> <p><code>xan map</code> valuta l'espressione per ogni riga e aggiunge le colonne risultanti in coda. L'espressione contiene tre chiamate separate, collegate dalla virgola: <code>soundex(cognome) as sdx</code> calcola il codice soundex del campo <code>cognome</code> e lo salva nella nuova colonna <code>sdx</code>; lo stesso pattern si ripete per <code>rsdx</code> e <code>phon</code>.</p> <p>Output:</p> <pre><code>id,cognome,nome,comune,sdx,rsdx,phon\n1,Rossi,Mario,Roma,R200,R030,rs\n2,Rosi,Anna,Milano,R200,R030,rs\n3,Russi,Giovanni,Napoli,R200,R030,rs\n4,Ferrari,Lucia,Torino,F660,F09090,frr\n5,Ferrary,Marco,Bologna,F660,F09090,frr\n6,Ferraro,Sara,Palermo,F660,F09090,frr\n7,Bianchi,Luca,Firenze,B520,B0830,bn\u0283\n8,Bianki,Elena,Genova,B520,B0830,bnk\n9,Esposito,Carlo,Napoli,E212,E3103060,spst\n10,Espositu,Rosa,Bari,E212,E3103060,spst\n11,Conti,Pietro,Roma,C530,C0860,knt\n12,Conte,Giulia,Milano,C530,C0860,knt\n13,De Luca,Fabio,Napoli,D420,D07030,dlk\n14,DeLuca,Marta,Salerno,D420,D07030,dlk\n15,Santoro,Bruno,Palermo,S536,S086090,sntr\n16,Santoru,Carla,Cagliari,S536,S086090,sntr\n17,Russo,Paolo,Napoli,R200,R030,rs\n18,Ruzzo,Teresa,Catanzaro,R200,R050,rs\n</code></pre> <p>Cosa notare:</p> <ul> <li>Rossi, Rosi, Russi, Russo producono tutti <code>R200</code> in soundex \u2014 stesso suono, grafia diversa;</li> <li>Ferrari, Ferrary, Ferraro danno tutti <code>F660</code>;</li> <li><code>refined_soundex</code> \u00e8 pi\u00f9 granulare: distingue Ruzzo da Russo (<code>R050</code> vs <code>R030</code>), cosa che soundex classico non fa;</li> <li><code>phonogram</code> produce codici pi\u00f9 compatti e focalizzati sulle consonanti.</li> </ul> <p>Quando scegliere quale funzione:</p> Funzione Caratteristiche <code>soundex</code> Algoritmo classico (anni '40), ampia tolleranza, pu\u00f2 raggruppare cognomi troppo diversi <code>refined_soundex</code> Pi\u00f9 preciso, distingue meglio varianti sottili <code>phonogram</code> Codici brevi basati sulle consonanti, adatto a testi brevi"},{"location":"xan/funzioni-fonetiche/#trovare-tutti-i-record-che-suonano-come-un-cognome","title":"Trovare tutti i record che suonano come un cognome","text":"<p>Per trovare nell'anagrafe tutti i record che suonano come Rossi:</p> <pre><code>xan map 'soundex(cognome) as sdx' anagrafe.csv \\\n  | xan filter 'eq(sdx, \"R200\")'\n</code></pre> <p>Output:</p> <pre><code>id,cognome,nome,comune,sdx\n1,Rossi,Mario,Roma,R200\n2,Rosi,Anna,Milano,R200\n3,Russi,Giovanni,Napoli,R200\n17,Russo,Paolo,Napoli,R200\n18,Ruzzo,Teresa,Catanzaro,R200\n</code></pre> <p>Cinque persone con cognomi scritti in modo diverso, ma tutti con lo stesso suono.</p> <p>Il flusso \u00e8:</p> <ol> <li><code>xan map</code> aggiunge la colonna <code>sdx</code> con il codice fonico per ogni riga;</li> <li><code>xan filter</code> seleziona solo le righe con il codice di Rossi (<code>R200</code>).</li> </ol>"},{"location":"xan/funzioni-fonetiche/#raggruppare-varianti-e-trovare-duplicati-fonetici","title":"Raggruppare varianti e trovare duplicati fonetici","text":"<p>Per vedere quali cognomi diversi condividono lo stesso codice fonico, ordina per codice e raggruppa:</p> <pre><code>xan map 'soundex(cognome) as sdx' anagrafe.csv \\\n  | xan sort -s sdx \\\n  | xan select sdx,cognome\n</code></pre> <p>Output:</p> <pre><code>sdx,cognome\nB520,Bianchi\nB520,Bianki\nC530,Conti\nC530,Conte\nD420,De Luca\nD420,DeLuca\nE212,Esposito\nE212,Espositu\nF660,Ferrari\nF660,Ferrary\nF660,Ferraro\nR200,Rossi\nR200,Rosi\nR200,Russi\nR200,Russo\nR200,Ruzzo\nS536,Santoro\nS536,Santoru\n</code></pre> <p>Ogni gruppo con lo stesso <code>sdx</code> rappresenta varianti foneticamente equivalenti.</p>"},{"location":"xan/funzioni-fonetiche/#deduplicare-per-suono","title":"Deduplicare per suono","text":"<p>Per tenere un solo record per ogni \"suono\" distinto di cognome:</p> <pre><code>xan map 'soundex(cognome) as sdx' anagrafe.csv \\\n  | xan sort -s sdx \\\n  | xan dedup -s sdx \\\n  | xan select cognome,sdx\n</code></pre> <p>Output:</p> <pre><code>cognome,sdx\nBianchi,B520\nConti,C530\nDe Luca,D420\nEsposito,E212\nFerrari,F660\nRossi,R200\nSantoro,S536\n</code></pre> <p>Da 18 record con varianti ortografiche, si passa a 7 cognomi foneticamente distinti.</p>"},{"location":"xan/funzioni-fonetiche/#quando-e-utile","title":"Quando \u00e8 utile","text":"<ul> <li>Pulizia di anagrafiche: individuare nomi duplicati inseriti con grafie diverse in database diversi o in momenti diversi.</li> <li>Ricerca tollerante agli errori: trovare un cognome anche se \u00e8 stato scritto male o traslitterato.</li> <li>Confronto tra dataset: unire due elenchi che usano convenzioni grafiche diverse per gli stessi nomi.</li> <li>Verifica di dati raccolti manualmente: moduli cartacei digitalizzati, dati storici, trascrizioni.</li> </ul>"},{"location":"xan/join/","title":"Join con xan: casi base, fuzzy, regex e URL","text":"<p>Questa pagina mostra i casi principali di join con esempi minimi.</p> <p>Questa guida \u00e8 scritta per il comportamento di <code>xan 0.56.0</code> o superiore:</p> <ul> <li>join classici con <code>xan join</code>;</li> <li>join fuzzy/regex/url con <code>xan fuzzy-join</code> (<code>-r</code> per regex, <code>-u</code> per URL prefix).</li> </ul>"},{"location":"xan/join/#file-di-esempio-download-raw","title":"File di esempio (download raw)","text":"<ul> <li><code>dati_a.csv</code></li> <li><code>dati_b.csv</code></li> <li><code>persone.csv</code></li> <li><code>ordini.csv</code></li> <li><code>lettere.csv</code></li> <li><code>numeri.csv</code></li> <li><code>testi.csv</code></li> <li><code>pattern.csv</code></li> <li><code>pattern_fuzzy.csv</code></li> <li><code>link.csv</code></li> <li><code>sorgenti_url.csv</code></li> </ul> <p>Per eseguire i comandi cos\u00ec come sono scritti sotto, posizionati nella cartella:</p> <pre><code>cd docs/xan/risorse/join\n</code></pre>"},{"location":"xan/join/#join-standard-xan-join","title":"Join standard (<code>xan join</code>)","text":"<p>Sintassi base:</p> <pre><code>xan join [opzioni] &lt;campi_sx&gt; &lt;file_sx.csv&gt; &lt;campi_dx&gt; &lt;file_dx.csv&gt;\n</code></pre> <p>Parametri chiave:</p> <ul> <li><code>&lt;campi_sx&gt;</code>: una o pi\u00f9 colonne chiave del file di sinistra (es. <code>id</code> oppure pi\u00f9 campi separati da virgola <code>anno,comune</code>);</li> <li><code>&lt;file_sx.csv&gt;</code>: CSV di sinistra;</li> <li><code>&lt;campi_dx&gt;</code>: una o pi\u00f9 colonne chiave del file di destra, nello stesso ordine di <code>&lt;campi_sx&gt;</code>;</li> <li><code>&lt;file_dx.csv&gt;</code>: CSV di destra.</li> </ul> <p>Regola pratica:</p> <ul> <li>puoi fare join su un campo (<code>id</code>);</li> <li>puoi fare join su pi\u00f9 campi (<code>anno,comune</code>), ma il numero dei campi deve coincidere a sinistra e a destra.</li> </ul> <p>Esempio multi-colonna:</p> <p><code>dati_a.csv</code></p> <pre><code>anno,comune,valore_a\n2024,Roma,10\n2024,Milano,20\n2025,Roma,30\n</code></pre> <p><code>dati_b.csv</code></p> <pre><code>anno,comune,valore_b\n2024,Roma,100\n2025,Roma,300\n2025,Torino,400\n</code></pre> <pre><code>xan join anno,comune dati_a.csv anno,comune dati_b.csv\n</code></pre> <p>Output (estratto):</p> <pre><code>anno,comune,valore_a,valore_b\n2024,Roma,10,100\n2025,Roma,30,300\n</code></pre> <p>Input usati negli esempi:</p> <p><code>persone.csv</code></p> <pre><code>id,nome,citta\n1,Ada,Roma\n2,Luca,Milano\n3,Sara,Torino\n</code></pre> <p><code>ordini.csv</code></p> <pre><code>id,ordine,valore\n1,Penna,5\n2,Taccuino,8\n4,Zaino,35\n</code></pre>"},{"location":"xan/join/#inner-join-default","title":"Inner join (default)","text":"<pre><code>xan join id persone.csv id ordini.csv\n</code></pre> <p>Output (estratto):</p> <pre><code>id,nome,citta,ordine,valore\n1,Ada,Roma,Penna,5\n2,Luca,Milano,Taccuino,8\n</code></pre>"},{"location":"xan/join/#left-join","title":"Left join","text":"<pre><code>xan join --left id persone.csv id ordini.csv\n</code></pre> <p>Output (estratto):</p> <pre><code>id,nome,citta,ordine,valore\n1,Ada,Roma,Penna,5\n2,Luca,Milano,Taccuino,8\n3,Sara,Torino,,\n</code></pre>"},{"location":"xan/join/#right-join","title":"Right join","text":"<pre><code>xan join --right id persone.csv id ordini.csv\n</code></pre> <p>Output (estratto):</p> <pre><code>nome,citta,id,ordine,valore\nAda,Roma,1,Penna,5\nLuca,Milano,2,Taccuino,8\n,,4,Zaino,35\n</code></pre>"},{"location":"xan/join/#full-join","title":"Full join","text":"<pre><code>xan join --full id persone.csv id ordini.csv\n</code></pre> <p>Output (estratto):</p> <pre><code>id,nome,citta,ordine,valore\n1,Ada,Roma,Penna,5\n2,Luca,Milano,Taccuino,8\n,,,Zaino,35\n3,Sara,Torino,,\n</code></pre>"},{"location":"xan/join/#cross-join","title":"Cross join","text":"<p>Input usati:</p> <p><code>lettere.csv</code></p> <pre><code>lettera\nA\nB\n</code></pre> <p><code>numeri.csv</code></p> <pre><code>numero\n1\n2\n</code></pre> <pre><code>xan join --cross lettere.csv numeri.csv\n</code></pre> <p>Output:</p> <pre><code>lettera,numero\nA,1\nA,2\nB,1\nB,2\n</code></pre>"},{"location":"xan/join/#fuzzy-join-substring","title":"Fuzzy join (substring)","text":"<p>Esempio classico: hai una lista di parole/etichette e vuoi agganciare i testi dove compaiono.</p> <p>Sintassi usata:</p> <pre><code>xan fuzzy-join [opzioni] &lt;colonne_testo&gt; &lt;file_testi.csv&gt; &lt;colonna_pattern&gt; &lt;file_pattern.csv&gt;\n</code></pre> <p>Parametri chiave:</p> <ul> <li><code>-i</code>: confronto case-insensitive (ignora maiuscole/minuscole);</li> <li><code>--left</code>: mantiene tutte le righe del file testi anche quando non c'\u00e8 match;</li> <li><code>&lt;colonne_testo&gt;</code>: una o pi\u00f9 colonne in cui cercare (qui <code>testo</code>, ma puoi usare anche <code>titolo,testo</code>);</li> <li><code>&lt;file_testi.csv&gt;</code>: file dove cercare (qui <code>testi.csv</code>);</li> <li><code>&lt;colonna_pattern&gt;</code>: colonna con le stringhe/pattern da cercare (qui <code>needle</code>);</li> <li><code>&lt;file_pattern.csv&gt;</code>: file che contiene i pattern (qui <code>pattern_fuzzy.csv</code>).</li> </ul> <p>Input usati:</p> <p><code>testi.csv</code></p> <pre><code>id,testo\n1,\"Il Comune di Roma annuncia nuovi cantieri sulla mobilit\u00e0 urbana.\"\n2,\"Bando scuola digitale per laboratori e formazione docenti a Milano.\"\n3,\"Nuovo parco urbano con 300 alberi nel quartiere nord.\"\n</code></pre> <p><code>pattern_fuzzy.csv</code></p> <pre><code>tema,needle\nscuola,scuola\nmobilita,mobilita\nverde,parco\n</code></pre> <pre><code>xan fuzzy-join -i testo testi.csv needle pattern_fuzzy.csv\n</code></pre> <p>Lettura rapida del comando:</p> <ul> <li>cerca in <code>testo</code> (dentro <code>testi.csv</code>) i valori di <code>needle</code> (dentro <code>pattern_fuzzy.csv</code>);</li> <li>con <code>-i</code> trova match anche se cambia il maiuscolo/minuscolo.</li> </ul> <p>Output (estratto):</p> <pre><code>id,testo,tema,needle\n2,Bando scuola digitale per laboratori e formazione docenti a Milano.,scuola,scuola\n3,Nuovo parco urbano con 300 alberi nel quartiere nord.,verde,parco\n</code></pre> <p>Versione left (mantiene tutte le righe dei testi):</p> <pre><code>xan fuzzy-join -i --left testo testi.csv needle pattern_fuzzy.csv\n</code></pre> <p>Differenza rispetto al comando precedente:</p> <ul> <li>con <code>--left</code> ottieni anche le righe senza match, con colonne pattern vuote.</li> </ul> <p>Output (estratto):</p> <pre><code>id,testo,tema,needle\n1,Il Comune di Roma annuncia nuovi cantieri sulla mobilit\u00e0 urbana.,,\n2,Bando scuola digitale per laboratori e formazione docenti a Milano.,scuola,scuola\n3,Nuovo parco urbano con 300 alberi nel quartiere nord.,verde,parco\n</code></pre>"},{"location":"xan/join/#regex-join","title":"Regex join","text":"<p>Sintassi usata:</p> <pre><code>xan fuzzy-join -r [opzioni] &lt;colonne_testo&gt; &lt;file_testi.csv&gt; &lt;colonna_regex&gt; &lt;file_regex.csv&gt;\n</code></pre> <p>Parametri chiave:</p> <ul> <li><code>-r</code>: attiva il matching via espressioni regolari;</li> <li><code>-i</code>: rende il matching case-insensitive;</li> <li><code>&lt;colonne_testo&gt;</code>: una o pi\u00f9 colonne in cui cercare (qui <code>testo</code>, ma puoi usare anche <code>titolo,testo</code>);</li> <li><code>&lt;file_testi.csv&gt;</code>: file da analizzare (qui <code>testi.csv</code>);</li> <li><code>&lt;colonna_regex&gt;</code>: colonna con i pattern regex (qui <code>pattern</code>);</li> <li><code>&lt;file_regex.csv&gt;</code>: file che contiene i regex pattern (qui <code>pattern.csv</code>).</li> </ul> <p>Input aggiuntivo usato:</p> <p><code>pattern.csv</code></p> <pre><code>tema,pattern\nscuola,scuola\nmobilita,mobilit[a\u00e0]\nverde,parco|alberi\n</code></pre> <pre><code>xan fuzzy-join -r -i testo testi.csv pattern pattern.csv\n</code></pre> <p>Lettura rapida del comando:</p> <ul> <li>cerca in <code>testo</code> i pattern regex definiti nella colonna <code>pattern</code>;</li> <li>con <code>-r</code> interpreta i pattern come regex, non come semplice sottostringa.</li> </ul> <p>Output (estratto):</p> <pre><code>id,testo,tema,pattern\n1,Il Comune di Roma annuncia nuovi cantieri sulla mobilit\u00e0 urbana.,mobilita,mobilit[a\u00e0]\n2,Bando scuola digitale per laboratori e formazione docenti a Milano.,scuola,scuola\n3,Nuovo parco urbano con 300 alberi nel quartiere nord.,verde,parco|alberi\n</code></pre> <p>Quando usarlo:</p> <ul> <li>hai una tabella di pattern (keyword o regex);</li> <li>vuoi etichettare automaticamente testi con tema/categoria.</li> </ul>"},{"location":"xan/join/#url-join","title":"URL join","text":"<p>Sintassi usata:</p> <pre><code>xan fuzzy-join -u [opzioni] &lt;colonne_url&gt; &lt;file_link.csv&gt; &lt;colonna_prefisso&gt; &lt;file_sorgenti.csv&gt;\n</code></pre> <p>Parametri chiave:</p> <ul> <li><code>-u</code>: attiva il matching per prefisso URL;</li> <li><code>-S</code>: semplifica gli URL prima del confronto (schema, <code>www</code>, ecc.);</li> <li><code>&lt;colonne_url&gt;</code>: una o pi\u00f9 colonne URL da classificare (qui <code>link</code>, ma puoi indicare pi\u00f9 colonne se servono);</li> <li><code>&lt;file_link.csv&gt;</code>: file con gli URL completi (qui <code>link.csv</code>);</li> <li><code>&lt;colonna_prefisso&gt;</code>: colonna con i prefissi/domini da usare come riferimento (qui <code>url</code>);</li> <li><code>&lt;file_sorgenti.csv&gt;</code>: file con la mappatura sorgenti (qui <code>sorgenti_url.csv</code>).</li> </ul> <p>Input usati:</p> <p><code>link.csv</code></p> <pre><code>link\nhttps://www.comune.roma.it/notizie/mobilita\nhttps://dati.regione.lombardia.it/dataset/trasporti\nhttps://example.org/post/1\n</code></pre> <p><code>sorgenti_url.csv</code></p> <pre><code>fonte,url\ncomune-roma,comune.roma.it\nregione-lombardia,dati.regione.lombardia.it\n</code></pre> <pre><code>xan fuzzy-join -u -S link link.csv url sorgenti_url.csv\n</code></pre> <p>Lettura rapida del comando:</p> <ul> <li>confronta ogni valore di <code>link</code> con i prefissi presenti in <code>url</code>;</li> <li>con <code>-S</code> aumenta i match utili anche se gli URL hanno forme diverse.</li> </ul> <p>Output (estratto):</p> <pre><code>link,fonte,url\nhttps://www.comune.roma.it/notizie/mobilita,comune-roma,comune.roma.it\nhttps://dati.regione.lombardia.it/dataset/trasporti,regione-lombardia,dati.regione.lombardia.it\n</code></pre> <p>Quando usarlo:</p> <ul> <li>devi attribuire URL completi a domini/prefissi noti;</li> <li>vuoi arricchire rapidamente liste di link con fonte/editore/categoria.</li> </ul>"},{"location":"xan/qualita-dati/","title":"Fare controlli su un CSV con xan","text":"<p>Prima di analizzare un dataset vale la pena fare un giro veloce per capire se i dati sono affidabili: ci sono celle vuote dove non dovrebbero esserci? Valori fuori scala? Email malformate? Righe duplicate?</p> <p>xan non ha un comando di validazione dedicato, ma ha un set di strumenti componibili che permettono di fare questa esplorazione in modo rapido.</p>"},{"location":"xan/qualita-dati/#dataset-di-esempio","title":"Dataset di esempio","text":"<p>Usa il file di esempio: <code>docs/xan/risorse/qualita/contribuenti.csv</code></p> <ul> <li>CSV nel repository: docs/xan/risorse/qualita/contribuenti.csv</li> <li>Download diretto CSV: raw.githubusercontent.com/.../contribuenti.csv</li> </ul> <p>Il file contiene 13 righe con vari problemi intenzionali: campi obbligatori vuoti, un'et\u00e0 negativa, un'et\u00e0 impossibile, un reddito non numerico, email malformate, una riga duplicata.</p> <pre><code>id,cognome,nome,codice_fiscale,eta,reddito,email,comune\n1,Rossi,Mario,RSSMRA80A01H501Z,44,28500,mario.rossi@comune.it,Roma\n2,Bianchi,Anna,BNCNNA92B42F205X,32,41200,anna.bianchi@email.com,Milano\n3,Ferrari,Luca,,28,19800,,Torino\n4,Conti,Sara,CNTSAR75C55G224W,-5,15300,sara.conti@pec.it,Firenze\n5,Esposito,Carlo,SPSCRLO88D01F839Y,36,250000000,carlo@esposito,Napoli\n...\n</code></pre>"},{"location":"xan/qualita-dati/#panoramica-generale-con-stats","title":"Panoramica generale con <code>stats</code>","text":"<p>Il punto di partenza \u00e8 sempre <code>xan stats</code>: calcola statistiche per ogni colonna e rivela subito i problemi pi\u00f9 evidenti.</p> <pre><code>xan stats contribuenti.csv | xan select field,count,count_empty,type,min,max\n</code></pre> <pre><code>field,count,count_empty,type,min,max\nid,13,0,int,1,13\ncognome,13,0,string,,\nnome,13,0,string,,\ncodice_fiscale,11,2,string,,\neta,13,0,int,-5,150\nreddito,12,1,string,15300,250000000\nemail,12,1,string,,\ncomune,12,1,string,,\n</code></pre> <p>Cosa leggere:</p> <ul> <li><code>count_empty</code>: <code>codice_fiscale</code> ha 2 celle vuote, <code>reddito</code>, <code>email</code> e <code>comune</code> ne hanno 1 ciascuno;</li> <li><code>type</code>: <code>reddito</code> risulta <code>string</code> invece di <code>int</code> \u2014 segnale che una cella contiene un valore non numerico;</li> <li><code>min</code>/<code>max</code> su <code>eta</code>: -5 e 150 \u2014 entrambi fuori da qualsiasi range plausibile.</li> </ul> <p>Per isolare subito le colonne con celle vuote:</p> <pre><code>xan stats contribuenti.csv | xan filter 'count_empty &gt; 0' | xan select field,count_empty\n</code></pre> <pre><code>field,count_empty\ncodice_fiscale,2\nreddito,1\nemail,1\ncomune,1\n</code></pre>"},{"location":"xan/qualita-dati/#celle-vuote","title":"Celle vuote","text":"<p>Per trovare le righe che hanno almeno un campo vuoto:</p> <pre><code>xan search --empty contribuenti.csv\n</code></pre> <pre><code>id,cognome,nome,codice_fiscale,eta,reddito,email,comune\n3,Ferrari,Luca,,28,19800,,Torino\n7,De Luca,Marco,DLCMRC70F01L219P,54,,marco.deluca@libero.it,\n12,Fontana,Pietro,,150,18900,pietro.fontana@gmail.com,Milano\n</code></pre> <p>Per restringere la ricerca a una colonna specifica, aggiungi <code>-s</code>:</p> <pre><code>xan search --empty -s codice_fiscale contribuenti.csv\n</code></pre> <pre><code>id,cognome,nome,codice_fiscale,eta,reddito,email,comune\n3,Ferrari,Luca,,28,19800,,Torino\n12,Fontana,Pietro,,150,18900,pietro.fontana@gmail.com,Milano\n</code></pre>"},{"location":"xan/qualita-dati/#valori-fuori-range","title":"Valori fuori range","text":"<p>Con <code>xan filter</code> puoi definire soglie numeriche e isolare i valori anomali. Per trovare le righe con <code>eta</code> fuori dal range plausibile:</p> <pre><code>xan filter 'eta &lt; 0 or eta &gt; 120' contribuenti.csv\n</code></pre> <pre><code>id,cognome,nome,codice_fiscale,eta,reddito,email,comune\n4,Conti,Sara,CNTSAR75C55G224W,-5,15300,sara.conti@pec.it,Firenze\n12,Fontana,Pietro,,150,18900,pietro.fontana@gmail.com,Milano\n</code></pre> <p>Lo stesso approccio funziona per qualsiasi campo numerico. Per trovare redditi oltre un milione:</p> <pre><code>xan filter 'try(int(reddito) &gt; 1000000)' contribuenti.csv | xan select id,cognome,reddito\n</code></pre> <pre><code>id,cognome,reddito\n5,Esposito,250000000\n</code></pre> <p><code>try()</code> fa s\u00ec che le righe con <code>reddito</code> vuoto o non numerico non generino errori \u2014 vengono semplicemente ignorate dal filtro.</p>"},{"location":"xan/qualita-dati/#valori-non-numerici-in-colonne-numeriche","title":"Valori non numerici in colonne numeriche","text":"<p>Se <code>stats</code> segnala una colonna numerica con <code>type</code> uguale a <code>string</code>, significa che almeno un valore non \u00e8 un numero. Per trovare quali righe causano il problema:</p> <pre><code>xan filter 'try(int(reddito)) eq null and reddito ne \"\"' contribuenti.csv | xan select id,cognome,reddito\n</code></pre> <pre><code>id,cognome,reddito\n13,Lombardi,Chiara,abc\n</code></pre> <p>La condizione <code>reddito ne \"\"</code> esclude le celle semplicemente vuote, isolando solo quelle con un valore testuale non convertibile.</p>"},{"location":"xan/qualita-dati/#formato-errato-con-regex","title":"Formato errato con regex","text":"<p><code>xan search -r</code> filtra le righe che corrispondono a un pattern regex su una colonna. Con <code>-v</code> (invert) restituisce invece le righe che NON corrispondono \u2014 utile per trovare valori malformati.</p> <p>Per trovare email che non rispettano il formato base <code>qualcosa@dominio.estensione</code>:</p> <pre><code>xan search -v -r '^[^@]+@[^@]+\\.[^@]+$' -s email contribuenti.csv | xan select id,cognome,email\n</code></pre> <pre><code>id,cognome,email\n3,Ferrari,\n5,Esposito,carlo@esposito\n9,Greco,paolo.greco@\n</code></pre> <p>Tre problemi: una cella vuota, un dominio senza estensione, un indirizzo troncato.</p>"},{"location":"xan/qualita-dati/#duplicati","title":"Duplicati","text":"<p>Per verificare se una colonna (o combinazione di colonne) contiene duplicati usa <code>xan dedup --check</code>. Il comando non modifica il file: restituisce un messaggio e un codice di uscita, utile per script e pipeline automatizzate.</p> <pre><code>xan dedup --check -s codice_fiscale contribuenti.csv\n</code></pre> <pre><code>xan dedup: selection is NOT unique!\nFirst duplicate record found at index 10:\n\ncodice_fiscale RSSMRA80A01H501Z\n</code></pre> <p>Per trovare tutti i valori duplicati e contarli, usa invece <code>xan frequency</code>:</p> <pre><code>xan frequency -s codice_fiscale contribuenti.csv | xan filter 'count &gt; 1'\n</code></pre> <pre><code>field,value,count\ncodice_fiscale,&lt;empty&gt;,2\ncodice_fiscale,RSSMRA80A01H501Z,2\n</code></pre> <p>Due codici fiscali compaiono pi\u00f9 di una volta: le due celle vuote e il codice <code>RSSMRA80A01H501Z</code> che appartiene alla riga duplicata.</p>"},{"location":"xan/qualita-dati/#riepilogo-una-sequenza-di-controllo","title":"Riepilogo: una sequenza di controllo","text":"<p>Questi comandi possono essere eseguiti in sequenza su qualsiasi file CSV come primo controllo:</p> <pre><code># 1. panoramica generale\nxan stats file.csv | xan select field,count,count_empty,type,min,max\n\n# 2. colonne con celle vuote\nxan stats file.csv | xan filter 'count_empty &gt; 0' | xan select field,count_empty\n\n# 3. righe con almeno un campo vuoto\nxan search --empty file.csv\n\n# 4. duplicati sull'identificatore\nxan dedup --check -s id file.csv\n</code></pre> <p>I risultati orientano i controlli successivi pi\u00f9 mirati (range, formato, tipo).</p>"},{"location":"xan/risorse/from/comuni/","title":"Dati comuni","text":"<p>Tabella aggiornata al 2024.</p> comune provincia popolazione superficie_kmq Firenze FI 358079 102.3 Prato PO 194875 97.5 Siena SI 53901 118.5 Grosseto GR 80611 473.3 Arezzo AR 98822 388.0"},{"location":"blog/archive/2026/","title":"2026","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/strumenti/","title":"Strumenti","text":""},{"location":"blog/category/aggiornamenti/","title":"Aggiornamenti","text":""}]}